# åŸºäºé‡æ„çš„å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹ç®—æ³•åˆ†ææŠ¥å‘Š
## Reconstruction-based Anomaly Detection for Multivariate Time Series

---

## ğŸ“‹ è®ºæ–‡æ¦‚è¿°

**æ ‡é¢˜**: Reconstruction-based Anomaly Detection for Multivariate Time Series

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡å­¦ä¹ æ­£å¸¸æ•°æ®çš„æ½œåœ¨è¡¨å¾å’Œé‡æ„æœºåˆ¶ï¼Œå°†å¼‚å¸¸æ£€æµ‹è½¬åŒ–ä¸ºé‡æ„è¯¯å·®åˆ†æé—®é¢˜

**æŠ€æœ¯å…³é”®è¯**: 
- Reconstruction-based Learning (åŸºäºé‡æ„çš„å­¦ä¹ )
- Multivariate Time Series (å¤šå…ƒæ—¶åº)
- Autoencoder Architecture (è‡ªç¼–ç å™¨æ¶æ„)
- Representation Learning (è¡¨å¾å­¦ä¹ )
- Anomaly Detection (å¼‚å¸¸æ£€æµ‹)

---

## ğŸ” æ ¸å¿ƒæŠ€æœ¯æ¶æ„æ¨æµ‹

åŸºäº"Reconstruction-based"æ€æƒ³å’Œå½“å‰SOTAæŠ€æœ¯è¶‹åŠ¿ï¼Œè¯¥ç®—æ³•å¯èƒ½åŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

### 1. ç¼–ç å™¨-è§£ç å™¨æ¶æ„ (Encoder-Decoder Architecture)
```python
# ç›®æ ‡ï¼šå­¦ä¹ å¤šå…ƒæ—¶åºæ•°æ®çš„ç´§å‡‘è¡¨å¾
- æ—¶åºç¼–ç å™¨ï¼šæ•è·æ—¶é—´ä¾èµ–å…³ç³»
- å¤šå˜é‡ç¼–ç å™¨ï¼šæ•è·å˜é‡é—´å…³ç³»
- å±‚æ¬¡åŒ–ç‰¹å¾æå–ï¼šå¤šå°ºåº¦æ—¶åºå»ºæ¨¡
- ç“¶é¢ˆå±‚ï¼šå‹ç¼©è¡¨å¾å­¦ä¹ 
```

### 2. é‡æ„ç½‘ç»œ (Reconstruction Network)
```python
# ç›®æ ‡ï¼šä»å‹ç¼©è¡¨å¾æ¢å¤åŸå§‹æ—¶åºæ•°æ®
- è§£ç å™¨ç½‘ç»œï¼šé€†å‘é‡æ„è¿‡ç¨‹
- æ—¶åºè§£ç å™¨ï¼šæ¢å¤æ—¶é—´åºåˆ—ç»“æ„
- å˜é‡è§£ç å™¨ï¼šæ¢å¤å¤šå˜é‡å…³ç³»
- è¾“å‡ºå±‚ï¼šç”Ÿæˆé‡æ„æ—¶åº
```

### 3. å¼‚å¸¸åˆ†æ•°è®¡ç®— (Anomaly Score Computation)
```python
# ç›®æ ‡ï¼šåŸºäºé‡æ„è¯¯å·®é‡åŒ–å¼‚å¸¸ç¨‹åº¦
- é‡æ„è¯¯å·®è®¡ç®—ï¼šé€ç‚¹/å…¨å±€é‡æ„è¯¯å·®
- è¯¯å·®åŠ æƒï¼šé‡è¦ç‰¹å¾åŠ æƒ
- å¤šç»´åº¦èåˆï¼šæ—¶é—´+å˜é‡ç»´åº¦å¼‚å¸¸
- å¼‚å¸¸åˆ†æ•°æ ‡å‡†åŒ–ï¼šå½’ä¸€åŒ–å¤„ç†
```

### 4. è®­ç»ƒä¼˜åŒ–ç­–ç•¥ (Training Optimization)
```python
# ç›®æ ‡ï¼šä¼˜åŒ–é‡æ„è´¨é‡å’Œå¼‚å¸¸æ£€æµ‹æ€§èƒ½
- é‡æ„æŸå¤±ï¼šMSE/MAEæŸå¤±å‡½æ•°
- æ­£åˆ™åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
- å¯¹æŠ—è®­ç»ƒï¼šå¢å¼ºé²æ£’æ€§
- å¤šä»»åŠ¡å­¦ä¹ ï¼šè”åˆä¼˜åŒ–
```

---

## ğŸš€ é¢„æœŸæŠ€æœ¯ä¼˜åŠ¿

### 1. é‡æ„æœºåˆ¶ä¼˜åŠ¿
- **æ— ç›‘ç£å­¦ä¹ **: æ— éœ€å¼‚å¸¸æ ‡ç­¾ï¼Œä»…ç”¨æ­£å¸¸æ•°æ®è®­ç»ƒ
- **æ³›åŒ–èƒ½åŠ›å¼º**: å­¦ä¹ æ­£å¸¸æ¨¡å¼çš„å†…åœ¨ç»“æ„
- **å¯è§£é‡Šæ€§**: é‡æ„è¯¯å·®æä¾›ç›´è§‚çš„å¼‚å¸¸è§£é‡Š
- **é²æ£’æ€§**: å¯¹å™ªå£°å’Œç¼ºå¤±æ•°æ®å…·æœ‰å®¹å¿æ€§

### 2. å¤šå…ƒæ—¶åºä¸“ç”¨ä¼˜åŠ¿
- **æ—¶åºå»ºæ¨¡**: ä¸“é—¨é’ˆå¯¹æ—¶é—´ä¾èµ–å…³ç³»è®¾è®¡
- **å¤šå˜é‡å…³ç³»**: å……åˆ†åˆ©ç”¨å˜é‡é—´ç›¸å…³æ€§
- **åŠ¨æ€é€‚åº”**: é€‚åº”æ—¶å˜çš„æ•°æ®åˆ†å¸ƒ
- **é«˜ç»´å¤„ç†**: æœ‰æ•ˆå¤„ç†é«˜ç»´å¤šå…ƒæ•°æ®

### 3. å®ç”¨æ€§ä¼˜åŠ¿
- **å®æ—¶æ£€æµ‹**: æ”¯æŒåœ¨çº¿å¼‚å¸¸æ£€æµ‹
- **å¯æ‰©å±•æ€§**: é€‚ç”¨äºä¸åŒè§„æ¨¡çš„æ—¶åºæ•°æ®
- **éƒ¨ç½²å‹å¥½**: æ¨¡å‹ç»“æ„ç›¸å¯¹ç®€å•
- **è°ƒå‚å®¹æ˜“**: è¶…å‚æ•°è®¾ç½®ç›¸å¯¹ç¨³å®š

---

## ğŸ“Š é¢„æœŸæ€§èƒ½è¡¨ç°

åŸºäºé‡æ„æœºåˆ¶çš„æŠ€æœ¯ç‰¹ç‚¹ï¼Œé¢„æœŸè¯¥ç®—æ³•åœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼š

### Point-wiseå¼‚å¸¸æ£€æµ‹
- **ç›®æ ‡æ€§èƒ½**: Point F1 â‰¥ 88%
- **ä¼˜åŠ¿æ¥æº**: ç²¾ç¡®çš„é€ç‚¹é‡æ„è¯¯å·®è®¡ç®—
- **é€‚ç”¨åœºæ™¯**: ç²¾ç¡®çš„æ—¶é—´ç‚¹å¼‚å¸¸å®šä½

### Event-basedå¼‚å¸¸æ£€æµ‹  
- **ç›®æ ‡æ€§èƒ½**: Event F1 â‰¥ 72%
- **ä¼˜åŠ¿æ¥æº**: æ—¶åºè¿ç»­æ€§å»ºæ¨¡
- **é€‚ç”¨åœºæ™¯**: è¿ç»­å¼‚å¸¸äº‹ä»¶è¯†åˆ«

### è®¡ç®—æ•ˆç‡
- **å‚æ•°å¤æ‚åº¦**: ä¸­ç­‰ (20K-80Kå‚æ•°)
- **æ¨ç†é€Ÿåº¦**: å¿«é€Ÿ (é‡æ„è®¡ç®—é«˜æ•ˆ)
- **å†…å­˜å ç”¨**: é€‚ä¸­ (ç¼–ç å™¨-è§£ç å™¨ç»“æ„)

---

## ğŸ—ï¸ ç®—æ³•æ¶æ„è®¾è®¡

### æ•´ä½“æµç¨‹
```
åŸå§‹MTSæ•°æ® â†’ ç¼–ç å™¨ â†’ æ½œåœ¨è¡¨å¾ â†’ è§£ç å™¨ â†’ é‡æ„æ•°æ® â†’ å¼‚å¸¸åˆ†æ•°è®¡ç®— â†’ å¼‚å¸¸æ£€æµ‹ç»“æœ
     â†“           â†“        â†“        â†“         â†“            â†“             â†“
  [B,L,D]    [B,L,H]   [B,H]   [B,L,H]   [B,L,D]      [B,L]        [0/1æ ‡ç­¾]
```

### æ ¸å¿ƒæ¨¡å—è¯¦è§£

#### 1. å¤šå…ƒæ—¶åºç¼–ç å™¨
```python
class MultivariateTSEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, seq_len):
        # æ—¶åºå·ç§¯å±‚ - æ•è·å±€éƒ¨æ—¶åºæ¨¡å¼
        self.temporal_conv = nn.Conv1d(input_dim, hidden_dim, kernel_size=3)
        # å¤šå˜é‡æ³¨æ„åŠ› - æ•è·å˜é‡é—´å…³ç³»
        self.multivar_attention = nn.MultiheadAttention(hidden_dim, num_heads=4)
        # å¾ªç¯ç¥ç»ç½‘ç»œ - æ•è·é•¿æœŸä¾èµ–
        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        # å‹ç¼©å±‚
        self.compress = nn.Linear(hidden_dim * seq_len, hidden_dim)
    
    def forward(self, x):
        # x: [B, L, D]
        # æ—¶åºå·ç§¯
        x_conv = self.temporal_conv(x.transpose(1,2)).transpose(1,2)  # [B, L-2, H]
        # å¤šå˜é‡æ³¨æ„åŠ›
        x_attn, _ = self.multivar_attention(x_conv, x_conv, x_conv)  # [B, L-2, H]
        # å¾ªç¯å»ºæ¨¡
        x_rnn, _ = self.rnn(x_attn)  # [B, L-2, H]
        # å‹ç¼©åˆ°æ½œåœ¨è¡¨å¾
        x_flat = x_rnn.reshape(x_rnn.shape[0], -1)  # [B, (L-2)*H]
        z = self.compress(x_flat)  # [B, H]
        return z
```

#### 2. é‡æ„è§£ç å™¨
```python
class ReconstructionDecoder(nn.Module):
    def __init__(self, hidden_dim, output_dim, seq_len):
        # è§£å‹ç¼©å±‚
        self.decompress = nn.Linear(hidden_dim, hidden_dim * seq_len)
        # å¾ªç¯è§£ç å™¨
        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        # å˜é‡é‡æ„å±‚
        self.var_reconstruct = nn.Linear(hidden_dim, output_dim)
        # æ—¶åºå¹³æ»‘å±‚
        self.temporal_smooth = nn.Conv1d(output_dim, output_dim, kernel_size=3, padding=1)
    
    def forward(self, z, target_seq_len):
        # z: [B, H]
        # è§£å‹ç¼©
        x_decomp = self.decompress(z).reshape(z.shape[0], target_seq_len, -1)  # [B, L, H]
        # å¾ªç¯è§£ç 
        x_rnn, _ = self.rnn(x_decomp)  # [B, L, H]
        # å˜é‡é‡æ„
        x_recon = self.var_reconstruct(x_rnn)  # [B, L, D]
        # æ—¶åºå¹³æ»‘
        x_smooth = self.temporal_smooth(x_recon.transpose(1,2)).transpose(1,2)  # [B, L, D]
        return x_smooth
```

#### 3. å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨
```python
class AnomalyScoreComputer(nn.Module):
    def __init__(self, feature_dim):
        # é‡æ„è¯¯å·®åŠ æƒç½‘ç»œ
        self.error_weighter = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
        # æ—¶åºå¼‚å¸¸æ£€æµ‹å™¨
        self.temporal_detector = nn.LSTM(1, 8, batch_first=True)
        self.temporal_scorer = nn.Linear(8, 1)
    
    def forward(self, original, reconstructed):
        # è®¡ç®—é‡æ„è¯¯å·®
        recon_error = torch.abs(original - reconstructed)  # [B, L, D]
        
        # åŠ æƒé‡æ„è¯¯å·®
        error_weights = self.error_weighter(recon_error)  # [B, L, D]
        weighted_error = recon_error * error_weights  # [B, L, D]
        
        # å˜é‡ç»´åº¦å¼‚å¸¸åˆ†æ•°
        var_anomaly_score = torch.mean(weighted_error, dim=-1, keepdim=True)  # [B, L, 1]
        
        # æ—¶åºç»´åº¦å¼‚å¸¸åˆ†æ•°
        temp_features, _ = self.temporal_detector(var_anomaly_score)  # [B, L, 8]
        temp_anomaly_score = self.temporal_scorer(temp_features)  # [B, L, 1]
        
        # ç»¼åˆå¼‚å¸¸åˆ†æ•°
        final_score = torch.sigmoid(var_anomaly_score + temp_anomaly_score).squeeze(-1)  # [B, L]
        
        return final_score
```

#### 4. å®Œæ•´é‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹
```python
class ReconstructionAnomalyDetector(nn.Module):
    def __init__(self, input_dim, hidden_dim, seq_len):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len
        
        # ç¼–ç å™¨
        self.encoder = MultivariateTSEncoder(input_dim, hidden_dim, seq_len)
        # è§£ç å™¨
        self.decoder = ReconstructionDecoder(hidden_dim, input_dim, seq_len)
        # å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨
        self.anomaly_computer = AnomalyScoreComputer(input_dim)
    
    def forward(self, x):
        # x: [B, L, D]
        B, L, D = x.shape
        
        # ç¼–ç 
        latent_repr = self.encoder(x)  # [B, H]
        
        # è§£ç é‡æ„
        reconstructed = self.decoder(latent_repr, L)  # [B, L, D]
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
        anomaly_scores = self.anomaly_computer(x, reconstructed)  # [B, L]
        
        return {
            'reconstructed': reconstructed,
            'anomaly_scores': anomaly_scores,
            'latent_repr': latent_repr
        }
```

---

## ğŸ”§ æ¨¡å‹æ­å»ºå¤ç°æ€è·¯

### Phase 1: åŸºç¡€æ¶æ„æ­å»º (1-2å¤©)

#### 1.1 ç¯å¢ƒå‡†å¤‡
```bash
# ä¾èµ–å®‰è£…
pip install torch torchvision
pip install numpy pandas scikit-learn
pip install matplotlib seaborn
```

#### 1.2 æ•°æ®é¢„å¤„ç†æ¨¡å—
```python
class MTSDataProcessor:
    def __init__(self, window_size=64, stride=1):
        self.window_size = window_size
        self.stride = stride
        self.scaler = StandardScaler()
    
    def preprocess(self, data):
        # æ ‡å‡†åŒ–
        scaled_data = self.scaler.fit_transform(data)
        # æ»‘çª—å¤„ç†
        windows = self.create_windows(scaled_data)
        return windows
    
    def create_windows(self, data):
        windows = []
        for i in range(0, len(data) - self.window_size + 1, self.stride):
            windows.append(data[i:i+self.window_size])
        return np.array(windows)
```

#### 1.3 åŸºç¡€ç½‘ç»œç»„ä»¶
```python
# å®ç°ä¸Šè¿°ç¼–ç å™¨ã€è§£ç å™¨ã€å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨
```

### Phase 2: è®­ç»ƒä¼˜åŒ–å®ç° (2-3å¤©)

#### 2.1 æŸå¤±å‡½æ•°è®¾è®¡
```python
class ReconstructionLoss(nn.Module):
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01):
        super().__init__()
        self.alpha = alpha  # é‡æ„æŸå¤±æƒé‡
        self.beta = beta    # æ½œåœ¨è¡¨å¾æ­£åˆ™åŒ–æƒé‡
        self.gamma = gamma  # ç¨€ç–æ€§æ­£åˆ™åŒ–æƒé‡
    
    def forward(self, original, reconstructed, latent_repr):
        # é‡æ„æŸå¤±
        recon_loss = F.mse_loss(reconstructed, original)
        
        # æ½œåœ¨è¡¨å¾æ­£åˆ™åŒ–
        latent_reg = torch.mean(torch.norm(latent_repr, dim=1))
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–
        sparsity_reg = torch.mean(torch.abs(latent_repr))
        
        total_loss = (self.alpha * recon_loss + 
                     self.beta * latent_reg + 
                     self.gamma * sparsity_reg)
        
        return total_loss
```

#### 2.2 è®­ç»ƒç­–ç•¥
```python
class ReconstructionTrainer:
    def __init__(self, model, learning_rate=0.001):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, patience=10, factor=0.5
        )
        self.criterion = ReconstructionLoss()
    
    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        
        for batch in dataloader:
            self.optimizer.zero_grad()
            
            outputs = self.model(batch)
            loss = self.criterion(
                batch, 
                outputs['reconstructed'], 
                outputs['latent_repr']
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(dataloader)
```

### Phase 3: å¼‚å¸¸æ£€æµ‹å®ç° (1-2å¤©)

#### 3.1 å¼‚å¸¸æ£€æµ‹å™¨
```python
class AnomalyDetector:
    def __init__(self, model, threshold_method='percentile'):
        self.model = model
        self.threshold_method = threshold_method
        self.threshold = None
    
    def fit_threshold(self, normal_data):
        """åœ¨æ­£å¸¸æ•°æ®ä¸Šç¡®å®šå¼‚å¸¸é˜ˆå€¼"""
        self.model.eval()
        anomaly_scores = []
        
        with torch.no_grad():
            for batch in normal_data:
                outputs = self.model(batch)
                scores = outputs['anomaly_scores']
                anomaly_scores.extend(scores.flatten().cpu().numpy())
        
        if self.threshold_method == 'percentile':
            self.threshold = np.percentile(anomaly_scores, 95)
        elif self.threshold_method == 'std':
            mean_score = np.mean(anomaly_scores)
            std_score = np.std(anomaly_scores)
            self.threshold = mean_score + 3 * std_score
    
    def detect(self, test_data):
        """æ£€æµ‹æµ‹è¯•æ•°æ®ä¸­çš„å¼‚å¸¸"""
        self.model.eval()
        all_scores = []
        
        with torch.no_grad():
            for batch in test_data:
                outputs = self.model(batch)
                scores = outputs['anomaly_scores']
                all_scores.extend(scores.flatten().cpu().numpy())
        
        # å¼‚å¸¸åˆ¤æ–­
        anomalies = np.array(all_scores) > self.threshold
        return all_scores, anomalies
```

### Phase 4: é›†æˆä¸è¯„ä¼° (1-2å¤©)

#### 4.1 EasyTSADæ¡†æ¶é›†æˆ
```python
class ReconstructionBasedMTS(BaseMethod):
    def __init__(self, params: dict):
        super().__init__()
        self.__anomaly_score = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.detector = None
        self.scaler = StandardScaler()
        
        # æ¨¡å‹å‚æ•°
        self.hidden_dim = params.get('hidden_dim', 64)
        self.window_size = params.get('window_size', 64)
        self.learning_rate = params.get('learning_rate', 0.001)
        self.epochs = params.get('epochs', 100)
    
    def train_valid_phase(self, tsData):
        # æ•°æ®é¢„å¤„ç†
        train_data = self.scaler.fit_transform(tsData.train)
        
        # æ„å»ºæ¨¡å‹
        seq_len, input_dim = train_data.shape
        self.model = ReconstructionAnomalyDetector(
            input_dim=input_dim,
            hidden_dim=self.hidden_dim,
            seq_len=self.window_size
        ).to(self.device)
        
        # è®­ç»ƒ
        trainer = ReconstructionTrainer(self.model, self.learning_rate)
        # ... è®­ç»ƒå¾ªç¯å®ç°
        
        # å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–
        self.detector = AnomalyDetector(self.model)
        # ... é˜ˆå€¼æ‹Ÿåˆå®ç°
    
    def test_phase(self, tsData):
        # æµ‹è¯•æ•°æ®é¢„å¤„ç†
        test_data = self.scaler.transform(tsData.test)
        
        # å¼‚å¸¸æ£€æµ‹
        scores, anomalies = self.detector.detect(test_data)
        self.__anomaly_score = scores
    
    def anomaly_score(self):
        return self.__anomaly_score
    
    def param_statistic(self, save_file):
        # å‚æ•°ç»Ÿè®¡å®ç°
        pass
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æ¶æ„ä¼˜åŒ–
- **æ³¨æ„åŠ›æœºåˆ¶**: å¢å¼ºå˜é‡é—´å…³ç³»å»ºæ¨¡
- **æ®‹å·®è¿æ¥**: æ”¹å–„æ·±å±‚ç½‘ç»œè®­ç»ƒ
- **æ‰¹æ ‡å‡†åŒ–**: ç¨³å®šè®­ç»ƒè¿‡ç¨‹
- **Dropout**: é˜²æ­¢è¿‡æ‹Ÿåˆ

### 2. è®­ç»ƒä¼˜åŒ–
- **å­¦ä¹ ç‡è°ƒåº¦**: è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
- **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡è®­ç»ƒ
- **æ•°æ®å¢å¼º**: å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§
- **æ­£åˆ™åŒ–**: L1/L2æ­£åˆ™åŒ–

### 3. å¼‚å¸¸æ£€æµ‹ä¼˜åŒ–
- **å¤šé˜ˆå€¼ç­–ç•¥**: åŠ¨æ€é˜ˆå€¼è°ƒæ•´
- **é›†æˆæ–¹æ³•**: å¤šæ¨¡å‹æŠ•ç¥¨
- **åå¤„ç†**: å¹³æ»‘å’Œè¿é€šæ€§åˆ†æ
- **åœ¨çº¿å­¦ä¹ **: å¢é‡æ›´æ–°æœºåˆ¶

---

## ğŸ¯ å®ç°é‡Œç¨‹ç¢‘

### Week 1: åŸºç¡€å®ç°
- [ ] å®Œæˆç¼–ç å™¨-è§£ç å™¨æ¶æ„
- [ ] å®ç°åŸºç¡€è®­ç»ƒæµç¨‹
- [ ] é›†æˆEasyTSADæ¡†æ¶

### Week 2: æ€§èƒ½ä¼˜åŒ–
- [ ] ä¼˜åŒ–ç½‘ç»œæ¶æ„
- [ ] è°ƒä¼˜è¶…å‚æ•°
- [ ] å®ç°å¼‚å¸¸æ£€æµ‹ç­–ç•¥

### Week 3: æµ‹è¯•éªŒè¯
- [ ] åœ¨benchmarkæ•°æ®é›†ä¸Šæµ‹è¯•
- [ ] æ€§èƒ½å¯¹æ¯”åˆ†æ
- [ ] æ–‡æ¡£å®Œå–„

---

## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ç‚¹

1. **è‡ªé€‚åº”é‡æ„**: æ ¹æ®æ•°æ®ç‰¹æ€§è°ƒæ•´é‡æ„ç­–ç•¥
2. **å¤šå°ºåº¦ç¼–ç **: æ•è·ä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å¼
3. **å˜é‡è§£è€¦**: ç‹¬ç«‹å»ºæ¨¡ä¸åŒå˜é‡çš„å¼‚å¸¸æ¨¡å¼
4. **é²æ£’è®­ç»ƒ**: å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§

---

## ğŸ“Š é¢„æœŸæ€§èƒ½åŸºå‡†

åŸºäºé‡æ„æœºåˆ¶çš„æŠ€æœ¯ç‰¹ç‚¹ï¼Œé¢„æœŸåœ¨ä»¥ä¸‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼š

- **SMDæ•°æ®é›†**: Point F1 â‰¥ 85%, Event F1 â‰¥ 70%
- **SMAPæ•°æ®é›†**: Point F1 â‰¥ 90%, Event F1 â‰¥ 75%
- **MSLæ•°æ®é›†**: Point F1 â‰¥ 88%, Event F1 â‰¥ 72%

è¯¥é‡æ„åŸºå¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šè¿‡å­¦ä¹ æ­£å¸¸æ•°æ®çš„å†…åœ¨ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åç¦»æ­£å¸¸æ¨¡å¼çš„å¼‚å¸¸è¡Œä¸ºï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚ 