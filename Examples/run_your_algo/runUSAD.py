"""
USAD (UnSupervised Anomaly Detection) ç®—æ³•å®ç° - é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
åŸºäº EasyTSAD æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ machine æ•°æ®é›†ä¼˜åŒ–

æ€§èƒ½ç›®æ ‡: åœ¨EasyTSADæ•°æ®é›†ä¸Šè¾¾åˆ°93%+ F1åˆ†æ•°
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


def to_device(data, device):
    """å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(data, (list, tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)


class OptimizedUSADDataset(Dataset):
    """ä¼˜åŒ–çš„USADæ•°æ®é›†ç±»"""
    
    def __init__(self, data: np.ndarray, window_size: int, stride: int = 1):
        """
        Args:
            data: æ—¶åºæ•°æ® [time_steps, features] - å·²ç»æ ‡å‡†åŒ–
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            stride: æ»‘åŠ¨æ­¥é•¿
        """
        # å¢å¼ºæ•°æ®é¢„å¤„ç†ï¼šé¢å¤–çš„å½’ä¸€åŒ–ç¡®ä¿æ•°å€¼ç¨³å®š
        data_normalized = self._robust_normalize(data)
        self.data = torch.FloatTensor(data_normalized)
        self.window_size = window_size
        self.stride = stride
        self.num_samples = max(0, (len(data) - window_size) // stride + 1)
        
    def _robust_normalize(self, data):
        """é²æ£’çš„å½’ä¸€åŒ–æ–¹æ³•"""
        # ä½¿ç”¨robust scaling (ä¸­ä½æ•°å’ŒIQR)
        median = np.median(data, axis=0, keepdims=True)
        q75, q25 = np.percentile(data, [75, 25], axis=0, keepdims=True)
        iqr = q75 - q25
        iqr[iqr == 0] = 1.0  # é¿å…é™¤é›¶
        
        normalized = (data - median) / iqr
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        normalized = np.clip(normalized, -3, 3)
        # å†åšmin-maxå½’ä¸€åŒ–åˆ°[0,1]
        min_val = np.min(normalized, axis=0, keepdims=True)
        max_val = np.max(normalized, axis=0, keepdims=True)
        range_val = max_val - min_val
        range_val[range_val == 0] = 1.0
        
        final_normalized = (normalized - min_val) / range_val
        return final_normalized
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        if idx >= self.num_samples:
            raise IndexError(f"Index {idx} out of range")
            
        start_idx = idx * self.stride
        window = self.data[start_idx:start_idx + self.window_size]
        window_flat = window.view(-1)
        return window_flat


class StableEncoder(nn.Module):
    """è¶…ç¨³å®šç¼–ç å™¨ - å»é™¤BatchNormï¼Œä½¿ç”¨LayerNorm"""
    
    def __init__(self, input_size, latent_size):
        super().__init__()
        # æ›´ä¿å®ˆçš„éšè—å±‚è®¾è®¡
        hidden1 = max(128, input_size // 4)
        hidden2 = max(64, input_size // 8) 
        hidden3 = max(32, input_size // 16)
        
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden1),
            nn.LayerNorm(hidden1),  # LayerNormæ¯”BatchNormæ›´ç¨³å®š
            nn.ReLU(),
            nn.Dropout(0.05),  # é™ä½dropoutç‡
            
            nn.Linear(hidden1, hidden2),
            nn.LayerNorm(hidden2),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            nn.Linear(hidden2, hidden3),
            nn.LayerNorm(hidden3),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            nn.Linear(hidden3, latent_size),
            nn.ReLU()
        )
        
    def forward(self, x):
        return self.layers(x)


class StableDecoder(nn.Module):
    """è¶…ç¨³å®šè§£ç å™¨ - å»é™¤BatchNormï¼Œä½¿ç”¨LayerNorm"""
    
    def __init__(self, latent_size, output_size):
        super().__init__()
        # å¯¹ç§°ä½†æ›´ä¿å®ˆçš„è§£ç å™¨ç»“æ„
        hidden1 = max(32, output_size // 16)
        hidden2 = max(64, output_size // 8)
        hidden3 = max(128, output_size // 4)
        
        self.layers = nn.Sequential(
            nn.Linear(latent_size, hidden1),
            nn.LayerNorm(hidden1),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            nn.Linear(hidden1, hidden2),
            nn.LayerNorm(hidden2),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            nn.Linear(hidden2, hidden3),
            nn.LayerNorm(hidden3),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            nn.Linear(hidden3, output_size),
            nn.Tanh()  # ä½¿ç”¨Tanhæ›¿ä»£Sigmoidï¼Œè¾“å‡ºèŒƒå›´[-1,1]
        )
        
    def forward(self, x):
        return self.layers(x)


class SuperStableUSADModel(nn.Module):
    """è¶…ç¨³å®šUSADæ¨¡å‹"""
    
    def __init__(self, input_size, latent_size=None):
        super().__init__()
        
        if latent_size is None:
            # æ›´å°çš„æ½œåœ¨ç©ºé—´ï¼Œå‡å°‘å¤æ‚åº¦
            latent_size = max(16, min(64, input_size // 10))
            
        self.input_size = input_size
        self.latent_size = latent_size
        
        # ä½¿ç”¨è¶…ç¨³å®šçš„ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = StableEncoder(input_size, latent_size)
        self.decoder1 = StableDecoder(latent_size, input_size)
        self.decoder2 = StableDecoder(latent_size, input_size)
        
        # æƒé‡åˆå§‹åŒ– - ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–èŒƒå›´
            nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                
    def forward(self, x):
        z = self.encoder(x)
        recon1 = self.decoder1(z)
        recon2 = self.decoder2(z)
        return recon1, recon2
    
    def compute_losses(self, x, epoch):
        """è¶…ç¨³å®šçš„USADæŸå¤±è®¡ç®—"""
        z = self.encoder(x)
        w1 = self.decoder1(z)
        w2 = self.decoder2(z)
        
        # w3: decoder2é‡æ„decoder1çš„è¾“å‡º (æ–­å¼€æ¢¯åº¦)
        with torch.no_grad():  # å®Œå…¨æ–­å¼€æ¢¯åº¦
            w1_detached = w1.clone()
        z1 = self.encoder(w1_detached)
        w3 = self.decoder2(z1)
        
        # è¶…å¹³æ»‘çš„æƒé‡è¿‡æ¸¡ - é¿å…çªç„¶å˜åŒ–
        progress = min(epoch / 50.0, 0.8)  # 50è½®å†…å¹³æ»‘è¿‡æ¸¡ï¼Œæœ€å¤§0.8
        alpha = 1.0 - progress
        beta = progress
        
        # åŸºç¡€é‡æ„æŸå¤±
        rec_loss1 = F.mse_loss(x, w1, reduction='mean')
        rec_loss2 = F.mse_loss(x, w2, reduction='mean')
        rec_loss3 = F.mse_loss(x, w3, reduction='mean')
        
        # è¶…ç¨³å®šçš„æŸå¤±ç»„åˆ
        loss1 = alpha * rec_loss1 + beta * rec_loss3
        loss2 = alpha * rec_loss2 - beta * rec_loss3
        
        # é™åˆ¶loss2çš„èŒƒå›´ï¼Œé˜²æ­¢è¿‡åº¦è´Ÿå€¼
        loss2 = torch.clamp(loss2, min=-0.1, max=1.0)
        
        # è½»å¾®æ­£åˆ™åŒ–
        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())
        loss1 = loss1 + 1e-8 * l2_reg
        loss2 = loss2 + 1e-8 * l2_reg
        
        return loss1, loss2


class USAD(BaseMethod):
    """USADå¼‚å¸¸æ£€æµ‹æ–¹æ³• - é«˜æ€§èƒ½ç‰ˆæœ¬"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None
        
        if params is None:
            params = {}
        
        # è¶…ç¨³å®šå‚æ•°é…ç½®
        self.config = {
            'window_size': params.get('window', 16),          # å‡å°çª—å£
            'latent_size': params.get('latent_size', None),
            'batch_size': params.get('batch_size', 256),      # å¢å¤§æ‰¹æ¬¡
            'epochs': params.get('epochs', 15),              # å‡å°‘è®­ç»ƒè½®æ•°
            'learning_rate': params.get('lr', 5e-4),         # è¶…ä½å­¦ä¹ ç‡
            'weight_decay': params.get('weight_decay', 1e-6), # å¾ˆå°çš„æƒé‡è¡°å‡
            'alpha': params.get('alpha', 0.5),
            'beta': params.get('beta', 0.5),
            'patience': params.get('patience', 8),           # æ›´å¤§è€å¿ƒ
            'stride': params.get('stride', 1),
            'grad_clip': params.get('grad_clip', 0.5)        # æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
        }
        
        print(f"[LOG] USADè¶…ç¨³å®šç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[LOG] è¶…ç¨³å®šé…ç½®: window={self.config['window_size']}, epochs={self.config['epochs']}, lr={self.config['learning_rate']}")
        print(f"[LOG] æ¢¯åº¦è£å‰ª: {self.config['grad_clip']}, æƒé‡è¡°å‡: {self.config['weight_decay']}")
    
    def train_valid_phase(self, tsTrain: MTSData):
        """è¶…ç¨³å®šè®­ç»ƒé˜¶æ®µ"""
        print(f"\n[LOG] ========== USADè¶…ç¨³å®šè®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        input_size = self.config['window_size'] * self.n_features
        
        print(f"[LOG] è®­ç»ƒæ•°æ®: {train_data.shape}, è¾“å…¥ç»´åº¦: {input_size}")
        
        # åˆ›å»ºè¶…ç¨³å®šæ¨¡å‹
        self.model = SuperStableUSADModel(
            input_size=input_size,
            latent_size=self.config['latent_size']
        ).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = OptimizedUSADDataset(
            train_data, 
            self.config['window_size'],
            self.config['stride']
        )
        dataloader = DataLoader(
            dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True,
            num_workers=0,
            pin_memory=True
        )
        
        if len(dataloader) == 0:
            print("[ERROR] æ•°æ®åŠ è½½å™¨ä¸ºç©º!")
            return
        
        # è¶…ä¿å®ˆçš„ä¼˜åŒ–å™¨ - ä½¿ç”¨Adamè€Œä¸æ˜¯AdamW
        opt1 = torch.optim.Adam(
            list(self.model.encoder.parameters()) + list(self.model.decoder1.parameters()),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            eps=1e-8  # æ•°å€¼ç¨³å®šæ€§
        )
        opt2 = torch.optim.Adam(
            list(self.model.encoder.parameters()) + list(self.model.decoder2.parameters()),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            eps=1e-8
        )
        
        # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(
            opt1, mode='min', factor=0.5, patience=2, verbose=False, min_lr=1e-6
        )
        scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(
            opt2, mode='min', factor=0.5, patience=2, verbose=False, min_lr=1e-6
        )
        
        print(f"[LOG] å¼€å§‹è¶…ç¨³å®šè®­ç»ƒï¼Œ{self.config['epochs']}è½®ï¼Œæ¢¯åº¦è£å‰ª={self.config['grad_clip']}")
        
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss1, total_loss2 = 0, 0
            num_batches = 0
            
            # å®æ—¶ç›‘æ§æ¢¯åº¦çˆ†ç‚¸
            max_grad_norm1, max_grad_norm2 = 0, 0
            
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.config['epochs']}")
            
            for batch in pbar:
                try:
                    batch = batch.to(self.device, non_blocking=True)
                    
                    # æ£€æŸ¥è¾“å…¥æ•°æ®çš„æ•°å€¼ç¨³å®šæ€§
                    if torch.isnan(batch).any() or torch.isinf(batch).any():
                        print(f"[WARNING] å‘ç°NaN/Infè¾“å…¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    # è®­ç»ƒAE1 - æ›´å°å¿ƒçš„è®­ç»ƒ
                    opt1.zero_grad()
                    loss1, _ = self.model.compute_losses(batch, epoch)
                    
                    # æ£€æŸ¥æŸå¤±å€¼
                    if torch.isnan(loss1) or torch.isinf(loss1) or loss1 > 10.0:
                        print(f"[WARNING] Loss1å¼‚å¸¸: {loss1.item():.6f}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                        
                    loss1.backward()
                    
                    # å¼ºæ¢¯åº¦è£å‰ª
                    grad_norm1 = torch.nn.utils.clip_grad_norm_(
                        list(self.model.encoder.parameters()) + list(self.model.decoder1.parameters()), 
                        max_norm=self.config['grad_clip']
                    )
                    max_grad_norm1 = max(max_grad_norm1, grad_norm1.item())
                    
                    opt1.step()
                    
                    # è®­ç»ƒAE2 - æ›´å°å¿ƒçš„è®­ç»ƒ
                    opt2.zero_grad()
                    _, loss2 = self.model.compute_losses(batch, epoch)
                    
                    # æ£€æŸ¥æŸå¤±å€¼
                    if torch.isnan(loss2) or torch.isinf(loss2) or abs(loss2) > 10.0:
                        print(f"[WARNING] Loss2å¼‚å¸¸: {loss2.item():.6f}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                        
                    loss2.backward()
                    
                    # å¼ºæ¢¯åº¦è£å‰ª
                    grad_norm2 = torch.nn.utils.clip_grad_norm_(
                        list(self.model.encoder.parameters()) + list(self.model.decoder2.parameters()), 
                        max_norm=self.config['grad_clip']
                    )
                    max_grad_norm2 = max(max_grad_norm2, grad_norm2.item())
                    
                    opt2.step()
                    
                    total_loss1 += loss1.item()
                    total_loss2 += loss2.item()
                    num_batches += 1
                    
                    pbar.set_postfix({
                        'L1': f'{loss1.item():.4f}',
                        'L2': f'{loss2.item():.4f}',
                        'G1': f'{grad_norm1:.3f}',
                        'G2': f'{grad_norm2:.3f}',
                        'LR': f'{opt1.param_groups[0]["lr"]:.6f}'
                    })
                    
                except Exception as e:
                    print(f"[ERROR] è®­ç»ƒæ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
            
            if num_batches == 0:
                print("[ERROR] æ²¡æœ‰æˆåŠŸçš„è®­ç»ƒæ‰¹æ¬¡ï¼")
                break
                
            avg_loss1 = total_loss1 / num_batches
            avg_loss2 = total_loss2 / num_batches
            combined_loss = avg_loss1 + abs(avg_loss2)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler1.step(avg_loss1)
            scheduler2.step(abs(avg_loss2))
            
            print(f"âœ… Epoch {epoch+1}: Loss1={avg_loss1:.4f}, Loss2={avg_loss2:.4f}, Combined={combined_loss:.4f}")
            print(f"ğŸ“Š æ¢¯åº¦èŒƒæ•°: Max_G1={max_grad_norm1:.3f}, Max_G2={max_grad_norm2:.3f}")
            
            # æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹
            if max_grad_norm1 > 5.0 or max_grad_norm2 > 5.0:
                print(f"[WARNING] æ£€æµ‹åˆ°æ½œåœ¨æ¢¯åº¦çˆ†ç‚¸ï¼ŒG1={max_grad_norm1:.3f}, G2={max_grad_norm2:.3f}")
            
            # æ—©åœæ£€æŸ¥ - æ›´ä¸¥æ ¼çš„æ¡ä»¶
            if combined_loss < best_loss and avg_loss1 < 1.0 and abs(avg_loss2) < 1.0:
                best_loss = combined_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                self.best_model_state = {
                    'model': self.model.state_dict(),
                    'epoch': epoch
                }
            else:
                patience_counter += 1
                if patience_counter >= self.config['patience']:
                    print(f"[LOG] æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
                    break
                    
            # å¼ºåˆ¶æ—©åœæ¡ä»¶ - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ç»§ç»­
            if avg_loss1 > 1.0 or abs(avg_loss2) > 1.0:
                print(f"[LOG] æŸå¤±è¿‡å¤§ï¼Œå¼ºåˆ¶åœæ­¢è®­ç»ƒ")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_model_state'):
            self.model.load_state_dict(self.best_model_state['model'])
            print(f"[LOG] åŠ è½½ç¬¬{self.best_model_state['epoch']+1}è½®çš„æœ€ä½³æ¨¡å‹")
        
        print(f"[LOG] ========== USADè¶…ç¨³å®šè®­ç»ƒå®Œæˆ ==========\n")
    
    def test_phase(self, tsData: MTSData):
        """ä¼˜åŒ–çš„æµ‹è¯•é˜¶æ®µ"""
        print(f"\n[LOG] ========== USADé«˜æ€§èƒ½æµ‹è¯•å¼€å§‹ ==========")
        
        if self.model is None:
            print("[ERROR] æ¨¡å‹æœªè®­ç»ƒ")
            return
        
        test_data = tsData.test
        print(f"[LOG] æµ‹è¯•æ•°æ®: {test_data.shape}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = OptimizedUSADDataset(
            test_data, 
            self.config['window_size'],
            stride=1  # æµ‹è¯•æ—¶ä½¿ç”¨æ­¥é•¿1ç¡®ä¿å®Œæ•´è¦†ç›–
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=0,
            pin_memory=True
        )
        
        self.model.eval()
        scores = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="é«˜æ€§èƒ½æµ‹è¯•"):
                try:
                    batch = batch.to(self.device, non_blocking=True)
                    recon1, recon2 = self.model(batch)
                    
                    # å¤šç§å¼‚å¸¸åˆ†æ•°è®¡ç®—æ–¹å¼
                    error1 = torch.mean((batch - recon1) ** 2, dim=1)
                    error2 = torch.mean((batch - recon2) ** 2, dim=1)
                    
                    # æ–¹æ³•1: åŸºç¡€ç»„åˆ
                    score_basic = self.config['alpha'] * error1 + self.config['beta'] * error2
                    
                    # æ–¹æ³•2: å¯¹æŠ—æ€§åˆ†æ•°
                    z = self.model.encoder(batch)
                    w1 = self.model.decoder1(z)
                    w2_from_w1 = self.model.decoder2(self.model.encoder(w1))
                    adversarial_error = torch.mean((batch - w2_from_w1) ** 2, dim=1)
                    
                    # ç»„åˆåˆ†æ•°
                    final_score = score_basic + 0.3 * adversarial_error
                    
                    # æ•°å€¼ç¨³å®šæ€§å¤„ç†
                    final_score = torch.clamp(final_score, min=1e-8, max=1e6)
                    
                    scores.extend(final_score.cpu().numpy())
                    
                except Exception as e:
                    print(f"[ERROR] æµ‹è¯•æ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
        
        # å¤„ç†åˆ†æ•°é•¿åº¦å’Œå¹³æ»‘
        full_scores = self._process_scores(scores, len(test_data))
        
        self.__anomaly_score = full_scores
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡: å‡å€¼={np.mean(full_scores):.4f}, æ ‡å‡†å·®={np.std(full_scores):.4f}")
        print(f"[LOG] ========== USADæµ‹è¯•å®Œæˆ ==========\n")
    
    def _process_scores(self, scores, data_length):
        """å¤„ç†å’Œå¹³æ»‘å¼‚å¸¸åˆ†æ•°"""
        full_scores = np.zeros(data_length)
        
        if len(scores) > 0:
            # ä½¿ç”¨æ»‘åŠ¨å¹³å‡å¡«å……å‰é¢çš„ç‚¹
            window_fill = min(self.config['window_size'] - 1, len(scores))
            if window_fill > 0:
                avg_score = np.mean(scores[:window_fill])
                full_scores[:self.config['window_size']-1] = avg_score
            
            # å¡«å……å®é™…åˆ†æ•°
            end_idx = min(len(scores), data_length - self.config['window_size'] + 1)
            if end_idx > 0:
                full_scores[self.config['window_size']-1:self.config['window_size']-1+end_idx] = scores[:end_idx]
            
            # å¯¹åˆ†æ•°è¿›è¡Œå¯¹æ•°å˜æ¢å¢å¼ºåŒºåˆ†åº¦
            full_scores = np.log1p(full_scores)
            
            # åˆ†æ•°å¹³æ»‘ (ç§»åŠ¨å¹³å‡)
            window_size = 5
            if len(full_scores) >= window_size:
                smoothed_scores = np.copy(full_scores)
                for i in range(window_size//2, len(full_scores) - window_size//2):
                    smoothed_scores[i] = np.mean(full_scores[i-window_size//2:i+window_size//2+1])
                full_scores = smoothed_scores
        
        return full_scores
    
    def anomaly_score(self) -> np.ndarray:
        """è¿”å›å¼‚å¸¸åˆ†æ•°"""
        return self.__anomaly_score
    
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                USADé«˜æ€§èƒ½ç‰ˆæœ¬å‚æ•°ç»Ÿè®¡:
                ==================================================
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                çª—å£å¤§å°: {self.config['window_size']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['learning_rate']}
                æ½œåœ¨ç»´åº¦: {self.model.latent_size}
                ==================================================
                æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§:
                âœ… å¢å¼ºç½‘ç»œæ¶æ„ (BatchNorm + Dropout)
                âœ… é²æ£’æ•°æ®å½’ä¸€åŒ– (Robust Scaling)
                âœ… å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒåº¦
                âœ… æ—©åœæœºåˆ¶
                âœ… æ¢¯åº¦è£å‰ª
                âœ… å¤šé‡å¼‚å¸¸åˆ†æ•°è®¡ç®—
                âœ… åˆ†æ•°å¹³æ»‘å¤„ç†
                ==================================================
            """
        else:
            param_info = "USADæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== USADé«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ ==========")
    
    # Create a global controller
    gctrl = TSADController()
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )

    """============= Run Optimized USAD ============="""
    
    method = "USAD"

    # è¶…ç¨³å®šé…ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            "window": 16,           # å°çª—å£
            "batch_size": 256,      # å¤§æ‰¹æ¬¡
            "epochs": 15,           # å°‘è½®æ•°
            "lr": 5e-4,            # è¶…ä½å­¦ä¹ ç‡
            "latent_size": 32,      # å°æ½œåœ¨ç©ºé—´
            "weight_decay": 1e-6,   # æå°æƒé‡è¡°å‡
            "alpha": 0.5,
            "beta": 0.5,
            "patience": 8,          # å¤§è€å¿ƒ
            "stride": 1,
            "grad_clip": 0.5        # å¼ºæ¢¯åº¦è£å‰ª
        },
        preprocess="z-score",
    )
       
    """============= [EVALUATION SETTINGS] ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
        
    """============= [PLOTTING SETTINGS] ============="""
    
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== USADé«˜æ€§èƒ½ç‰ˆæœ¬æ‰§è¡Œå®Œæ¯• ==========") 