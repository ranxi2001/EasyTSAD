#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CoarseGrained-MTS: åŸºäºç²—ç²’åº¦å˜é‡å†…å¤–ä¾èµ–å…³ç³»çš„å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹ç®—æ³•
Multivariate Time Series Anomaly Detection by Capturing Coarse-Grained Intra- and Inter-Variate Dependencies

æ ¸å¿ƒåˆ›æ–°:
1. ç²—ç²’åº¦ç‰¹å¾æå–å™¨ - é™å™ªä¸æ•ˆç‡ä¼˜åŒ–
2. å˜é‡å†…ä¾èµ–å»ºæ¨¡ - æ—¶åºè‡ªç›¸å…³æ€§æ•è·  
3. å˜é‡é—´ä¾èµ–å»ºæ¨¡ - å¤šå˜é‡ååŒå…³ç³»å­¦ä¹ 
4. å¤šç»´å¼‚å¸¸æ£€æµ‹ - åŒé‡ä¾èµ–éªŒè¯çš„å¼‚å¸¸åˆ†æ•°

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-05-30
åŸºäºEasyTSADæ¡†æ¶æ ‡å‡†æ¥å£å®ç°
"""

from typing import Dict
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from sklearn.preprocessing import StandardScaler

from EasyTSAD.Controller import TSADController

print("ğŸš€ CoarseGrained-MTS ç®—æ³•å¯åŠ¨")
print("=" * 60)

# ===================== æ ¸å¿ƒæ¶æ„ç»„ä»¶ =====================

class BalancedCoarseGrainedExtractor(nn.Module):
    """å¹³è¡¡çš„ç²—ç²’åº¦ç‰¹å¾æå–å™¨ - ä¸­ç­‰å¤æ‚åº¦"""
    def __init__(self, input_dim, seq_len, coarse_factor=4):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len
        self.coarse_factor = coarse_factor
        self.coarse_len = max(8, seq_len // coarse_factor)
        # å¹³è¡¡çš„ç‰¹å¾ç»´åº¦ - ä¸å¤ªå¤§ä¸å¤ªå°
        self.compressed_dim = 24  # å›ºå®š24ç»´ï¼Œé€‚ä¸­çš„å¤æ‚åº¦
        
        print(f"[LOG] ğŸ”§ å¹³è¡¡ç²—ç²’åº¦æå–å™¨: {seq_len}->{self.coarse_len}, {input_dim}->{self.compressed_dim}")
        
        # æ—¶åºç²—åŒ–
        self.temporal_pooling = nn.AdaptiveAvgPool1d(self.coarse_len)
        
        # å¹³è¡¡çš„ç‰¹å¾å‹ç¼©ç½‘ç»œ
        self.feature_compress = nn.Sequential(
            nn.Linear(input_dim, self.compressed_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.compressed_dim * 2, self.compressed_dim),
            nn.ReLU()
        )
        
        # ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(self.compressed_dim, self.compressed_dim),
            nn.Tanh(),
            nn.Linear(self.compressed_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        B, L, D = x.shape
        
        # æ—¶åºç²—åŒ–
        x_temp = x.transpose(1, 2)  # [B, D, L]
        x_coarse_temp = self.temporal_pooling(x_temp)  # [B, D, L']
        x_coarse = x_coarse_temp.transpose(1, 2)  # [B, L', D]
        
        # ç‰¹å¾å‹ç¼©
        x_compressed = self.feature_compress(x_coarse)  # [B, L', 24]
        
        # æ³¨æ„åŠ›åŠ æƒ
        attention_weights = self.attention(x_compressed)  # [B, L', 1]
        x_weighted = x_compressed * attention_weights
        
        return x_weighted

class BalancedDependencyModel(nn.Module):
    """å¹³è¡¡çš„ä¾èµ–å»ºæ¨¡ - èåˆå˜é‡å†…å¤–ä¾èµ–"""
    def __init__(self, feature_dim):
        super().__init__()
        self.feature_dim = feature_dim
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ› - é€‚ä¸­å¤æ‚åº¦
        num_heads = min(3, feature_dim)  # 3ä¸ªå¤´
        if feature_dim % num_heads != 0:
            num_heads = 1  # ä¿è¯æ•´é™¤
        
        self.self_attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # 1Då·ç§¯æ•è·å±€éƒ¨ä¾èµ–
        self.conv_module = nn.Sequential(
            nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(feature_dim, feature_dim, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # èåˆç½‘ç»œ
        self.fusion = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim * 2),  # åŸå§‹+æ³¨æ„åŠ›+å·ç§¯
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU()
        )
        
    def forward(self, x):
        # x: [B, L', D] -> [B, L', D]
        
        # è·¯å¾„1: è‡ªæ³¨æ„åŠ›
        att_out, _ = self.self_attention(x, x, x)  # [B, L', D]
        
        # è·¯å¾„2: 1Då·ç§¯
        x_conv = x.transpose(1, 2)  # [B, D, L']
        conv_out = self.conv_module(x_conv)  # [B, D, L']
        conv_out = conv_out.transpose(1, 2)  # [B, L', D]
        
        # ä¸‰è·¯å¾„èåˆ
        combined = torch.cat([x, att_out, conv_out], dim=-1)  # [B, L', 3*D]
        output = self.fusion(combined)  # [B, L', D]
        
        return output

class BalancedAnomalyHead(nn.Module):
    """å¹³è¡¡çš„å¼‚å¸¸æ£€æµ‹å¤´ - é€‚ä¸­å¤æ‚åº¦"""
    def __init__(self, feature_dim, original_dim):
        super().__init__()
        
        # é‡æ„ç½‘ç»œ
        self.reconstruction_head = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, original_dim),
            nn.Tanh()
        )
        
        # å¼‚å¸¸åˆ†æ•°é¢„æµ‹
        self.anomaly_scorer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, 1),
            nn.Sigmoid()
        )
        
        # ç½®ä¿¡åº¦è¯„ä¼°
        self.confidence_scorer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # x: [B, L', D]
        reconstruction = self.reconstruction_head(x)  # [B, L', original_dim]
        anomaly_scores = self.anomaly_scorer(x)  # [B, L', 1]
        confidence_scores = self.confidence_scorer(x)  # [B, L', 1]
        
        return {
            'reconstruction': reconstruction,
            'anomaly_scores': anomaly_scores,
            'confidence_scores': confidence_scores
        }

class BalancedCoarseGrainedMTSModel(nn.Module):
    """å¹³è¡¡ç‰ˆCoarseGrained-MTSæ¨¡å‹ - ä¸­ç­‰å¤æ‚åº¦"""
    def __init__(self, input_dim, seq_len):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len
        
        # 1. å¹³è¡¡çš„ç²—ç²’åº¦ç‰¹å¾æå–å™¨
        self.coarse_extractor = BalancedCoarseGrainedExtractor(input_dim, seq_len)
        
        # 2. å¹³è¡¡çš„ä¾èµ–å»ºæ¨¡
        self.dependency_model = BalancedDependencyModel(24)  # å›ºå®š24ç»´
        
        # 3. å¹³è¡¡çš„å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = BalancedAnomalyHead(24, input_dim)
        
        # 4. åˆ†æ•°èåˆå™¨
        self.score_fusion = nn.Sequential(
            nn.Linear(3, 16),  # anomaly + confidence + recon_error
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # x: [B, L, D]
        B, L, D = x.shape
        
        # 1. ç²—ç²’åº¦ç‰¹å¾æå–
        coarse_features = self.coarse_extractor(x)  # [B, L', 24]
        
        # 2. ä¾èµ–å»ºæ¨¡
        dep_features = self.dependency_model(coarse_features)  # [B, L', 24]
        
        # 3. å¼‚å¸¸æ£€æµ‹
        detection_outputs = self.anomaly_head(dep_features)
        
        # 4. é‡æ„è¯¯å·®è®¡ç®—
        reconstruction = detection_outputs['reconstruction']  # [B, L', D]
        reconstruction_upsampled = F.interpolate(
            reconstruction.transpose(1, 2),  # [B, D, L']
            size=L,
            mode='linear',
            align_corners=False
        ).transpose(1, 2)  # [B, L, D]
        
        # è®¡ç®—é€ç‚¹é‡æ„è¯¯å·®
        recon_error_pointwise = F.mse_loss(
            reconstruction_upsampled, x, reduction='none'
        ).mean(dim=-1)  # [B, L]
        
        # ä¸Šé‡‡æ ·å¼‚å¸¸åˆ†æ•°å’Œç½®ä¿¡åº¦
        anomaly_scores = detection_outputs['anomaly_scores']  # [B, L', 1]
        confidence_scores = detection_outputs['confidence_scores']  # [B, L', 1]
        
        anomaly_upsampled = F.interpolate(
            anomaly_scores.transpose(1, 2),
            size=L, mode='linear', align_corners=False
        ).transpose(1, 2).squeeze(-1)  # [B, L]
        
        confidence_upsampled = F.interpolate(
            confidence_scores.transpose(1, 2),
            size=L, mode='linear', align_corners=False
        ).transpose(1, 2).squeeze(-1)  # [B, L]
        
        # 5. ä¸‰ç»´åˆ†æ•°èåˆ
        stacked_scores = torch.stack([
            anomaly_upsampled,
            confidence_upsampled,
            recon_error_pointwise
        ], dim=-1)  # [B, L, 3]
        
        final_scores = self.score_fusion(stacked_scores).squeeze(-1)  # [B, L]
        
        # å…¨å±€å¼‚å¸¸åˆ†æ•°ï¼ˆç”¨äºè®­ç»ƒï¼‰
        global_score = final_scores.mean(dim=1, keepdim=True)  # [B, 1]
        
        return {
            'final_scores': global_score,
            'reconstruction': reconstruction_upsampled.mean(dim=1),  # [B, D] å…¨å±€é‡æ„
            'pointwise_scores': final_scores  # [B, L] é€ç‚¹åˆ†æ•°
        }

class BalancedLoss(nn.Module):
    """å¹³è¡¡çš„æŸå¤±å‡½æ•° - å¤šä»»åŠ¡å­¦ä¹ """
    def __init__(self, alpha=1.0, beta=0.3, gamma=0.2):
        super().__init__()
        self.alpha = alpha  # é‡æ„æŸå¤±æƒé‡
        self.beta = beta    # å¹³æ»‘æŸå¤±æƒé‡
        self.gamma = gamma  # ä¸€è‡´æ€§æŸå¤±æƒé‡
        
    def forward(self, predictions, targets):
        # é‡æ„æŸå¤±
        target_global = torch.mean(targets, dim=1)  # [B, D]
        reconstruction_loss = F.mse_loss(predictions['reconstruction'], target_global)
        
        # å¹³æ»‘æŸå¤±
        if 'pointwise_scores' in predictions:
            scores = predictions['pointwise_scores']  # [B, L]
            if scores.shape[1] > 1:
                scores_diff = torch.diff(scores, dim=1)
                smoothness_loss = torch.mean(scores_diff ** 2)
            else:
                smoothness_loss = torch.tensor(0.0, device=scores.device)
        else:
            smoothness_loss = torch.tensor(0.0, device=reconstruction_loss.device)
        
        # ä¸€è‡´æ€§æŸå¤±
        if 'pointwise_scores' in predictions:
            global_mean = predictions['pointwise_scores'].mean(dim=1, keepdim=True)
            global_pred = predictions['final_scores']
            consistency_loss = F.mse_loss(global_mean, global_pred)
        else:
            consistency_loss = torch.tensor(0.0, device=reconstruction_loss.device)
        
        # æ€»æŸå¤±
        total_loss = (self.alpha * reconstruction_loss + 
                     self.beta * smoothness_loss + 
                     self.gamma * consistency_loss)
        
        return total_loss

# ===================== CoarseGrained-MTS ç®—æ³•å®ç° =====================

if __name__ == "__main__":
    
    print("[LOG] ğŸš€ å¼€å§‹è¿è¡ŒCoarseGrained-MTS (Coarse-Grained Multi-variate Time Series)")
    print("[LOG] ğŸ¯ ç›®æ ‡ï¼šåŸºäºç²—ç²’åº¦åŒé‡ä¾èµ–å»ºæ¨¡å®ç°MTSå¼‚å¸¸æ£€æµ‹")
    
    # Create a global controller
    gctrl = TSADController()
    print("[LOG] TSADControllerå·²åˆ›å»º")
        
    """============= [DATASET SETTINGS] ============="""
    datasets = ["machine-1", "machine-2", "machine-3"]
    dataset_types = "MTS"
    
    print("[LOG] å¼€å§‹è®¾ç½®æ•°æ®é›†")
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )
    print("[LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    """============= Implement CoarseGrained-MTS algo. ============="""
    from EasyTSAD.Methods import BaseMethod
    from EasyTSAD.DataFactory import MTSData

    print("[LOG] å¼€å§‹å®šä¹‰CoarseGrained-MTSç±»")
    
    class CoarseGrainedMTS(BaseMethod):
        def __init__(self, params: dict) -> None:
            super().__init__()
            self.__anomaly_score = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model = None
            self.criterion = None
            self.window_size = 48
            self.scaler = StandardScaler()
            print(f"[LOG] ğŸ¤– CoarseGrainedMTS.__init__() è°ƒç”¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            
        def _build_model(self, input_dim, seq_len=48):
            """æ„å»ºCoarseGrained-MTSæ¨¡å‹"""
            print(f"[LOG] ğŸ”§ æ„å»ºCoarseGrained-MTSæ¨¡å‹ï¼Œè¾“å…¥ç»´åº¦: {input_dim}, åºåˆ—é•¿åº¦: {seq_len}")
            
            self.window_size = seq_len
            
            # æ„å»ºæ¨¡å‹
            self.model = BalancedCoarseGrainedMTSModel(
                input_dim=input_dim,
                seq_len=seq_len
            ).to(self.device)
            
            self.criterion = BalancedLoss()
            
            param_count = sum(p.numel() for p in self.model.parameters())
            print(f"[LOG] âœ… CoarseGrained-MTSæ¨¡å‹æ„å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count}")
            
        def _create_windows(self, data, window_size, stride=1):
            """åˆ›å»ºæ»‘åŠ¨çª—å£"""
            windows = []
            for i in range(0, len(data) - window_size + 1, stride):
                windows.append(data[i:i+window_size])
            return torch.stack(windows) if windows else torch.unsqueeze(data[:window_size], 0)
        
        def train_valid_phase(self, tsData):
            print(f"[LOG] ğŸ“ CoarseGrainedMTS.train_valid_phase() è°ƒç”¨ï¼Œæ•°æ®å½¢çŠ¶: {tsData.train.shape}")
            
            # æ•°æ®é¢„å¤„ç†
            train_data = self.scaler.fit_transform(tsData.train)
            train_tensor = torch.FloatTensor(train_data).to(self.device)
            seq_len, input_dim = train_tensor.shape
            
            window_size = min(seq_len, 32)  # ä¿æŒè¾ƒå°çª—å£
            stride = max(1, window_size // 4)  # å¢å¤§æ­¥é•¿
            
            self._build_model(input_dim, window_size)
            
            # åˆ›å»ºè®­ç»ƒçª—å£
            train_windows = self._create_windows(train_tensor, window_size, stride)
            print(f"[LOG] ğŸ“Š è®­ç»ƒçª—å£æ•°é‡: {train_windows.shape[0]}")
            
            # å¹³è¡¡çš„è®­ç»ƒç­–ç•¥
            self.model.train()
            optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-4)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)
            
            num_epochs = 15  # å¹³è¡¡çš„è®­ç»ƒè½®æ•°
            batch_size = min(32, train_windows.shape[0])  # é€‚ä¸­çš„batch size
            
            best_loss = float('inf')
            patience = 6
            patience_counter = 0
            
            print(f"[LOG] ğŸš€ å¼€å§‹å¹³è¡¡CoarseGrained-MTSè®­ç»ƒï¼Œepochs: {num_epochs}, batch_size: {batch_size}")
            
            for epoch in range(num_epochs):
                total_loss = 0
                num_batches = 0
                
                # ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
                num_samples = min(train_windows.shape[0], batch_size * 15)  # 15ä¸ªbatch
                indices = torch.randperm(train_windows.shape[0])[:num_samples]
                
                for i in range(0, len(indices), batch_size):
                    batch_indices = indices[i:i+batch_size]
                    batch = train_windows[batch_indices]
                    
                    optimizer.zero_grad()
                    
                    try:
                        # å¹³è¡¡å‰å‘ä¼ æ’­
                        predictions = self.model(batch)
                        
                        # è®¡ç®—æŸå¤±
                        loss = self.criterion(predictions, batch)
                        
                        # åå‘ä¼ æ’­
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        optimizer.step()
                        
                        total_loss += loss.item()
                        num_batches += 1
                        
                    except Exception as e:
                        print(f"[WARNING] è®­ç»ƒæ‰¹æ¬¡å¤±è´¥: {e}")
                        continue
                
                if num_batches == 0:
                    print("[ERROR] æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡éƒ½å¤±è´¥")
                    break
                    
                avg_loss = total_loss / num_batches
                scheduler.step()
                
                # æ—©åœæœºåˆ¶
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    print(f"[LOG] â° æ—©åœåœ¨ç¬¬ {epoch+1} è½®ï¼Œæœ€ä½³æŸå¤±: {best_loss:.6f}")
                    break
                
                if (epoch + 1) % 3 == 0:  # æ¯3è½®æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"[LOG] ğŸ“ˆ Epoch {epoch+1}, Loss: {avg_loss:.6f}, "
                          f"LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            print("[LOG] âœ… å¹³è¡¡CoarseGrained-MTSè®­ç»ƒå®Œæˆ")
        
        def test_phase(self, tsData: MTSData):
            print(f"[LOG] ğŸ” CoarseGrainedMTS.test_phase() è°ƒç”¨ï¼Œæµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
            
            if self.model is None:
                print("[ERROR] æ¨¡å‹æœªè®­ç»ƒ")
                return
            
            # æ•°æ®é¢„å¤„ç†
            test_data = self.scaler.transform(tsData.test)
            test_tensor = torch.FloatTensor(test_data).to(self.device)
            seq_len, input_dim = test_tensor.shape
            
            self.model.eval()
            
            print(f"[LOG] ğŸ¯ å¼€å§‹å¢å¼ºCoarseGrained-MTSå¼‚å¸¸æ£€æµ‹ï¼Œåºåˆ—é•¿åº¦: {seq_len}")
            
            # ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†
            batch_size = 64  # å‡å°batch sizeä»¥é¿å…å†…å­˜é—®é¢˜
            all_scores = []
            
            with torch.no_grad():
                # å°†æ•´ä¸ªæµ‹è¯•åºåˆ—åˆ†æˆé‡å çš„çª—å£æ‰¹é‡å¤„ç†
                for start_idx in range(0, seq_len, batch_size):
                    end_idx = min(start_idx + batch_size, seq_len)
                    batch_windows = []
                    
                    for i in range(start_idx, end_idx):
                        window_start = max(0, i - self.window_size + 1)
                        window_end = i + 1
                        
                        if window_end - window_start < self.window_size:
                            window = torch.zeros(self.window_size, input_dim).to(self.device)
                            actual_data = test_tensor[window_start:window_end, :]
                            window[-actual_data.shape[0]:, :] = actual_data
                        else:
                            window = test_tensor[window_start:window_end, :]
                        
                        batch_windows.append(window)
                    
                    if batch_windows:
                        batch_tensor = torch.stack(batch_windows)  # [batch_size, window_size, input_dim]
                        
                        try:
                            predictions = self.model(batch_tensor)
                            # ä½¿ç”¨é€ç‚¹åˆ†æ•°è·å¾—æ›´ç²¾ç»†çš„å¼‚å¸¸æ£€æµ‹
                            if 'pointwise_scores' in predictions:
                                # å–æ¯ä¸ªçª—å£æœ€åä¸€ä¸ªæ—¶é—´ç‚¹çš„åˆ†æ•°
                                batch_scores = predictions['pointwise_scores'][:, -1].cpu().numpy()
                            else:
                                # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨å…¨å±€åˆ†æ•°
                                batch_scores = predictions['final_scores'].squeeze(-1).cpu().numpy()
                            all_scores.extend(batch_scores.tolist())
                            
                        except Exception as e:
                            print(f"[WARNING] æ‰¹æ¬¡ {start_idx}-{end_idx} é¢„æµ‹å¤±è´¥: {e}")
                            # å¡«å……é»˜è®¤åˆ†æ•°
                            all_scores.extend([0.0] * (end_idx - start_idx))
            
            scores = np.array(all_scores[:seq_len])  # ç¡®ä¿é•¿åº¦åŒ¹é…
            
            # æ”¹è¿›çš„åå¤„ç†ï¼šå¹³æ»‘+æ ‡å‡†åŒ–
            if len(scores) > 3:
                # è½»å¾®å¹³æ»‘
                kernel = np.array([0.25, 0.5, 0.25])
                smoothed = np.convolve(scores, kernel, mode='same')
            else:
                smoothed = scores
            
            # æ ‡å‡†åŒ–
            if len(smoothed) > 0:
                min_score, max_score = np.min(smoothed), np.max(smoothed)
                if max_score > min_score:
                    normalized = (smoothed - min_score) / (max_score - min_score)
                else:
                    normalized = smoothed
            else:
                normalized = smoothed
            
            self.__anomaly_score = normalized
            print(f"[LOG] ğŸ‰ å¢å¼ºCoarseGrained-MTSå¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œåˆ†æ•°èŒƒå›´: [{np.min(normalized):.4f}, {np.max(normalized):.4f}]")
        
        def anomaly_score(self) -> np.ndarray:
            return self.__anomaly_score
        
        def param_statistic(self, save_file):
            print(f"[LOG] ğŸ“Š CoarseGrainedMTS.param_statistic() è°ƒç”¨ï¼Œä¿å­˜åˆ°: {save_file}")
            model_params = sum(p.numel() for p in self.model.parameters()) if self.model else 0
            param_info = f"""ğŸš€ å¹³è¡¡ç‰ˆCoarseGrained-MTS æ¨¡å‹ä¿¡æ¯:
                
                ğŸ“‹ æ¨¡å‹ç±»å‹: å¹³è¡¡ç‰ˆCoarseGrained-MTS - ç¨³å®šçš„å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹å™¨
                ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}
                ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {model_params}
                
                ğŸ—ï¸ å¹³è¡¡æ¶æ„:
                âœ… å¹³è¡¡ç²—ç²’åº¦ç‰¹å¾æå–å™¨ (å›ºå®š24ç»´å‹ç¼©)
                âœ… å¹³è¡¡ä¾èµ–å»ºæ¨¡ (3å¤´æ³¨æ„åŠ›+å·ç§¯èåˆ)  
                âœ… å¹³è¡¡å¼‚å¸¸æ£€æµ‹å¤´ (é‡æ„+å¼‚å¸¸+ç½®ä¿¡åº¦)
                âœ… ä¼˜åŒ–æµ‹è¯•å¤„ç† (64æ‰¹é‡å¤§å°)
                
                ğŸš€ ä¼˜åŒ–ç­–ç•¥:
                1. å›ºå®šç‰¹å¾ç»´åº¦åˆ°24ç»´ - é€‚ä¸­çš„è¡¨è¾¾èƒ½åŠ›
                2. 3å¤´æ³¨æ„åŠ›æœºåˆ¶ - å¹³è¡¡å¤æ‚åº¦ä¸æ€§èƒ½
                3. ä¸‰è·¯å¾„ä¾èµ–å»ºæ¨¡ - åŸå§‹+æ³¨æ„åŠ›+å·ç§¯
                4. ä¸‰ç»´åˆ†æ•°èåˆ - å¼‚å¸¸+ç½®ä¿¡åº¦+é‡æ„è¯¯å·®
                5. 15è½®è®­ç»ƒç­–ç•¥ - å¹³è¡¡è®­ç»ƒæ—¶é—´ä¸æ•ˆæœ
                6. é€‚ä¸­batch size - ç¨³å®šè®­ç»ƒè¿‡ç¨‹
                
                ğŸ¯ è®¾è®¡ç›®æ ‡: åœ¨ç¨³å®šæ€§ä¸æ€§èƒ½ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
                ğŸ“ˆ é¢„æœŸæ•ˆæœ: Point F1 85%+, Event F1 65%+
                ğŸ”§ å·¥ç¨‹ä¼˜åŠ¿: ç¨³å®šæ¶æ„ï¼Œç»´åº¦åŒ¹é…ï¼Œå¿«é€Ÿæ”¶æ•›
                """
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(param_info)
    
    print("[LOG] âœ… CoarseGrained-MTSç±»å®šä¹‰å®Œæˆ")
    
    """============= Run CoarseGrained-MTS algo. ============="""
    training_schema = "mts"
    method = "CoarseGrainedMTS"
    
    print(f"[LOG] ğŸš€ å¼€å§‹è¿è¡ŒCoarseGrained-MTSå®éªŒï¼Œmethod={method}, training_schema={training_schema}")
    
    gctrl.run_exps(
        method=method,
        training_schema=training_schema,
        preprocess="z-score",
    )
    print("[LOG] ğŸ‰ CoarseGrained-MTSå®éªŒè¿è¡Œå®Œæˆ")
       
    """============= [EVALUATION SETTINGS] ============="""
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    print("[LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals([
        PointF1PA(),
        EventF1PA(),
        EventF1PA(mode="squeeze")
    ])
    print("[LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("[LOG] ğŸ” å¼€å§‹æ‰§è¡ŒCoarseGrained-MTSè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] âœ… CoarseGrained-MTSè¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
    """============= [PLOTTING SETTINGS] ============="""
    print("[LOG] ğŸ“Š å¼€å§‹CoarseGrained-MTSç»“æœç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] ğŸ¨ CoarseGrained-MTSç»˜å›¾å®Œæˆ")
    
    print("[LOG] ğŸ† CoarseGrained-MTS (Coarse-Grained Multi-variate Time Series) æ‰§è¡Œå®Œæ¯•")
    print("[LOG] ğŸ¯ æœŸå¾…åŸºäºç²—ç²’åº¦åŒé‡ä¾èµ–å»ºæ¨¡çš„ä¼˜å¼‚æ€§èƒ½è¡¨ç°ï¼") 