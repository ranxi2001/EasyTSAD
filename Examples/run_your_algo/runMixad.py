"""
MIXAD (Memory-Induced Explainable Time Series Anomaly Detection) ç®—æ³•å®ç°
åŸºäº EasyTSAD æ¡†æ¶ï¼Œé€‚ç”¨äºå¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹

æ ¸å¿ƒæŠ€æœ¯ï¼šè®°å¿†å¢å¼ºã€å›¾å·ç§¯ç½‘ç»œã€æ—¶ç©ºå»ºæ¨¡ã€å¯¹æ¯”å­¦ä¹ 
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
import math

warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


class MIXADDataset(Dataset):
    """MIXADä¸“ç”¨æ•°æ®é›†ç±»"""
    
    def __init__(self, data: np.ndarray, window_size: int, stride: int = 1):
        self.data = torch.FloatTensor(data)
        self.window_size = window_size
        self.stride = stride
        self.num_samples = max(0, (len(data) - window_size) // stride + 1)
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        start_idx = idx * self.stride
        window = self.data[start_idx:start_idx + self.window_size]
        return window, window


# ============= MIXADæ ¸å¿ƒç»„ä»¶å®ç° =============

def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> torch.Tensor:
    """Gumbel Softmaxé‡‡æ ·"""
    if eps != 1e-10:
        warnings.warn("`eps` parameter is deprecated and has no effect.")

    gumbels = (
        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
    )
    gumbels = (logits + gumbels) / tau
    y_soft = gumbels

    if hard:
        # Straight through.
        index = y_soft.max(dim, keepdim=True)[1]
        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)
        ret = y_hard - y_soft.detach() + y_soft
    else:
        # Reparametrization trick.
        ret = y_soft
    return ret


class GC(nn.Module):
    """å›¾å·ç§¯å±‚"""
    def __init__(self, dim_in, dim_out, cheb_k):
        super(GC, self).__init__()
        self.cheb_k = cheb_k 
        self.weights = nn.Parameter(torch.FloatTensor(2*cheb_k*dim_in, dim_out))
        self.bias = nn.Parameter(torch.FloatTensor(dim_out))
        nn.init.xavier_normal_(self.weights)
        nn.init.constant_(self.bias, val=0)
        
    def forward(self, x, supports):
        x_g = []        
        support_set = []

        # èšåˆé‚»å±…ä¿¡æ¯
        for support in supports:
            support_ks = [torch.eye(support.shape[0]).to(support.device), support]
            for k in range(2, self.cheb_k):
                support_ks.append(torch.matmul(2 * support, support_ks[-1]) - support_ks[-2]) 
            support_set.extend(support_ks)
        
        for support in support_set:
            x_g.append(torch.einsum("nm,bmc->bnc", support, x))
        x_g = torch.cat(x_g, dim=-1)

        # å˜æ¢
        x_gconv = torch.einsum('bni,io->bno', x_g, self.weights) + self.bias
        return x_gconv


class STRGCCell(nn.Module):
    """æ—¶ç©ºé€’å½’å›¾å·ç§¯å•å…ƒ"""
    def __init__(self, node_num, dim_in, dim_out, cheb_k):
        super(STRGCCell, self).__init__()
        self.node_num = node_num
        self.hidden_dim = dim_out
        self.gate = GC(dim_in+self.hidden_dim, 2*dim_out, cheb_k) 
        self.update = GC(dim_in+self.hidden_dim, dim_out, cheb_k)

    def forward(self, x, state, supports):
        state = state.to(x.device)
        input_and_state = torch.cat((x, state), dim=-1)

        z_r = torch.sigmoid(self.gate(input_and_state, supports)) 
        z, r = torch.split(z_r, self.hidden_dim, dim=-1) 
        candidate = torch.cat((x, z*state), dim=-1)
        hc = torch.tanh(self.update(candidate, supports))
        h = r*state + (1-r)*hc
        return h

    def init_hidden_state(self, batch_size):
        return torch.zeros(batch_size, self.node_num, self.hidden_dim)


class STRGC_Encoder(nn.Module):
    """æ—¶ç©ºé€’å½’å›¾å·ç§¯ç¼–ç å™¨"""
    def __init__(self, node_num, dim_in, dim_out, cheb_k, num_layers):
        super(STRGC_Encoder, self).__init__()
        assert num_layers >= 1, 'At least one DCRNN layer in the Encoder.'
        self.node_num = node_num 
        self.input_dim = dim_in 
        self.num_layers = num_layers 
        self.strgc_cells = nn.ModuleList()
        self.strgc_cells.append(STRGCCell(node_num, dim_in, dim_out, cheb_k))
        for _ in range(1, num_layers):
            self.strgc_cells.append(STRGCCell(node_num, dim_out, dim_out, cheb_k))

    def forward(self, x, init_state, supports):
        assert x.shape[2] == self.node_num and x.shape[3] == self.input_dim
        seq_length = x.shape[1]
        current_inputs = x 
        output_hidden = []

        for i in range(self.num_layers): 
            state = init_state[i] 
            inner_states = []

            for t in range(seq_length): 
                state = self.strgc_cells[i](current_inputs[:, t, :, :], state, supports)
                inner_states.append(state)

            output_hidden.append(state)
            current_inputs = torch.stack(inner_states, dim=1)

        return current_inputs, output_hidden 
    
    def init_hidden(self, batch_size): 
        init_states = []
        for i in range(self.num_layers):
            init_states.append(self.strgc_cells[i].init_hidden_state(batch_size)) 
        return init_states


class STRGC_Decoder(nn.Module):
    """æ—¶ç©ºé€’å½’å›¾å·ç§¯è§£ç å™¨"""
    def __init__(self, node_num, dim_in, dim_out, cheb_k, num_layers):
        super(STRGC_Decoder, self).__init__()
        assert num_layers >= 1, 'At least one DCRNN layer in the Decoder.'
        self.node_num = node_num
        self.input_dim = dim_in 
        self.num_layers = num_layers 
        self.strgc_cells = nn.ModuleList()
        self.strgc_cells.append(STRGCCell(node_num, dim_in, dim_out, cheb_k))
        for _ in range(1, num_layers):
            self.strgc_cells.append(STRGCCell(node_num, dim_out, dim_out, cheb_k))

    def forward(self, xt, init_state, supports): 
        assert xt.shape[1] == self.node_num and xt.shape[2] == self.input_dim
        current_inputs = xt 
        output_hidden = []

        for i in range(self.num_layers): 
            state = self.strgc_cells[i](current_inputs, init_state[i], supports) 
            output_hidden.append(state)
            current_inputs = state

        return current_inputs, output_hidden


class MIXADModel(nn.Module):
    """MIXADä¸»æ¨¡å‹"""
    def __init__(self, args):
        super(MIXADModel, self).__init__()
        self.num_nodes = args.num_nodes 
        self.input_dim = args.input_dim
        self.rnn_units = args.rnn_units
        self.output_dim = args.output_dim 
        self.horizon = args.seq_len
        self.num_layers = args.num_rnn_layers
        self.cheb_k = args.max_diffusion_step
        self.cl_decay_steps = args.cl_decay_steps
        self.use_curriculum_learning = args.use_curriculum_learning 
        
        self.mem_num = args.mem_num
        self.mem_dim = args.mem_dim
        self.memory = self.construct_memory()

        self.encoder = STRGC_Encoder(self.num_nodes, self.input_dim, self.rnn_units, self.cheb_k, self.num_layers)
        self.decoder_dim = self.rnn_units + self.mem_dim
        self.decoder = STRGC_Decoder(self.num_nodes, self.output_dim, self.decoder_dim, self.cheb_k, self.num_layers)

        self.proj = nn.Sequential(nn.Linear(self.decoder_dim, self.output_dim, bias=True))
    
    def compute_sampling_threshold(self, batches_seen):
        return self.cl_decay_steps / (self.cl_decay_steps + np.exp(batches_seen / self.cl_decay_steps))

    def construct_memory(self):
        """æ„å»ºè®°å¿†æ¨¡å—"""
        memory_dict = nn.ParameterDict()
        memory_dict['Memory'] = nn.Parameter(torch.randn(self.mem_num, self.mem_dim), requires_grad=True)
        memory_dict['Wq'] = nn.Parameter(torch.randn(self.rnn_units, self.mem_dim), requires_grad=True)
        memory_dict['We1'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num), requires_grad=True) 
        memory_dict['We2'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num), requires_grad=True) 
        for param in memory_dict.values():
            nn.init.xavier_normal_(param)
        return memory_dict
    
    def query_memory(self, h_t: torch.Tensor): 
        """æŸ¥è¯¢è®°å¿†æ¨¡å—"""
        query = torch.matmul(h_t, self.memory['Wq']) 
        att_score = torch.softmax(torch.matmul(query, self.memory['Memory'].t()), dim=-1)
        value = torch.matmul(att_score, self.memory['Memory'])

        _, ind = torch.topk(att_score, k=min(2, self.mem_num), dim=-1) 
        pos = self.memory['Memory'][ind[:, :, 0]] 
        neg = self.memory['Memory'][ind[:, :, 1]] if self.mem_num > 1 else pos
        return value, query, pos, neg, att_score
    
    def scaled_laplacian(self, node_embeddings1, node_embeddings2, is_eval=False):
        """å½’ä¸€åŒ–å›¾æ‹‰æ™®æ‹‰æ–¯å‡½æ•°"""
        node_num = self.num_nodes
        learned_graph = torch.mm(node_embeddings1, node_embeddings2.transpose(0, 1))
        
        # å½’ä¸€åŒ–å›¾
        norm1 = torch.norm(node_embeddings1, p=2, dim=1, keepdim=True)
        norm2 = torch.norm(node_embeddings2, p=2, dim=1, keepdim=True)
        norm = torch.mm(norm1, norm2.transpose(0, 1))
        learned_graph = learned_graph / (norm + 1e-6)
        learned_graph = (learned_graph + 1) / 2.
        learned_graph = torch.stack([learned_graph, 1-learned_graph], dim=-1)
       
        # åˆ¶ä½œç¨€ç–é‚»æ¥çŸ©é˜µ
        if is_eval:
            adj = gumbel_softmax(learned_graph, tau=1, hard=True)
        else:
            adj = gumbel_softmax(learned_graph, tau=1, hard=True)
        adj = adj[:, :, 0].clone().reshape(node_num, -1)
        mask = torch.eye(node_num, node_num).bool().to(node_embeddings1.device)
        adj.masked_fill_(mask, 0)
       
        W = adj
        n = W.shape[0]
        d = torch.sum(W, axis=1)
        L = -W
        L[range(len(L)), range(len(L))] = d
        
        try:
            lambda_max = (L.max() - L.min())
        except Exception as e:
            print("ç‰¹å¾å€¼è®¡ç®—é”™è¯¯: {}".format(e))
            lambda_max = 1.0
       
        tilde = (2 * L / lambda_max - torch.eye(n).to(node_embeddings1.device))
        return adj, tilde
            
    def forward(self, x, batches_seen=None):
        # èŠ‚ç‚¹åµŒå…¥å’Œå›¾æ„å»º
        node_embeddings1 = torch.matmul(self.memory['We1'], self.memory['Memory'])
        node_embeddings2 = torch.matmul(self.memory['We2'], self.memory['Memory'])
        
        if self.training:
            adj1, learned_tilde1 = self.scaled_laplacian(node_embeddings1, node_embeddings2, is_eval=False)
            adj2, learned_tilde2 = self.scaled_laplacian(node_embeddings2, node_embeddings1, is_eval=False)
        else:
            adj1, learned_tilde1 = self.scaled_laplacian(node_embeddings1, node_embeddings2, is_eval=True)
            adj2, learned_tilde2 = self.scaled_laplacian(node_embeddings2, node_embeddings1, is_eval=True)
        supports = [learned_tilde1, learned_tilde2] 

        # ç¼–ç 
        init_state = self.encoder.init_hidden(x.shape[0]) 
        h_en, state_en = self.encoder(x, init_state, supports) 
        h_t = h_en[:, -1, :, :] 

        # è®°å¿†æŸ¥è¯¢
        h_att, query, pos, neg, att_score = self.query_memory(h_t)
        h_t = torch.cat([h_t, h_att], dim=-1)
        
        # è§£ç 
        ht_list = [h_t] * self.num_layers
        go = torch.zeros((x.shape[0], self.num_nodes, self.output_dim), device=x.device) 
        out = []

        for t in range(self.horizon):
            h_de, ht_list = self.decoder(go, ht_list, supports)
            go = self.proj(h_de)
            out.append(go)

        output = torch.stack(out, dim=1)

        return output, h_att, query, pos, neg, att_score, (adj1, adj2)


def contrastive_loss(query, pos, neg, temperature=0.5):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""
    # è®¡ç®—ç›¸ä¼¼åº¦
    pos_sim = F.cosine_similarity(query, pos, dim=-1) / temperature
    neg_sim = F.cosine_similarity(query, neg, dim=-1) / temperature
    
    # å¯¹æ¯”æŸå¤±
    loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(neg_sim) + 1e-8))
    return loss.mean()


def consistency_loss(adj1, adj2):
    """ä¸€è‡´æ€§æŸå¤±å‡½æ•°"""
    return F.mse_loss(adj1, adj2)


def kl_loss(att_score):
    """KLæ•£åº¦æŸå¤±"""
    uniform = torch.ones_like(att_score) / att_score.shape[-1]
    return F.kl_div(torch.log(att_score + 1e-8), uniform, reduction='batchmean')


class MIXAD(BaseMethod):
    """MIXADå¼‚å¸¸æ£€æµ‹æ–¹æ³•"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None
        
        if params is None:
            params = {}
        
        # MIXADå‚æ•°é…ç½®
        self.config = {
            'window_size': params.get('window', 30),
            'batch_size': params.get('batch_size', 32),
            'epochs': params.get('epochs', 10),
            'learning_rate': params.get('lr', 1e-3),
            'rnn_units': params.get('rnn_units', 64),
            'max_diffusion_step': params.get('max_diffusion_step', 2),
            'num_rnn_layers': params.get('num_rnn_layers', 1),
            'mem_num': params.get('mem_num', 3),
            'mem_dim': params.get('mem_dim', 32),
            'lamb_cont': params.get('lamb_cont', 0.01),
            'lamb_cons': params.get('lamb_cons', 0.1),
            'lamb_kl': params.get('lamb_kl', 0.0001),
            'stride': params.get('stride', 1),
            'cl_decay_steps': params.get('cl_decay_steps', 2000),
            'use_curriculum_learning': params.get('use_curriculum_learning', True)
        }
        
        print(f"[LOG] MIXADåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[LOG] æ ¸å¿ƒç‰¹æ€§: è®°å¿†å¢å¼ºã€å›¾å·ç§¯ç½‘ç»œã€æ—¶ç©ºå»ºæ¨¡")
    
    def train_valid_phase(self, tsTrain: MTSData):
        """è®­ç»ƒé˜¶æ®µ"""
        print(f"\n[LOG] ========== MIXADè®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        
        print(f"[LOG] è®­ç»ƒæ•°æ®: {train_data.shape}")
        
        # å‡†å¤‡å‚æ•°
        class Args:
            def __init__(self, config, n_features):
                self.num_nodes = n_features
                self.input_dim = 1
                self.output_dim = 1
                self.seq_len = config['window_size']
                self.rnn_units = config['rnn_units']
                self.max_diffusion_step = config['max_diffusion_step']
                self.num_rnn_layers = config['num_rnn_layers']
                self.mem_num = config['mem_num']
                self.mem_dim = config['mem_dim']
                self.cl_decay_steps = config['cl_decay_steps']
                self.use_curriculum_learning = config['use_curriculum_learning']
        
        args = Args(self.config, self.n_features)
        
        # åˆ›å»ºæ¨¡å‹
        self.model = MIXADModel(args).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MIXADDataset(train_data, self.config['window_size'], self.config['stride'])
        dataloader = DataLoader(dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=0)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['learning_rate'])
        criterion = nn.MSELoss()
        
        print(f"[LOG] å¼€å§‹è®­ç»ƒï¼Œ{self.config['epochs']}è½®")
        
        batches_seen = 0
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss = 0
            num_batches = 0
            
            for batch_data, batch_targets in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
                try:
                    batch_data = batch_data.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    # é‡å¡‘æ•°æ® [B, T, N, 1]
                    batch_data = batch_data.unsqueeze(-1)
                    batch_targets = batch_targets.unsqueeze(-1)
                    
                    optimizer.zero_grad()
                    
                    # å‰å‘ä¼ æ’­
                    output, h_att, query, pos, neg, att_score, (adj1, adj2) = self.model(batch_data, batches_seen)
                    
                    # é‡æ„æŸå¤±
                    rec_loss = criterion(output, batch_targets)
                    
                    # å¯¹æ¯”æŸå¤±
                    cont_loss = contrastive_loss(query, pos, neg)
                    
                    # ä¸€è‡´æ€§æŸå¤±
                    cons_loss = consistency_loss(adj1, adj2)
                    
                    # KLæŸå¤±
                    kl_loss_val = kl_loss(att_score)
                    
                    # æ€»æŸå¤±
                    total_batch_loss = (rec_loss + 
                                      self.config['lamb_cont'] * cont_loss +
                                      self.config['lamb_cons'] * cons_loss +
                                      self.config['lamb_kl'] * kl_loss_val)
                    
                    total_batch_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
                    optimizer.step()
                    
                    total_loss += total_batch_loss.item()
                    num_batches += 1
                    batches_seen += 1
                    
                except Exception as e:
                    print(f"[WARNING] æ‰¹æ¬¡è®­ç»ƒé”™è¯¯: {e}")
                    continue
            
            avg_loss = total_loss / max(num_batches, 1)
            print(f"âœ… Epoch {epoch+1}: Loss = {avg_loss:.4f}")
        
        print(f"[LOG] ========== MIXADè®­ç»ƒå®Œæˆ ==========\n")
    
    def test_phase(self, tsData: MTSData):
        """æµ‹è¯•é˜¶æ®µ"""
        print(f"\n[LOG] ========== MIXADæµ‹è¯•å¼€å§‹ ==========")
        
        test_data = tsData.test
        test_dataset = MIXADDataset(test_data, self.config['window_size'], stride=1)
        test_loader = DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=0)
        
        self.model.eval()
        scores = []
        
        with torch.no_grad():
            for batch_data, batch_targets in tqdm(test_loader, desc="MIXADæµ‹è¯•"):
                try:
                    batch_data = batch_data.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    # é‡å¡‘æ•°æ®
                    batch_data = batch_data.unsqueeze(-1)
                    batch_targets = batch_targets.unsqueeze(-1)
                    
                    # å‰å‘ä¼ æ’­
                    output, _, _, _, _, _, _ = self.model(batch_data)
                    
                    # é‡æ„è¯¯å·®
                    rec_error = torch.mean((output - batch_targets) ** 2, dim=(1, 2, 3))
                    scores.extend(rec_error.cpu().numpy())
                    
                except Exception as e:
                    print(f"[WARNING] æµ‹è¯•æ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
        
        # å¤„ç†åˆ†æ•°é•¿åº¦
        full_scores = np.zeros(len(test_data))
        if len(scores) > 0:
            full_scores[:self.config['window_size']-1] = scores[0] if scores else 0.0
            end_idx = min(len(scores), len(full_scores) - self.config['window_size'] + 1)
            if end_idx > 0:
                full_scores[self.config['window_size']-1:self.config['window_size']-1+end_idx] = scores[:end_idx]
        
        self.__anomaly_score = full_scores
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] ========== MIXADæµ‹è¯•å®Œæˆ ==========\n")
    
    def anomaly_score(self) -> np.ndarray:
        return self.__anomaly_score
    
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                MIXAD (Memory-Induced Explainable Time Series Anomaly Detection) å‚æ•°ç»Ÿè®¡:
                ==================================================================
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                çª—å£å¤§å°: {self.config['window_size']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['learning_rate']}
                RNNå•å…ƒæ•°: {self.config['rnn_units']}
                è®°å¿†å•å…ƒæ•°: {self.config['mem_num']}
                è®°å¿†ç»´åº¦: {self.config['mem_dim']}
                å¯¹æ¯”æŸå¤±æƒé‡: {self.config['lamb_cont']}
                ä¸€è‡´æ€§æŸå¤±æƒé‡: {self.config['lamb_cons']}
                KLæŸå¤±æƒé‡: {self.config['lamb_kl']}
                ==================================================================
                MIXADæ ¸å¿ƒç‰¹æ€§:
                âœ… è®°å¿†å¢å¼ºæœºåˆ¶ (å¯å­¦ä¹ çš„è®°å¿†æ¨¡å—)
                âœ… æ—¶ç©ºé€’å½’å›¾å·ç§¯ (STRGC)
                âœ… è‡ªé€‚åº”å›¾å­¦ä¹  (Gumbel Softmax)
                âœ… å¯¹æ¯”å­¦ä¹  (æ­£è´Ÿæ ·æœ¬å¯¹æ¯”)
                âœ… å¤šä»»åŠ¡å­¦ä¹  (é‡æ„+å¯¹æ¯”+ä¸€è‡´æ€§+KL)
                âœ… è¯¾ç¨‹å­¦ä¹  (æ¸è¿›å¼è®­ç»ƒç­–ç•¥)
                ==================================================================
            """
        else:
            param_info = "MIXADæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== MIXAD: è®°å¿†å¢å¼ºæ—¶åºå¼‚å¸¸æ£€æµ‹ ==========")
    
    gctrl = TSADController()
    
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(dataset_type="MTS", dirname="./datasets", datasets=datasets)

    method = "MIXAD"

    # MIXADé…ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            "window": 30,              # çª—å£å¤§å°
            "batch_size": 32,          # æ‰¹æ¬¡å¤§å°
            "epochs": 10,              # è®­ç»ƒè½®æ•°
            "lr": 1e-3,               # å­¦ä¹ ç‡
            "rnn_units": 64,          # RNNå•å…ƒæ•°
            "max_diffusion_step": 2,   # æœ€å¤§æ‰©æ•£æ­¥æ•°
            "num_rnn_layers": 1,      # RNNå±‚æ•°
            "mem_num": 3,             # è®°å¿†å•å…ƒæ•°
            "mem_dim": 32,            # è®°å¿†ç»´åº¦
            "lamb_cont": 0.01,        # å¯¹æ¯”æŸå¤±æƒé‡
            "lamb_cons": 0.1,         # ä¸€è‡´æ€§æŸå¤±æƒé‡
            "lamb_kl": 0.0001,        # KLæŸå¤±æƒé‡
            "stride": 1,              # æ­¥é•¿
            "cl_decay_steps": 2000,   # è¯¾ç¨‹å­¦ä¹ è¡°å‡æ­¥æ•°
            "use_curriculum_learning": True  # æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
        },
        preprocess="z-score",
    )
       
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== MIXADæ‰§è¡Œå®Œæ¯• ==========") 