from typing import Dict
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from sklearn.preprocessing import StandardScaler

from EasyTSAD.Controller import TSADController

print("[LOG] ğŸš€ å¼€å§‹åˆå§‹åŒ–ReconstructionMTSç®—æ³•...")

# ============================================================================
# æ ¸å¿ƒæ¨¡å—1: å¤šå…ƒæ—¶åºç¼–ç å™¨
# ============================================================================
class MultivariateTSEncoder(nn.Module):
    """
    å¤šå…ƒæ—¶åºç¼–ç å™¨ - æ•è·æ—¶é—´ä¾èµ–å’Œå˜é‡é—´å…³ç³»
    """
    def __init__(self, input_dim, hidden_dim, seq_len):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        
        # æ—¶åºå·ç§¯å±‚ - æ•è·å±€éƒ¨æ—¶åºæ¨¡å¼
        self.temporal_conv1 = nn.Conv1d(input_dim, hidden_dim//2, kernel_size=3, padding=1)
        self.temporal_conv2 = nn.Conv1d(hidden_dim//2, hidden_dim, kernel_size=3, padding=1)
        
        # å¤šå˜é‡æ³¨æ„åŠ› - æ•è·å˜é‡é—´å…³ç³»
        self.multivar_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, 
            num_heads=max(1, hidden_dim//16), 
            batch_first=True
        )
        
        # å¾ªç¯ç¥ç»ç½‘ç»œ - æ•è·é•¿æœŸä¾èµ–ï¼ˆç§»é™¤dropouté¿å…è­¦å‘Šï¼‰
        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        
        # å‹ç¼©å±‚
        self.compress = nn.Sequential(
            nn.Linear(hidden_dim * seq_len, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, x):
        # x: [B, L, D]
        B, L, D = x.shape
        
        # æ—¶åºå·ç§¯å¤„ç†
        x_conv = F.relu(self.temporal_conv1(x.transpose(1,2)))  # [B, H//2, L]
        x_conv = F.relu(self.temporal_conv2(x_conv))  # [B, H, L]
        x_conv = x_conv.transpose(1,2)  # [B, L, H]
        
        # å¤šå˜é‡æ³¨æ„åŠ›
        x_attn, _ = self.multivar_attention(x_conv, x_conv, x_conv)  # [B, L, H]
        
        # å¾ªç¯å»ºæ¨¡
        x_rnn, _ = self.rnn(x_attn)  # [B, L, H]
        
        # å‹ç¼©åˆ°æ½œåœ¨è¡¨å¾
        x_flat = x_rnn.reshape(B, -1)  # [B, L*H]
        z = self.compress(x_flat)  # [B, H]
        
        return z

# ============================================================================
# æ ¸å¿ƒæ¨¡å—2: é‡æ„è§£ç å™¨
# ============================================================================
class ReconstructionDecoder(nn.Module):
    """
    é‡æ„è§£ç å™¨ - ä»æ½œåœ¨è¡¨å¾æ¢å¤åŸå§‹æ—¶åºæ•°æ®
    """
    def __init__(self, hidden_dim, output_dim, seq_len):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.seq_len = seq_len
        
        # è§£å‹ç¼©å±‚
        self.decompress = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim * seq_len)
        )
        
        # å¾ªç¯è§£ç å™¨ï¼ˆç§»é™¤dropouté¿å…è­¦å‘Šï¼‰
        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        
        # å˜é‡é‡æ„å±‚
        self.var_reconstruct = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, output_dim)
        )
        
        # æ—¶åºå¹³æ»‘å±‚
        self.temporal_smooth = nn.Conv1d(output_dim, output_dim, kernel_size=3, padding=1)
        
    def forward(self, z):
        # z: [B, H]
        B = z.shape[0]
        
        # è§£å‹ç¼©
        x_decomp = self.decompress(z).reshape(B, self.seq_len, self.hidden_dim)  # [B, L, H]
        
        # å¾ªç¯è§£ç 
        x_rnn, _ = self.rnn(x_decomp)  # [B, L, H]
        
        # å˜é‡é‡æ„
        x_recon = self.var_reconstruct(x_rnn)  # [B, L, D]
        
        # æ—¶åºå¹³æ»‘
        x_smooth = self.temporal_smooth(x_recon.transpose(1,2)).transpose(1,2)  # [B, L, D]
        
        return x_smooth

# ============================================================================
# æ ¸å¿ƒæ¨¡å—3: å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨
# ============================================================================
class AnomalyScoreComputer(nn.Module):
    """
    å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨ - åŸºäºé‡æ„è¯¯å·®è®¡ç®—å¼‚å¸¸åˆ†æ•°
    """
    def __init__(self, feature_dim):
        super().__init__()
        self.feature_dim = feature_dim
        
        # é‡æ„è¯¯å·®åŠ æƒç½‘ç»œ
        self.error_weighter = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
        
        # æ—¶åºå¼‚å¸¸æ£€æµ‹å™¨
        self.temporal_detector = nn.LSTM(1, 8, batch_first=True)
        self.temporal_scorer = nn.Linear(8, 1)
        
    def forward(self, original, reconstructed):
        # è®¡ç®—é‡æ„è¯¯å·®
        recon_error = torch.abs(original - reconstructed)  # [B, L, D]
        
        # åŠ æƒé‡æ„è¯¯å·®
        error_weights = self.error_weighter(recon_error)  # [B, L, D]
        weighted_error = recon_error * error_weights  # [B, L, D]
        
        # å˜é‡ç»´åº¦å¼‚å¸¸åˆ†æ•°
        var_anomaly_score = torch.mean(weighted_error, dim=-1, keepdim=True)  # [B, L, 1]
        
        # æ—¶åºç»´åº¦å¼‚å¸¸åˆ†æ•°
        temp_features, _ = self.temporal_detector(var_anomaly_score)  # [B, L, 8]
        temp_anomaly_score = self.temporal_scorer(temp_features)  # [B, L, 1]
        
        # ç»¼åˆå¼‚å¸¸åˆ†æ•°
        final_score = torch.sigmoid(var_anomaly_score + temp_anomaly_score).squeeze(-1)  # [B, L]
        
        return final_score

# ============================================================================
# æ ¸å¿ƒæ¨¡å—4: é‡æ„æŸå¤±å‡½æ•°
# ============================================================================
class ReconstructionLoss(nn.Module):
    """
    é‡æ„æŸå¤±å‡½æ•° - å¤šç›®æ ‡æŸå¤±ä¼˜åŒ–
    """
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01):
        super().__init__()
        self.alpha = alpha  # é‡æ„æŸå¤±æƒé‡
        self.beta = beta    # æ½œåœ¨è¡¨å¾æ­£åˆ™åŒ–æƒé‡
        self.gamma = gamma  # ç¨€ç–æ€§æ­£åˆ™åŒ–æƒé‡
    
    def forward(self, original, reconstructed, latent_repr):
        # é‡æ„æŸå¤±
        recon_loss = F.mse_loss(reconstructed, original)
        
        # æ½œåœ¨è¡¨å¾æ­£åˆ™åŒ–
        latent_reg = torch.mean(torch.norm(latent_repr, dim=1))
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–
        sparsity_reg = torch.mean(torch.abs(latent_repr))
        
        total_loss = (self.alpha * recon_loss + 
                     self.beta * latent_reg + 
                     self.gamma * sparsity_reg)
        
        return total_loss, {
            'reconstruction': recon_loss,
            'latent_reg': latent_reg,
            'sparsity_reg': sparsity_reg
        }

# ============================================================================
# æ ¸å¿ƒæ¨¡å—5: å®Œæ•´é‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹
# ============================================================================
class ReconstructionAnomalyDetector(nn.Module):
    """
    å®Œæ•´çš„é‡æ„åŸºå¼‚å¸¸æ£€æµ‹æ¨¡å‹
    """
    def __init__(self, input_dim, hidden_dim, seq_len):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len
        
        # ç¼–ç å™¨
        self.encoder = MultivariateTSEncoder(input_dim, hidden_dim, seq_len)
        # è§£ç å™¨
        self.decoder = ReconstructionDecoder(hidden_dim, input_dim, seq_len)
        # å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨
        self.anomaly_computer = AnomalyScoreComputer(input_dim)
    
    def forward(self, x):
        # x: [B, L, D]
        B, L, D = x.shape
        
        # ç¼–ç 
        latent_repr = self.encoder(x)  # [B, H]
        
        # è§£ç é‡æ„
        reconstructed = self.decoder(latent_repr)  # [B, L, D]
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
        anomaly_scores = self.anomaly_computer(x, reconstructed)  # [B, L]
        
        return {
            'reconstructed': reconstructed,
            'anomaly_scores': anomaly_scores,
            'latent_repr': latent_repr
        }

# ============================================================================
# æ™ºèƒ½åå¤„ç†å™¨
# ============================================================================
class IntelligentPostProcessor:
    """
    æ™ºèƒ½åå¤„ç†å™¨ - ä¼˜åŒ–å¼‚å¸¸åˆ†æ•°
    """
    def __init__(self, contamination=0.1):
        self.contamination = contamination
        self.threshold = None
        self.fitted = False
    
    def fit(self, train_scores):
        """åœ¨è®­ç»ƒåˆ†æ•°ä¸Šæ‹Ÿåˆé˜ˆå€¼"""
        self.threshold = np.percentile(train_scores, (1 - self.contamination) * 100)
        self.fitted = True
    
    def process(self, raw_scores):
        """å¤„ç†åŸå§‹å¼‚å¸¸åˆ†æ•°"""
        if not self.fitted:
            # å¦‚æœæœªæ‹Ÿåˆï¼Œä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
            self.threshold = np.percentile(raw_scores, 95)
        
        # å¹³æ»‘å¤„ç†
        smoothed_scores = self._smooth_scores(raw_scores)
        
        # æ ‡å‡†åŒ–
        normalized_scores = self._normalize_scores(smoothed_scores)
        
        return normalized_scores
    
    def _smooth_scores(self, scores, window=5):
        """å¹³æ»‘å¼‚å¸¸åˆ†æ•°"""
        if len(scores) < window:
            return scores
        
        smoothed = np.convolve(scores, np.ones(window)/window, mode='same')
        return smoothed
    
    def _normalize_scores(self, scores):
        """æ ‡å‡†åŒ–å¼‚å¸¸åˆ†æ•°"""
        scores = np.array(scores)
        min_score = np.min(scores)
        max_score = np.max(scores)
        
        if max_score - min_score > 0:
            normalized = (scores - min_score) / (max_score - min_score)
        else:
            normalized = scores
        
        return normalized

print("[LOG] ğŸ”§ æ ¸å¿ƒæ¨¡å—å®šä¹‰å®Œæˆ")

# ============================================================================
# ä¸»ç®—æ³•ç±»: ReconstructionMTS
# ============================================================================
if __name__ == "__main__":
    # åˆ›å»ºæ§åˆ¶å™¨
    gctrl = TSADController()
    print("[LOG] TSADControllerå·²åˆ›å»º")
        
    """============= [DATASET SETTINGS] ============="""
    # æŒ‡å®šæ•°æ®é›†
    datasets = ["machine-1", "machine-2", "machine-3"]
    dataset_types = "MTS"
    
    print("[LOG] å¼€å§‹è®¾ç½®æ•°æ®é›†")
    # Use all curves in datasets:
    gctrl.set_dataset(
        dataset_type="MTS",
        # dirname="../../datasets",
        dirname="./datasets", # é¡¹ç›®æ ¹ç›®å½•ä¸­çš„ç›¸å¯¹è·¯å¾„ å°±æ˜¯å½“å‰è·¯å¾„
        datasets=datasets,
    )
    print("[LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    """============= Implement your algo. ============="""
    from EasyTSAD.Methods import BaseMethod
    from EasyTSAD.DataFactory import MTSData
    
    print("[LOG] å¼€å§‹å®šä¹‰ReconstructionMTSç±»")

    class ReconstructionMTS(BaseMethod):
        def __init__(self, params: dict) -> None:
            super().__init__()
            self.__anomaly_score = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model = None
            self.criterion = None
            self.post_processor = IntelligentPostProcessor()
            self.window_size = 64
            self.training_scores = []
            self.scaler = StandardScaler()
            
            # æ¨¡å‹å‚æ•°
            self.hidden_dim = params.get('hidden_dim', 64)
            self.learning_rate = params.get('learning_rate', 0.001)
            self.epochs = params.get('epochs', 30)
            self.batch_size = params.get('batch_size', 16)
            
            print(f"[LOG] ğŸ¤– ReconstructionMTS.__init__() è°ƒç”¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"[LOG] ğŸ“Š å‚æ•°é…ç½® - hidden_dim: {self.hidden_dim}, epochs: {self.epochs}")
            
        def _build_model(self, input_dim, seq_len=64):
            """æ„å»ºé‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
            print(f"[LOG] ğŸ”§ æ„å»ºReconstructionMTSæ¨¡å‹ï¼Œè¾“å…¥ç»´åº¦: {input_dim}, åºåˆ—é•¿åº¦: {seq_len}")
            
            self.window_size = seq_len
            
            self.model = ReconstructionAnomalyDetector(
                input_dim=input_dim,
                hidden_dim=self.hidden_dim,
                seq_len=seq_len
            ).to(self.device)
            
            self.criterion = ReconstructionLoss(alpha=1.0, beta=0.1, gamma=0.01)
            
            param_count = sum(p.numel() for p in self.model.parameters())
            print(f"[LOG] âœ… ReconstructionMTSæ¨¡å‹æ„å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count}")
            
        def _create_windows(self, data, window_size, stride=1):
            """åˆ›å»ºæ»‘åŠ¨çª—å£"""
            windows = []
            for i in range(0, len(data) - window_size + 1, stride):
                windows.append(data[i:i+window_size])
            return torch.stack(windows) if windows else torch.unsqueeze(data[:window_size], 0)
        
        def train_valid_phase(self, tsData):
            print(f"[LOG] ğŸ“ ReconstructionMTS.train_valid_phase() è°ƒç”¨ï¼Œæ•°æ®å½¢çŠ¶: {tsData.train.shape}")
            
            # æ•°æ®é¢„å¤„ç†
            train_data_scaled = self.scaler.fit_transform(tsData.train)
            train_data = torch.FloatTensor(train_data_scaled).to(self.device)
            seq_len, input_dim = train_data.shape
            
            window_size = min(seq_len, 64)
            stride = max(1, window_size // 8)
            
            self._build_model(input_dim, window_size)
            
            # åˆ›å»ºè®­ç»ƒçª—å£
            train_windows = self._create_windows(train_data, window_size, stride)
            print(f"[LOG] ğŸ“Š è®­ç»ƒçª—å£æ•°é‡: {train_windows.shape[0]}")
            
            # è®­ç»ƒæ¨¡å‹
            self.model.train()
            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-4)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs, eta_min=1e-6)
            
            batch_size = min(self.batch_size, train_windows.shape[0])
            
            best_loss = float('inf')
            patience = 15
            patience_counter = 0
            
            print(f"[LOG] ğŸš€ å¼€å§‹é‡æ„è®­ç»ƒï¼Œepochs: {self.epochs}, batch_size: {batch_size}")
            
            for epoch in range(self.epochs):
                total_loss = 0
                total_losses = {'reconstruction': 0, 'latent_reg': 0, 'sparsity_reg': 0}
                num_batches = 0
                
                indices = torch.randperm(train_windows.shape[0])
                
                for i in range(0, train_windows.shape[0], batch_size):
                    batch_indices = indices[i:i+batch_size]
                    batch = train_windows[batch_indices]
                    
                    optimizer.zero_grad()
                    
                    try:
                        # å‰å‘ä¼ æ’­
                        outputs = self.model(batch)
                        
                        # è®¡ç®—æŸå¤±
                        loss, loss_dict = self.criterion(
                            batch, 
                            outputs['reconstructed'], 
                            outputs['latent_repr']
                        )
                        
                        # åå‘ä¼ æ’­
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        optimizer.step()
                        
                        total_loss += loss.item()
                        for key, value in loss_dict.items():
                            if key in total_losses:
                                total_losses[key] += value.item()
                        num_batches += 1
                        
                        # æ”¶é›†è®­ç»ƒå¼‚å¸¸åˆ†æ•°ç”¨äºåå¤„ç†å™¨
                        if epoch == self.epochs - 1:  # æœ€åä¸€è½®æ”¶é›†
                            with torch.no_grad():
                                scores = outputs['anomaly_scores'].cpu().numpy()
                                self.training_scores.extend(scores.flatten())
                        
                    except Exception as e:
                        print(f"[WARNING] è®­ç»ƒæ‰¹æ¬¡å¤±è´¥: {e}")
                        continue
                
                if num_batches == 0:
                    print("[ERROR] æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡éƒ½å¤±è´¥")
                    break
                    
                avg_loss = total_loss / num_batches
                scheduler.step()
                
                # æ—©åœæœºåˆ¶
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    print(f"[LOG] â° æ—©åœåœ¨ç¬¬ {epoch+1} è½®ï¼Œæœ€ä½³æŸå¤±: {best_loss:.6f}")
                    break
                
                if (epoch + 1) % 10 == 0:
                    print(f"[LOG] ğŸ“ˆ Epoch {epoch+1}, Loss: {avg_loss:.6f}, "
                          f"Recon: {total_losses['reconstruction']/num_batches:.6f}")
            
            # è®­ç»ƒåå¤„ç†å™¨
            if self.training_scores:
                self.post_processor.fit(np.array(self.training_scores))
                print(f"[LOG] ğŸ§  åå¤„ç†å™¨è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ {len(self.training_scores)} ä¸ªæ ·æœ¬")
            
            print("[LOG] âœ… ReconstructionMTSè®­ç»ƒå®Œæˆ")
        
        def test_phase(self, tsData: MTSData):
            print(f"[LOG] ğŸ” ReconstructionMTS.test_phase() è°ƒç”¨ï¼Œæµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
            
            if self.model is None:
                print("[ERROR] æ¨¡å‹æœªè®­ç»ƒ")
                return
            
            # æ•°æ®é¢„å¤„ç†
            test_data_scaled = self.scaler.transform(tsData.test)
            test_data = torch.FloatTensor(test_data_scaled).to(self.device)
            seq_len, input_dim = test_data.shape
            
            self.model.eval()
            scores = []
            
            print(f"[LOG] ğŸ¯ å¼€å§‹é‡æ„å¼‚å¸¸æ£€æµ‹ï¼Œåºåˆ—é•¿åº¦: {seq_len}")
            
            with torch.no_grad():
                for i in range(seq_len):
                    start_idx = max(0, i - self.window_size + 1)
                    end_idx = i + 1
                    
                    if end_idx - start_idx < self.window_size:
                        # å¯¹äºåºåˆ—å¼€å¤´ï¼Œä½¿ç”¨é›¶å¡«å……
                        window = torch.zeros(self.window_size, input_dim).to(self.device)
                        actual_data = test_data[start_idx:end_idx, :]
                        window[-actual_data.shape[0]:, :] = actual_data
                    else:
                        window = test_data[start_idx:end_idx, :]
                    
                    window_batch = window.unsqueeze(0)  # [1, window_size, input_dim]
                    
                    try:
                        outputs = self.model(window_batch)
                        
                        # è·å–å½“å‰æ—¶é—´ç‚¹çš„å¼‚å¸¸åˆ†æ•°
                        current_score = outputs['anomaly_scores'][0, -1].item()  # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
                        scores.append(current_score)
                        
                    except Exception as e:
                        if i < 10:
                            print(f"[WARNING] çª—å£ {i} é¢„æµ‹å¤±è´¥: {e}")
                        scores.append(0.0)
            
            scores = np.array(scores)
            
            # åå¤„ç†
            processed_scores = self.post_processor.process(scores)
            
            self.__anomaly_score = processed_scores
            print(f"[LOG] âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œåˆ†æ•°èŒƒå›´: [{np.min(processed_scores):.4f}, {np.max(processed_scores):.4f}]")
        
        def anomaly_score(self) -> np.ndarray:
            return self.__anomaly_score
        
        def param_statistic(self, save_file):
            if self.model is not None:
                param_count = sum(p.numel() for p in self.model.parameters())
                param_info = f"""ğŸš€ ReconstructionMTS (é‡æ„åŸºå¼‚å¸¸æ£€æµ‹å™¨) æ¨¡å‹ä¿¡æ¯:
                
                ğŸ“‹ æ¨¡å‹ç±»å‹: ReconstructionMTS - åŸºäºé‡æ„çš„å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹
                ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}
                ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {param_count}
                
                ğŸ—ï¸ æ ¸å¿ƒæ¶æ„:
                âœ… å¤šå…ƒæ—¶åºç¼–ç å™¨ (Multivariate TS Encoder)
                âœ… é‡æ„è§£ç å™¨ (Reconstruction Decoder)  
                âœ… å¼‚å¸¸åˆ†æ•°è®¡ç®—å™¨ (Anomaly Score Computer)
                âœ… æ™ºèƒ½åå¤„ç†å™¨ (Intelligent Post-Processor)
                
                ğŸš€ æŠ€æœ¯åˆ›æ–°:
                1. ç¼–ç å™¨-è§£ç å™¨æ¶æ„ - å­¦ä¹ æ­£å¸¸æ•°æ®å†…åœ¨ç»“æ„
                2. å¤šç»´é‡æ„è¯¯å·® - æ—¶é—´+å˜é‡ç»´åº¦å¼‚å¸¸æ£€æµ‹
                3. æ··åˆæŸå¤±ä¼˜åŒ– - é‡æ„+æ­£åˆ™åŒ–+ç¨€ç–æ€§çº¦æŸ
                4. æ™ºèƒ½åå¤„ç† - åˆ†æ•°å¹³æ»‘å’Œè‡ªé€‚åº”æ ‡å‡†åŒ–
                5. æ— ç›‘ç£å­¦ä¹  - ä»…éœ€æ­£å¸¸æ•°æ®è®­ç»ƒ
                
                ğŸ¯ è®¾è®¡ç›®æ ‡: å®ç°é«˜ç²¾åº¦é‡æ„åŸºå¼‚å¸¸æ£€æµ‹
                ğŸ“ˆ é¢„æœŸæ•ˆæœ: Point F1 85%+, Event F1 70%+
                ğŸ”§ å·¥ç¨‹ä¼˜åŠ¿: ç»“æ„ç®€æ´ï¼Œæ³›åŒ–èƒ½åŠ›å¼º
                """
                with open(save_file, 'w', encoding='utf-8') as f:
                    f.write(param_info)
                print(f"[LOG] ğŸ“Š å‚æ•°ç»Ÿè®¡å·²ä¿å­˜åˆ° {save_file}")

    print("[LOG] âœ… ReconstructionMTSç±»å®šä¹‰å®Œæˆ")
    
    """============= Run your algo. ============="""
    training_schema = "mts"
    method = "ReconstructionMTS"
    
    print(f"[LOG] ğŸš€ å¼€å§‹è¿è¡ŒReconstructionMTSå®éªŒï¼Œmethod={method}, training_schema={training_schema}")
    
    gctrl.run_exps(
        method=method,
        training_schema=training_schema,
        preprocess="z-score",
    )
    print("[LOG] ğŸ‰ ReconstructionMTSå®éªŒè¿è¡Œå®Œæˆ")
       
    """============= [EVALUATION SETTINGS] ============="""
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    print("[LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals([
        PointF1PA(),
        EventF1PA(),
        EventF1PA(mode="squeeze")
    ])
    print("[LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("[LOG] ğŸ” å¼€å§‹æ‰§è¡ŒReconstructionMTSè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] âœ… ReconstructionMTSè¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
    """============= [PLOTTING SETTINGS] ============="""
    print("[LOG] ğŸ“Š å¼€å§‹ReconstructionMTSç»“æœç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] ğŸ¨ ReconstructionMTSç»˜å›¾å®Œæˆ")
    
    print("[LOG] ğŸ† ReconstructionMTS (é‡æ„åŸºå¼‚å¸¸æ£€æµ‹å™¨) æ‰§è¡Œå®Œæ¯•")
    print("[LOG] ğŸ¯ æœŸå¾…ä¼˜ç§€çš„é‡æ„åŸºå¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼") 