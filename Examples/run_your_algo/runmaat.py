"""
MAAT (Mamba Adaptive Anomaly Transformer) ç®—æ³•å®ç° - å¿ å®åŸç‰ˆ
åŸºäº EasyTSAD æ¡†æ¶ï¼Œé€‚ç”¨äºå¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹

æ¢å¤MAATçš„æ ¸å¿ƒæŠ€æœ¯ï¼šå…³è”å·®å¼‚å»ºæ¨¡ã€å¼‚å¸¸æ³¨æ„åŠ›ã€ç¨€ç–æ³¨æ„åŠ›ç­‰
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
import math

warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


def my_kl_loss(p, q):
    """KLæ•£åº¦æŸå¤±å‡½æ•° - ç¨³å®šç‰ˆæœ¬"""
    # æ•°å€¼ç¨³å®šæ€§å¤„ç†
    p = torch.clamp(p, min=1e-8, max=1-1e-8)
    q = torch.clamp(q, min=1e-8, max=1-1e-8)
    
    # è®¡ç®—KLæ•£åº¦
    kl = p * (torch.log(p) - torch.log(q))
    
    # æ ¹æ®ç»´åº¦è¿›è¡Œä¸åŒçš„å¤„ç†
    if kl.dim() == 4:  # [B, H, L, L]
        return torch.mean(torch.sum(kl, dim=(-2, -1)))
    elif kl.dim() == 3:  # [B, L, D] 
        return torch.mean(torch.sum(kl, dim=(-2, -1)))
    else:
        return torch.mean(kl)


class MAATDataset(Dataset):
    """MAATä¸“ç”¨æ•°æ®é›†ç±»"""
    
    def __init__(self, data: np.ndarray, window_size: int, stride: int = 1):
        self.data = torch.FloatTensor(data)
        self.window_size = window_size
        self.stride = stride
        self.num_samples = max(0, (len(data) - window_size) // stride + 1)
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        start_idx = idx * self.stride
        window = self.data[start_idx:start_idx + self.window_size]
        return window, window


# ============= MAATæ ¸å¿ƒç»„ä»¶ - å¿ å®å®ç° =============

class TriangularCausalMask():
    """ä¸‰è§’å› æœæ©ç """
    def __init__(self, B, L, device="cuda"):
        mask_shape = [B, 1, L, L]
        with torch.no_grad():
            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)

    @property
    def mask(self):
        return self._mask


class SimplifiedMamba(nn.Module):
    """ç®€åŒ–ä½†ä¿æŒæ ¸å¿ƒæ€æƒ³çš„Mamba"""
    def __init__(self, d_model, d_state=8, expand=1.5):  # å‡å°‘d_stateï¼Œé™ä½expand
        super(SimplifiedMamba, self).__init__()
        self.d_model = d_model
        self.d_inner = int(expand * d_model)
        
        # è¾“å…¥æŠ•å½±
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)
        
        # çŠ¶æ€ç©ºé—´å‚æ•° - è°ƒæ•´ç»´åº¦
        self.A_log = nn.Parameter(torch.randn(self.d_inner, d_state))
        self.D = nn.Parameter(torch.randn(self.d_inner))
        
        # é€‰æ‹©æ€§å‚æ•°
        self.x_proj = nn.Linear(self.d_inner, d_state, bias=False)
        self.dt_proj = nn.Linear(d_state, self.d_inner, bias=True)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
        
        # æ¿€æ´»å‡½æ•°
        self.activation = nn.SiLU()
        
    def forward(self, x):
        B, L, D = x.shape
        
        # è¾“å…¥æŠ•å½±
        xz = self.in_proj(x)  # (B, L, 2*d_inner)
        x_ssm, z = xz.chunk(2, dim=-1)  # each (B, L, d_inner)
        
        # çŠ¶æ€ç©ºé—´æ¨¡å‹æ ¸å¿ƒ
        x_ssm = self.activation(x_ssm)
        
        # ç®€åŒ–çš„çŠ¶æ€ç©ºé—´è®¡ç®—
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        
        # é€‰æ‹©æ€§æœºåˆ¶
        delta = F.softplus(self.dt_proj(self.x_proj(x_ssm)))  # (B, L, d_inner)
        
        # çŠ¶æ€ç©ºé—´é€’æ¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        y = self._state_space_scan(x_ssm, delta, A)
        
        # é—¨æ§
        y = y * self.activation(z)
        
        # è¾“å‡ºæŠ•å½±
        output = self.out_proj(y)
        return output
    
    def _state_space_scan(self, x, delta, A):
        """ç®€åŒ–çš„çŠ¶æ€ç©ºé—´æ‰«æ"""
        B, L, D = x.shape
        N = A.shape[-1]
        
        # åˆå§‹çŠ¶æ€
        h = torch.zeros(B, D, N, device=x.device, dtype=x.dtype)
        outputs = []
        
        for t in range(L):
            # çŠ¶æ€æ›´æ–° 
            dt = delta[:, t, :].unsqueeze(-1)  # (B, D, 1)
            dA = torch.exp(dt * A.unsqueeze(0))  # (B, D, N)
            dB = dt  # ç®€åŒ–
            
            h = dA * h + dB * x[:, t, :].unsqueeze(-1)  # (B, D, N)
            
            # è¾“å‡º
            y = torch.sum(h, dim=-1) + self.D.unsqueeze(0) * x[:, t, :]  # (B, D)
            outputs.append(y)
        
        return torch.stack(outputs, dim=1)  # (B, L, D)


class AnomalyAttention(nn.Module):
    """å¼‚å¸¸æ³¨æ„åŠ›æœºåˆ¶ - æ¢å¤æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.1, output_attention=True):
        super(AnomalyAttention, self).__init__()
        self.scale = scale
        self.mask_flag = mask_flag
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)
        self.win_size = win_size

        # è·ç¦»çŸ©é˜µ - MAATçš„å…³é”®ç»„ä»¶
        self.register_buffer('distances', torch.zeros((win_size, win_size)))
        for i in range(win_size):
            for j in range(win_size):
                self.distances[i][j] = abs(i - j)

    def forward(self, queries, keys, values, sigma, attn_mask=None):
        B, L, H, E = queries.shape
        scale = self.scale or 1. / math.sqrt(E)

        # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
        scores = torch.einsum("blhe,bshe->bhls", queries, keys)
        
        if self.mask_flag and attn_mask is not None:
            scores.masked_fill_(attn_mask.mask, -np.inf)

        attn = scale * scores
        series = self.dropout(torch.softmax(attn, dim=-1))

        # è®¡ç®—å…ˆéªŒåˆ†å¸ƒ - MAATçš„æ ¸å¿ƒåˆ›æ–°
        try:
            sigma = sigma.transpose(1, 2)  # [B, H, L]
            actual_L = min(L, self.win_size)
            
            # ç¡®ä¿sigmaæ­£ç¡®
            if sigma.shape[-1] > actual_L:
                sigma = sigma[:, :, :actual_L]
            elif sigma.shape[-1] < actual_L:
                # å¡«å……
                pad_size = actual_L - sigma.shape[-1]
                sigma = F.pad(sigma, (0, pad_size), value=0.1)
            
            # å…ˆéªŒåˆ†å¸ƒè®¡ç®—
            sigma = torch.sigmoid(sigma * 5) + 1e-5
            sigma = torch.pow(3, sigma) - 1
            sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, actual_L)  # [B, H, L, L]

            # è·ç¦»å…ˆéªŒ
            distances_cropped = self.distances[:actual_L, :actual_L]
            prior = distances_cropped.unsqueeze(0).unsqueeze(0).repeat(B, H, 1, 1)
            prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))
            
            # å½’ä¸€åŒ–å…ˆéªŒåˆ†å¸ƒ
            prior = prior / (torch.sum(prior, dim=-1, keepdim=True) + 1e-8)
            
        except Exception as e:
            print(f"[WARNING] å…ˆéªŒè®¡ç®—å¤±è´¥: {e}")
            # åˆ›å»ºé»˜è®¤å…ˆéªŒåˆ†å¸ƒ
            prior = torch.ones(B, H, L, L, device=queries.device) * (1.0 / L)

        # è®¡ç®—è¾“å‡º
        V = torch.einsum("bhls,bshd->blhd", series, values)

        if self.output_attention:
            return (V.contiguous(), series, prior, sigma)
        else:
            return (V.contiguous(), None)


class AttentionLayer(nn.Module):
    """æ³¨æ„åŠ›å±‚"""
    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
        super(AttentionLayer, self).__init__()

        # ç¡®ä¿ç»´åº¦èƒ½è¢«æ•´é™¤
        assert d_model % n_heads == 0, f"d_model ({d_model}) must be divisible by n_heads ({n_heads})"
        
        d_keys = d_keys or (d_model // n_heads)
        d_values = d_values or (d_model // n_heads)
        
        self.inner_attention = attention
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)
        self.key_projection = nn.Linear(d_model, d_keys * n_heads)
        self.value_projection = nn.Linear(d_model, d_values * n_heads)
        self.sigma_projection = nn.Linear(d_model, n_heads)  # ä¿®å¤ï¼šç›´æ¥ä»d_modelæŠ•å½±åˆ°n_heads
        self.out_projection = nn.Linear(d_values * n_heads, d_model)
        self.n_heads = n_heads

    def forward(self, queries, keys, values, attn_mask=None):
        B, L, D = queries.shape  # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        H = self.n_heads
        
        queries = self.query_projection(queries).view(B, L, H, -1)
        keys = self.key_projection(keys).view(B, L, H, -1)
        values = self.value_projection(values).view(B, L, H, -1)
        sigma = self.sigma_projection(queries.view(B, L, D)).view(B, L, H)  # ä¿®å¤ï¼šç›´æ¥ä»åŸå§‹queriesæŠ•å½±

        out, series, prior, sigma = self.inner_attention(
            queries, keys, values, sigma, attn_mask
        )
        out = out.view(B, L, -1)

        return self.out_projection(out), series, prior, sigma


class TokenEmbedding(nn.Module):
    """TokenåµŒå…¥å±‚"""
    def __init__(self, c_in, d_model):
        super(TokenEmbedding, self).__init__()
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)

    def forward(self, x):
        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        return x


class PositionalEmbedding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEmbedding, self).__init__()
        pe = torch.zeros(max_len, d_model).float()
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1)
        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return self.pe[:, :x.size(1)]


class DataEmbedding(nn.Module):
    """æ•°æ®åµŒå…¥"""
    def __init__(self, c_in, d_model, dropout=0.1):
        super(DataEmbedding, self).__init__()
        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)
        self.position_embedding = PositionalEmbedding(d_model=d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        x = self.value_embedding(x) + self.position_embedding(x)
        return self.dropout(x)


class EncoderLayer(nn.Module):
    """ç¼–ç å™¨å±‚"""
    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None):
        new_x, attn, prior, sigma = self.attention(x, x, x, attn_mask=attn_mask)
        x = x + self.dropout(new_x)
        y = x = self.norm1(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))

        return self.norm2(x + y), attn, prior, sigma


class Encoder(nn.Module):
    """ç¼–ç å™¨ - èåˆMambaå’Œæ³¨æ„åŠ›"""
    def __init__(self, attn_layers, norm_layer=None, d_model=512):
        super(Encoder, self).__init__()
        self.attn_layers = nn.ModuleList(attn_layers)
        self.norm = norm_layer
        
        # Mambaç»„ä»¶
        self.mamba = SimplifiedMamba(d_model=d_model)
        self.gate = nn.Linear(d_model * 2, d_model)  # é—¨æ§èåˆ

    def forward(self, x, attn_mask=None):
        series_list = []
        prior_list = []
        sigma_list = []
        original_x = x

        for attn_layer in self.attn_layers:
            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)
            
            # Mambaå¤„ç† + è·³è·ƒè¿æ¥
            try:
                x_mamba = self.mamba(x) + original_x
                
                # é—¨æ§èåˆ - ç¡®ä¿ç»´åº¦åŒ¹é…
                if x.shape == x_mamba.shape:
                    gate_input = torch.cat((x, x_mamba), dim=-1)
                    gate = torch.sigmoid(self.gate(gate_input))
                    x = gate * x_mamba + (1 - gate) * x
                else:
                    # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåªä½¿ç”¨æ³¨æ„åŠ›è¾“å‡º
                    x = x
                    
            except Exception as e:
                # å¦‚æœMambaå¤„ç†å¤±è´¥ï¼Œåªä½¿ç”¨æ³¨æ„åŠ›è¾“å‡º
                print(f"[WARNING] Mambaèåˆå¤±è´¥: {e}, ä»…ä½¿ç”¨æ³¨æ„åŠ›è¾“å‡º")
                pass
            
            if self.norm is not None:
                x = self.norm(x)

            series_list.append(series)
            prior_list.append(prior)
            sigma_list.append(sigma)
            original_x = x

        return x, series_list, prior_list, sigma_list


class MAATModel(nn.Module):
    """MAATä¸»æ¨¡å‹ - æ¢å¤æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, win_size, enc_in, c_out, d_model=512, n_heads=8, e_layers=3, 
                 d_ff=512, dropout=0.1, activation='gelu', output_attention=True):
        super(MAATModel, self).__init__()
        self.output_attention = output_attention

        # æ•°æ®åµŒå…¥
        self.embedding = DataEmbedding(enc_in, d_model, dropout)

        # ç¼–ç å™¨
        self.encoder = Encoder(
            [
                EncoderLayer(
                    AttentionLayer(
                        AnomalyAttention(win_size, attention_dropout=dropout, 
                                       output_attention=output_attention),
                        d_model, n_heads),
                    d_model, d_ff, dropout=dropout, activation=activation
                ) for _ in range(e_layers)
            ],
            norm_layer=torch.nn.LayerNorm(d_model),
            d_model=d_model
        )

        self.projection = nn.Linear(d_model, c_out, bias=True)

    def forward(self, x):
        enc_out = self.embedding(x)
        enc_out, series, prior, sigmas = self.encoder(enc_out)
        enc_out = self.projection(enc_out)

        if self.output_attention:
            return enc_out, series, prior, sigmas
        else:
            return enc_out


class MAAT(BaseMethod):
    """MAATå¼‚å¸¸æ£€æµ‹æ–¹æ³• - å¿ å®åŸç‰ˆ"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None
        
        if params is None:
            params = {}
        
        # åŸç‰ˆMAATå‚æ•°é…ç½®
        self.config = {
            'window_size': params.get('window', 100),      # åŸå§‹MAATé»˜è®¤win_size=100
            'batch_size': params.get('batch_size', 128),   # åŸå§‹MAATé»˜è®¤batch_size=128
            'epochs': params.get('epochs', 10),            # åŸå§‹MAATé»˜è®¤num_epochs=10
            'learning_rate': params.get('lr', 1e-4),       # åŸå§‹MAATé»˜è®¤lr=1e-4
            'd_model': params.get('d_model', 512),          # å¢å¤§æ¨¡å‹å®¹é‡
            'n_heads': params.get('n_heads', 8),           # 8ä¸ªæ³¨æ„åŠ›å¤´
            'e_layers': params.get('e_layers', 3),         # åŸå§‹MAAT: e_layers=3
            'd_ff': params.get('d_ff', 2048),              # å¢å¤§å‰é¦ˆç½‘ç»œ
            'dropout': params.get('dropout', 0.1),
            'k': params.get('k', 3),                       # åŸå§‹MAAT: k=3 (å…³è”å·®å¼‚æƒé‡)
            'stride': params.get('stride', 1)
        }
        
        print(f"[LOG] MAATå¿ å®åŸç‰ˆåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[LOG] æ¢å¤æ ¸å¿ƒç‰¹æ€§: å¼‚å¸¸æ³¨æ„åŠ›ã€å…³è”å·®å¼‚å»ºæ¨¡ã€Mambaèåˆ")
    
    def train_valid_phase(self, tsTrain: MTSData):
        """è®­ç»ƒé˜¶æ®µ - æ¢å¤å…³è”å·®å¼‚å»ºæ¨¡"""
        print(f"\n[LOG] ========== MAATå¿ å®åŸç‰ˆè®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        
        print(f"[LOG] è®­ç»ƒæ•°æ®: {train_data.shape}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = MAATModel(
            win_size=self.config['window_size'],
            enc_in=self.n_features,
            c_out=self.n_features,
            d_model=self.config['d_model'],
            n_heads=self.config['n_heads'],
            e_layers=self.config['e_layers'],
            d_ff=self.config['d_ff'],
            dropout=self.config['dropout']
        ).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MAATDataset(train_data, self.config['window_size'], self.config['stride'])
        dataloader = DataLoader(dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=0)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['learning_rate'])
        criterion = nn.MSELoss()
        
        print(f"[LOG] å¼€å§‹å…³è”å·®å¼‚è®­ç»ƒï¼Œ{self.config['epochs']}è½®")
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss = 0
            num_batches = 0
            
            for batch_data, batch_targets in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
                try:
                    batch_data = batch_data.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    optimizer.zero_grad()
                    
                    # å‰å‘ä¼ æ’­
                    output, series, prior, _ = self.model(batch_data)
                    
                    # é‡æ„æŸå¤±
                    rec_loss = criterion(output, batch_targets)
                    
                    # å…³è”å·®å¼‚æŸå¤± - MAATçš„æ ¸å¿ƒ
                    association_loss = 0.0
                    
                    if series and prior:
                        valid_layers = 0
                        for u in range(len(prior)):
                            if series[u] is not None and prior[u] is not None:
                                try:
                                    # ç¡®ä¿serieså’Œprioréƒ½æ˜¯å½’ä¸€åŒ–çš„åˆ†å¸ƒ
                                    series_norm = series[u] / (torch.sum(series[u], dim=-1, keepdim=True) + 1e-8)
                                    prior_norm = prior[u] / (torch.sum(prior[u], dim=-1, keepdim=True) + 1e-8)
                                    
                                    # å…³è”å·®å¼‚ï¼šSeries(P) vs Prior(Q)
                                    kl_sp = my_kl_loss(series_norm, prior_norm.detach())
                                    kl_ps = my_kl_loss(prior_norm.detach(), series_norm)
                                    
                                    association_loss += (kl_sp + kl_ps)
                                    valid_layers += 1
                                    
                                except Exception as e:
                                    continue
                        
                        if valid_layers > 0:
                            association_loss = association_loss / valid_layers
                    
                    # æ€»æŸå¤±
                    if isinstance(association_loss, torch.Tensor):
                        total_batch_loss = rec_loss + self.config['k'] * association_loss
                    else:
                        total_batch_loss = rec_loss
                    
                    total_batch_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    optimizer.step()
                    
                    total_loss += total_batch_loss.item()
                    num_batches += 1
                    
                except Exception as e:
                    print(f"[WARNING] æ‰¹æ¬¡è®­ç»ƒé”™è¯¯: {e}")
                    continue
            
            avg_loss = total_loss / max(num_batches, 1)
            print(f"âœ… Epoch {epoch+1}: Loss = {avg_loss:.4f}")
        
        print(f"[LOG] ========== MAATå¿ å®åŸç‰ˆè®­ç»ƒå®Œæˆ ==========\n")
    
    def test_phase(self, tsData: MTSData):
        """æµ‹è¯•é˜¶æ®µ - æ¢å¤å…³è”å·®å¼‚åˆ†æ•°"""
        print(f"\n[LOG] ========== MAATå¿ å®åŸç‰ˆæµ‹è¯•å¼€å§‹ ==========")
        
        test_data = tsData.test
        test_dataset = MAATDataset(test_data, self.config['window_size'], stride=1)
        test_loader = DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=0)
        
        self.model.eval()
        scores = []
        
        with torch.no_grad():
            for batch_data, batch_targets in tqdm(test_loader, desc="MAATæµ‹è¯•"):
                try:
                    batch_data = batch_data.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    output, series, prior, _ = self.model(batch_data)
                    
                    # é‡æ„è¯¯å·®
                    rec_error = torch.mean((output - batch_targets) ** 2, dim=(1, 2))
                    
                    # å…³è”å·®å¼‚åˆ†æ•°
                    association_score = torch.zeros_like(rec_error)
                    
                    if series and prior:
                        assoc_scores = []
                        for i in range(len(series)):
                            if series[i] is not None and prior[i] is not None:
                                try:
                                    series_norm = series[i] / (torch.sum(series[i], dim=-1, keepdim=True) + 1e-8)
                                    prior_norm = prior[i] / (torch.sum(prior[i], dim=-1, keepdim=True) + 1e-8)
                                    
                                    kl_score = my_kl_loss(series_norm, prior_norm)
                                    if isinstance(kl_score, torch.Tensor):
                                        if kl_score.dim() == 0:
                                            assoc_scores.append(kl_score.expand(rec_error.shape[0]))
                                        else:
                                            assoc_scores.append(kl_score)
                                except:
                                    continue
                        
                        if assoc_scores:
                            try:
                                association_score = torch.stack(assoc_scores).mean(dim=0)
                            except:
                                association_score = torch.zeros_like(rec_error)
                    
                    # ç»„åˆåˆ†æ•°
                    combined_score = rec_error + self.config['k'] * association_score
                    scores.extend(combined_score.cpu().numpy())
                    
                except Exception as e:
                    print(f"[WARNING] æµ‹è¯•æ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
        
        # å¤„ç†åˆ†æ•°é•¿åº¦
        full_scores = np.zeros(len(test_data))
        if len(scores) > 0:
            full_scores[:self.config['window_size']-1] = scores[0] if scores else 0.0
            end_idx = min(len(scores), len(full_scores) - self.config['window_size'] + 1)
            if end_idx > 0:
                full_scores[self.config['window_size']-1:self.config['window_size']-1+end_idx] = scores[:end_idx]
        
        self.__anomaly_score = full_scores
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] ========== MAATå¿ å®åŸç‰ˆæµ‹è¯•å®Œæˆ ==========\n")
    
    def anomaly_score(self) -> np.ndarray:
        return self.__anomaly_score
    
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                MAAT (Mamba Adaptive Anomaly Transformer) å‚æ•°ç»Ÿè®¡ - å¿ å®åŸç‰ˆ:
                ==================================================
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                çª—å£å¤§å°: {self.config['window_size']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['learning_rate']}
                æ¨¡å‹ç»´åº¦: {self.config['d_model']}
                å…³è”å·®å¼‚æƒé‡: {self.config['k']}
                ==================================================
                æ¢å¤çš„MAATæ ¸å¿ƒç‰¹æ€§:
                âœ… å¼‚å¸¸æ³¨æ„åŠ›æœºåˆ¶ (è·ç¦»å…ˆéªŒåˆ†å¸ƒ)
                âœ… å…³è”å·®å¼‚å»ºæ¨¡ (KLæ•£åº¦æŸå¤±)
                âœ… MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ (é•¿åºåˆ—å»ºæ¨¡)
                âœ… é—¨æ§æ³¨æ„åŠ›èåˆ (ç‰¹å¾è‡ªé€‚åº”é€‰æ‹©)
                âœ… å®Œå…¨è‡ªåŒ…å«å®ç°
                ==================================================
            """
        else:
            param_info = "MAATæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== MAAT: å¿ å®åŸç‰ˆ - æ¢å¤æ ¸å¿ƒç‰¹æ€§ ==========")
    
    gctrl = TSADController()
    
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(dataset_type="MTS", dirname="./datasets", datasets=datasets)

    method = "MAAT"

    # å¿ å®åŸç‰ˆé…ç½® - åŸºäºåŸå§‹MAATé¡¹ç›®çš„SMDå‚æ•°è®¾ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            "window": 100,          # åŸå§‹MAAT: win_size=100
            "batch_size": 128,      # åŸå§‹MAAT: batch_size=128  
            "epochs": 10,           # åŸå§‹MAAT: num_epochs=10
            "lr": 1e-4,            # åŸå§‹MAAT: lr=1e-4
            "d_model": 512,         # å¢å¤§æ¨¡å‹ç»´åº¦ä»¥åŒ¹é…åŸå§‹MAATçš„å¤æ‚åº¦
            "n_heads": 8,          # æ¢å¤åˆ°8ä¸ªæ³¨æ„åŠ›å¤´
            "e_layers": 3,         # åŸå§‹MAAT: e_layers=3
            "d_ff": 2048,          # å¢å¤§å‰é¦ˆç½‘ç»œ
            "dropout": 0.1,        
            "k": 3,                # åŸå§‹MAAT: k=3 (å…³è”å·®å¼‚æƒé‡)
            "stride": 1
        },
        preprocess="z-score",
    )
       
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== MAATå¿ å®åŸç‰ˆæ‰§è¡Œå®Œæ¯• ==========") 