import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")

# ============= é«˜æ•ˆæ•°æ®é›†ç±»å®šä¹‰ =============
class FastMTSDataset(torch.utils.data.Dataset):
    def __init__(self, tsData: MTSData, set_type: str, window: int, horize: int):
        assert set_type in ['train', 'test']
        self.set_type = set_type
        self.window = window
        self.horize = horize        
        
        if set_type == "train":
            rawdata = tsData.train
        elif set_type == "test":
            rawdata = tsData.test
        else:
            raise ValueError('Arg "set_type" in MTSDataset() must be one of "train", "test"')

        self.len, self.var_num = rawdata.shape
        self.sample_num = max(self.len - self.window - self.horize + 1, 0)
        
        # æ•°æ®é‡‡æ ·ç­–ç•¥ - å¦‚æœæ•°æ®å¤ªå¤§å°±é‡‡æ ·
        if self.sample_num > 30000:  # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡
            indices = np.random.choice(self.sample_num, 30000, replace=False)
            indices.sort()
            self.selected_indices = indices
            self.sample_num = len(indices)
            print(f"ğŸ”§ [LOG] æ•°æ®é‡‡æ ·: {len(indices)} æ ·æœ¬")
        else:
            self.selected_indices = None
        
        # é¢„è®¡ç®—æ‰€æœ‰æ ·æœ¬
        self.samples, self.labels = self.__precompute_samples(rawdata)

    def __precompute_samples(self, data):
        if self.selected_indices is not None:
            actual_sample_num = len(self.selected_indices)
        else:
            actual_sample_num = self.sample_num
            
        X = torch.zeros((actual_sample_num, self.window, self.var_num), dtype=torch.float32)
        Y = torch.zeros((actual_sample_num, 1, self.var_num), dtype=torch.float32)

        for idx in range(actual_sample_num):
            if self.selected_indices is not None:
                i = self.selected_indices[idx]
            else:
                i = idx
                
            start = i
            end = i + self.window
            X[idx, :, :] = torch.from_numpy(data[start:end, :]).float()
            Y[idx, :, :] = torch.from_numpy(data[end+self.horize-1, :]).float()

        return X, Y

    def __len__(self):
        return self.sample_num

    def __getitem__(self, idx):
        return self.samples[idx, :, :], self.labels[idx, :, :]

# ============= ç®€åŒ–å¿«é€Ÿä¸“å®¶ç½‘ç»œ =============
class FastExpert(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, drop_out=0.1):
        super(FastExpert, self).__init__()
        # ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚ï¼Œä¸ç”¨å·ç§¯
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(inplace=True),
            nn.Dropout(drop_out),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(drop_out),
            nn.Linear(hidden_size // 2, output_size)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        # x shape: [batch, window, features] -> flatten
        x_flat = x.view(x.size(0), -1)
        return self.net(x_flat)

# ============= ç®€åŒ–é—¨æ§ç½‘ç»œ =============    
class FastGate(nn.Module):
    def __init__(self, input_size, num_experts):
        super(FastGate, self).__init__()
        self.gate = nn.Sequential(
            nn.Linear(input_size, num_experts),
            nn.Softmax(dim=1)
        )
        
        # æƒé‡åˆå§‹åŒ–
        nn.init.xavier_normal_(self.gate[0].weight)
        nn.init.constant_(self.gate[0].bias, 0)
        
    def forward(self, x):
        x_flat = x.view(x.size(0), -1)
        return self.gate(x_flat)

# ============= ç®€åŒ–å¡”ç½‘ç»œ =============
class FastTower(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=32):
        super(FastTower, self).__init__()
        self.tower = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size, output_size)
        )
        
        # æƒé‡åˆå§‹åŒ–
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.tower(x)

# ============= NewLMMoEå¿«é€Ÿä¼˜åŒ–æ¨¡å‹ =============
class FastNewLMMoEModel(nn.Module):
    def __init__(self, window_size, n_features, num_experts=3, expert_hidden=64, expert_output=32, tower_hidden=16):
        super(FastNewLMMoEModel, self).__init__()
        self.window_size = window_size
        self.n_features = n_features
        self.num_experts = num_experts
        self.expert_output = expert_output
        
        input_size = window_size * n_features
        
        # å¿«é€Ÿä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            FastExpert(input_size, expert_hidden, expert_output)
            for _ in range(num_experts)
        ])
        
        # ç®€åŒ–é—¨æ§ç½‘ç»œ - ä¸ºæ¯ä¸ªç‰¹å¾ç»´åº¦åˆ›å»ºé—¨æ§
        self.gates = nn.ModuleList([
            FastGate(input_size, num_experts)
            for _ in range(n_features)
        ])
        
        # å¿«é€Ÿå¡”ç½‘ç»œ
        self.towers = nn.ModuleList([
            FastTower(expert_output, 1, tower_hidden)
            for _ in range(n_features)
        ])

    def forward(self, x):
        batch_size = x.size(0)
        
        # è®¡ç®—æ‰€æœ‰ä¸“å®¶è¾“å‡º
        expert_outputs = []
        for expert in self.experts:
            expert_out = expert(x)  # [batch, expert_output]
            expert_outputs.append(expert_out)
        
        expert_outputs = torch.stack(expert_outputs, dim=2)  # [batch, expert_output, num_experts]
        
        # ä¸ºæ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—è¾“å‡º
        tower_outputs = []
        for i in range(self.n_features):
            # è®¡ç®—é—¨æ§æƒé‡
            gate_weights = self.gates[i](x)  # [batch, num_experts]
            gate_weights = gate_weights.unsqueeze(1)  # [batch, 1, num_experts]
            
            # åŠ æƒç»„åˆä¸“å®¶è¾“å‡º
            combined_output = torch.sum(expert_outputs * gate_weights, dim=2)  # [batch, expert_output]
            
            # é€šè¿‡å¡”ç½‘ç»œ
            tower_output = self.towers[i](combined_output)  # [batch, 1]
            tower_outputs.append(tower_output)
        
        # åˆå¹¶è¾“å‡º
        final_output = torch.stack(tower_outputs, dim=2)  # [batch, 1, n_features]
        
        return final_output

# ============= NewLMMoEå¿«é€Ÿå¼‚å¸¸æ£€æµ‹æ–¹æ³•ç±» =============
class NewLMMoE(BaseMethod):
    def __init__(self, params: dict) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.model = None
        
        # NewLMMoEå¿«é€Ÿé…ç½® - å‚è€ƒCADçš„é«˜æ•ˆè®¾ç½®
        self.config = {
            'window': 16,
            'batch_size': 64,        # ä¿æŒè¾ƒå¤§æ‰¹é‡
            'epochs': 3,             # ğŸš€ è¿›ä¸€æ­¥å‡å°‘è®­ç»ƒè½®æ•°
            'lr': 0.001,
            
            'num_experts': 3,        # ä¿æŒ3ä¸ªä¸“å®¶
            'expert_hidden': 48,     # ğŸ”§ è¿›ä¸€æ­¥å‡å°‘ 96â†’48
            'expert_output': 24,     # ğŸ”§ å‡å°‘è¾“å‡ºç»´åº¦ 32â†’24
            'tower_hidden': 16,      # ä¿æŒå¡”ç½‘ç»œå¤§å°
        }
        
        print(f"ğŸš€ [LOG] NewLMMoEå¿«é€Ÿç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆ")
        print(f"âš¡ [LOG] é«˜æ•ˆé…ç½®: {self.config['num_experts']}ä¸“å®¶, {self.config['epochs']}è½®è®­ç»ƒ")
        print(f"ğŸ¯ [LOG] è®¾è®¡ç†å¿µ: ç®€åŒ–æ¶æ„ï¼Œæé€Ÿè®­ç»ƒï¼Œä¿æŒæ€§èƒ½")

    def train_valid_phase(self, tsData: MTSData):
        print(f"\nâš¡ [LOG] ========== NewLMMoEå¿«é€Ÿè®­ç»ƒå¼€å§‹ ==========")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ [LOG] ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å¿«é€Ÿæ•°æ®åŠ è½½
        train_dataset = FastMTSDataset(tsData=tsData, set_type='train', 
                                      window=self.config['window'], horize=1)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True,
            num_workers=0,  # é¿å…å¤šçº¿ç¨‹é—®é¢˜
            pin_memory=False  # ç®€åŒ–å†…å­˜ç®¡ç†
        )
        
        n_features = tsData.train.shape[1]
        print(f"ğŸ“Š [LOG] æ•°æ®ç»´åº¦: {n_features}, è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        
        # åˆ›å»ºå¿«é€Ÿæ¨¡å‹
        self.model = FastNewLMMoEModel(
            window_size=self.config['window'],
            n_features=n_features,
            num_experts=self.config['num_experts'],
            expert_hidden=self.config['expert_hidden'],
            expert_output=self.config['expert_output'],
            tower_hidden=self.config['tower_hidden']
        ).to(device)
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['lr'])
        criterion = nn.MSELoss()
        
        self.model.train()
        print(f"âš¡ [LOG] å¼€å§‹å¿«é€Ÿè®­ç»ƒï¼Œå…± {self.config['epochs']} ä¸ªepoch")
        print(f"ğŸ—ï¸ [LOG] æ¨¡å‹å‚æ•°: experts={self.config['num_experts']}, expert_hidden={self.config['expert_hidden']}")
        
        # å¿«é€Ÿè®­ç»ƒå¾ªç¯
        for epoch in range(self.config['epochs']):
            total_loss = 0
            batch_count = 0
            
            progress_bar = tqdm(train_loader, desc=f"âš¡ Epoch {epoch+1}/{self.config['epochs']}", ncols=80)
            
            for batch_idx, (data, target) in enumerate(progress_bar):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
                
                progress_bar.set_postfix({'loss': f'{loss.item():.6f}'})
            
            avg_loss = total_loss / batch_count if batch_count > 0 else 0
            print(f"âœ… Epoch {epoch+1}/{self.config['epochs']}, Average Loss: {avg_loss:.6f}")
        
        print(f"ğŸ‰ [LOG] ========== NewLMMoEå¿«é€Ÿè®­ç»ƒå®Œæˆ ==========\n")

    def test_phase(self, tsData: MTSData):
        print(f"\nğŸ” [LOG] ========== NewLMMoEå¿«é€Ÿæµ‹è¯•å¼€å§‹ ==========")
        print(f"ğŸ“Š [LOG] æµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å¿«é€Ÿæµ‹è¯•æ•°æ®åŠ è½½
        test_dataset = FastMTSDataset(tsData=tsData, set_type='test', 
                                     window=self.config['window'], horize=1)
        test_loader = DataLoader(
            test_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=False,
            num_workers=0
        )
        
        self.model.eval()
        anomaly_scores = []
        
        print(f"ğŸ” [LOG] å¼€å§‹å¿«é€Ÿæµ‹è¯•ï¼Œå…± {len(test_loader)} ä¸ªbatch")
        
        with torch.no_grad():
            for data, target in tqdm(test_loader, desc="ğŸ” å¿«é€Ÿæµ‹è¯•", ncols=80):
                data, target = data.to(device), target.to(device)
                output = self.model(data)
                mse = torch.mean((output - target) ** 2, dim=(1, 2))
                anomaly_scores.extend(mse.cpu().numpy())
        
        # å¤„ç†å¼‚å¸¸åˆ†æ•°é•¿åº¦
        scores = np.array(anomaly_scores)
        full_scores = np.zeros(len(tsData.test))
        
        # å¡«å……å‰é¢çš„æ—¶é—´æ­¥
        if len(scores) < len(tsData.test):
            pad_length = len(tsData.test) - len(scores)
            avg_score = np.mean(scores) if len(scores) > 0 else 0
            full_scores = np.concatenate([np.full(pad_length, avg_score), scores])
        else:
            full_scores = scores[:len(tsData.test)]
        
        # å½’ä¸€åŒ–
        if len(full_scores) > 0 and np.max(full_scores) > np.min(full_scores):
            full_scores = (full_scores - np.min(full_scores)) / (np.max(full_scores) - np.min(full_scores))
        
        self.__anomaly_score = full_scores
        print(f"ğŸ‰ [LOG] ========== NewLMMoEå¿«é€Ÿæµ‹è¯•å®Œæˆ ==========\n")

    def anomaly_score(self) -> np.ndarray:
        return self.__anomaly_score

    def param_statistic(self, save_file):
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                NewLMMoEå¿«é€Ÿç‰ˆæœ¬æ¨¡å‹å‚æ•°ç»Ÿè®¡:
                ==================================================
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                ä¸“å®¶æ•°é‡: {self.config['num_experts']}
                ä¸“å®¶éšè—å±‚: {self.config['expert_hidden']}
                ä¸“å®¶è¾“å‡ºç»´åº¦: {self.config['expert_output']}
                å¡”ç½‘ç»œéšè—å±‚: {self.config['tower_hidden']}
                çª—å£å¤§å°: {self.config['window']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['lr']}

                âš¡ å¿«é€Ÿä¼˜åŒ–ç‰¹æ€§:
                - çº¿æ€§å±‚æ¶æ„ (å»æ‰å·ç§¯)
                - æ•°æ®é‡‡æ ·åŠ é€Ÿ
                - ç®€åŒ–é—¨æ§æœºåˆ¶
                - è½»é‡åŒ–ä¸“å®¶ç½‘ç»œ
                - å¿«é€Ÿè®­ç»ƒç­–ç•¥

                ==================================================
                è®¾è®¡ç†å¿µ: ç®€åŒ–æ¶æ„ + æé€Ÿè®­ç»ƒ + ä¿æŒæ€§èƒ½
            """
        else:
            param_info = "NewLMMoEæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)

# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("âš¡ ========== NewLMMoEè½»é‡çº§å¤šä¸“å®¶æ··åˆå¼‚å¸¸æ£€æµ‹å¿«é€Ÿç‰ˆ ==========")
    print("ğŸš€ [LOG] ç¨‹åºå¼€å§‹æ‰§è¡Œ")
    
    # Create a global controller
    gctrl = TSADController()
    print("ğŸ”§ [LOG] TSADControllerå·²åˆ›å»º")
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )
    print("ğŸ“Š [LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    print("ğŸ—ï¸ [LOG] NewLMMoEç±»å®šä¹‰å®Œæˆ")

    """============= Run NewLMMoE Fast algo. ============="""
    
    method = "NewLMMoE"

    print(f"âš¡ [LOG] å¼€å§‹è¿è¡Œå¿«é€Ÿå®éªŒï¼Œmethod={method}")
    # run models with fast hyperparameters
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            "window": 16,
            "batch_size": 64,
            "epochs": 3,         # âš¡ è¶…å¿«è®­ç»ƒ
            "lr": 0.001,
        },
        preprocess="z-score",
    )
    print("ğŸ‰ [LOG] å¿«é€Ÿå®éªŒè¿è¡Œå®Œæˆ")
       
        
    """============= [EVALUATION SETTINGS] ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    print("ğŸ“Š [LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals(
        [
            PointF1PA(),
            EventF1PA(),
            EventF1PA(mode="squeeze")
        ]
    )
    print("âœ… [LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("ğŸ” [LOG] å¼€å§‹æ‰§è¡Œè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema="mts"
    )
    print("ğŸ‰ [LOG] è¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
        
    """============= [PLOTTING SETTINGS] ============="""
    
    print("ğŸ“ˆ [LOG] å¼€å§‹ç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema="mts"
    )
    print("ğŸ¨ [LOG] ç»˜å›¾å®Œæˆ")
    
    print("âš¡ ========== NewLMMoEå¿«é€Ÿç‰ˆæ‰§è¡Œå®Œæ¯• ==========")