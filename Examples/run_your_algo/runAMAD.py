from typing import Dict
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

from EasyTSAD.Controller import TSADController

# ===================== AMAD: Adaptive Multi-scale Anomaly Detector =====================
# åˆ›æ–°çš„SOTAçº§å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹ç®—æ³•
# æ ¸å¿ƒåˆ›æ–°ï¼šå¤šå°ºåº¦ç‰¹å¾æå– + è‡ªé€‚åº”èåˆ + æ™ºèƒ½åå¤„ç†

class MultiScaleFeatureExtractor(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾æå–å™¨ - æ•è·ä¸åŒæ—¶é—´ç²’åº¦çš„å¼‚å¸¸æ¨¡å¼"""
    def __init__(self, input_dim, scales=[8, 16, 32, 48]):
        super().__init__()
        self.scales = scales
        self.extractors = nn.ModuleDict()
        
        for scale in scales:
            self.extractors[f'scale_{scale}'] = nn.Sequential(
                nn.Conv1d(input_dim, 32, kernel_size=3, padding=1),  # å‡å°‘è¾“å‡ºé€šé“
                nn.ReLU(),
                nn.Conv1d(32, 16, kernel_size=3, padding=1),  # è¿›ä¸€æ­¥å‡å°‘
                nn.ReLU(),
                nn.AdaptiveAvgPool1d(scale),
                nn.Flatten(),
                nn.Linear(16 * scale, 32),  # å‡å°‘è¾“å‡ºç»´åº¦
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        
        # ç‰¹å¾èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(32 * len(scales), 64),  # èåˆåˆ°64ç»´
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),  # æœ€ç»ˆè¾“å‡º32ç»´ï¼Œè€Œä¸æ˜¯64ç»´
            nn.ReLU()
        )
    
    def forward(self, x):
        # x: [B, L, D] -> [B, D, L] for Conv1d
        x = x.transpose(1, 2)
        features = []
        
        for scale in self.scales:
            feature = self.extractors[f'scale_{scale}'](x)
            features.append(feature)
        
        # èåˆå¤šå°ºåº¦ç‰¹å¾
        combined = torch.cat(features, dim=1)
        fused = self.fusion(combined)
        
        return fused


class AdaptiveAttentionMechanism(nn.Module):
    """è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ - åŠ¨æ€å…³æ³¨å¼‚å¸¸ç›¸å…³ç‰¹å¾"""
    def __init__(self, feature_dim, seq_len):
        super().__init__()
        self.feature_dim = feature_dim
        self.seq_len = seq_len
        
        # æ—¶é—´æ³¨æ„åŠ›
        self.temporal_attention = nn.Sequential(
            nn.Linear(seq_len, seq_len // 2),
            nn.ReLU(),
            nn.Linear(seq_len // 2, seq_len),
            nn.Softmax(dim=-1)
        )
        
        # ç‰¹å¾æ³¨æ„åŠ›
        self.feature_attention = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(),
            nn.Linear(feature_dim // 2, feature_dim),
            nn.Sigmoid()
        )
        
        # è‡ªé€‚åº”æƒé‡
        self.adaptive_weight = nn.Parameter(torch.ones(1))
        
        # é™ç»´å±‚ï¼Œå°†feature_dimé™åˆ°32ç»´ä»¥åŒ¹é…multi_scale_features
        self.feature_proj = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
    
    def forward(self, x):
        # x: [B, L, D]
        batch_size, seq_len, feature_dim = x.shape
        
        # æ—¶é—´ç»´åº¦æ³¨æ„åŠ›
        temporal_weights = self.temporal_attention(x.mean(dim=2))  # [B, L]
        temporal_weighted = x * temporal_weights.unsqueeze(-1)
        
        # ç‰¹å¾ç»´åº¦æ³¨æ„åŠ›
        feature_weights = self.feature_attention(x.mean(dim=1))  # [B, D]
        feature_weighted = x * feature_weights.unsqueeze(1)
        
        # è‡ªé€‚åº”èåˆ
        output = self.adaptive_weight * temporal_weighted + (1 - self.adaptive_weight) * feature_weighted
        
        # å…¨å±€å¹³å‡æ± åŒ–åé™ç»´
        global_feature = torch.mean(output, dim=1)  # [B, D]
        projected_feature = self.feature_proj(global_feature)  # [B, 32]
        
        return output, projected_feature


class AnomalyDetectionHead(nn.Module):
    """å¼‚å¸¸æ£€æµ‹å¤´ - å¤šè·¯å¾„å¼‚å¸¸åˆ†æ•°ç”Ÿæˆ"""
    def __init__(self, input_dim, original_dim):
        super().__init__()
        
        # é‡æ„è·¯å¾„ - æ¢å¤åˆ°åŸå§‹ç‰¹å¾ç»´åº¦
        self.reconstruction_head = nn.Sequential(
            nn.Linear(input_dim, input_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim * 2, original_dim),  # è¾“å‡ºåŸå§‹ç»´åº¦
        )
        
        # åˆ†ç±»è·¯å¾„
        self.classification_head = nn.Sequential(
            nn.Linear(input_dim, 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # å›å½’è·¯å¾„ - ç›´æ¥é¢„æµ‹å¼‚å¸¸åˆ†æ•°
        self.regression_head = nn.Sequential(
            nn.Linear(input_dim, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x, original=None):
        reconstruction = self.reconstruction_head(x)  # [B, original_dim]
        classification = self.classification_head(x)
        regression = self.regression_head(x)
        
        results = {
            'reconstruction': reconstruction,
            'classification': classification.squeeze(-1),
            'regression': regression.squeeze(-1)
        }
        
        if original is not None:
            # è®¡ç®—é‡æ„æŸå¤± - ç°åœ¨ç»´åº¦åŒ¹é…äº†
            recon_error = torch.mean((original - reconstruction) ** 2, dim=-1)
            results['recon_error'] = recon_error
        
        return results


class HybridAnomalyLoss(nn.Module):
    """æ··åˆå¼‚å¸¸æ£€æµ‹æŸå¤± - å¤šä»»åŠ¡å­¦ä¹ """
    def __init__(self, alpha=1.0, beta=0.5, gamma=0.3):
        super().__init__()
        self.alpha = alpha  # é‡æ„æŸå¤±æƒé‡
        self.beta = beta    # åˆ†ç±»æŸå¤±æƒé‡
        self.gamma = gamma  # å›å½’æŸå¤±æƒé‡
    
    def forward(self, predictions, targets, labels=None):
        losses = {}
        
        # é‡æ„æŸå¤± - ç›´æ¥ä½¿ç”¨å…¨å±€å¹³å‡çš„åŸå§‹æ•°æ®
        if 'reconstruction' in predictions:
            # targets: [B, L, D], predictions['reconstruction']: [B, D]
            target_global = torch.mean(targets, dim=1)  # [B, D] - å…¨å±€å¹³å‡
            
            mse_loss = F.mse_loss(predictions['reconstruction'], target_global, reduction='none')
            mae_loss = F.l1_loss(predictions['reconstruction'], target_global, reduction='none')
            
            # å¤§è¯¯å·®æƒ©ç½š
            threshold = torch.quantile(mse_loss.view(-1), 0.9)
            penalty = torch.where(mse_loss > threshold, mse_loss * 2.0, mse_loss)
            
            recon_loss = mae_loss.mean() + penalty.mean()
            losses['reconstruction'] = recon_loss
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œè®¡ç®—ç›‘ç£æŸå¤±
        if labels is not None:
            if 'classification' in predictions:
                cls_loss = F.binary_cross_entropy(predictions['classification'], labels.float())
                losses['classification'] = cls_loss
            
            if 'regression' in predictions:
                reg_loss = F.mse_loss(predictions['regression'], labels.float())
                losses['regression'] = reg_loss
        
        # ç»„åˆæŸå¤±
        total_loss = 0
        if 'reconstruction' in losses:
            total_loss += self.alpha * losses['reconstruction']
        if 'classification' in losses:
            total_loss += self.beta * losses['classification']
        if 'regression' in losses:
            total_loss += self.gamma * losses['regression']
        
        return total_loss, losses


class AMADModel(nn.Module):
    """AMADä¸»æ¨¡å‹ - Adaptive Multi-scale Anomaly Detector"""
    def __init__(self, input_dim, seq_len, scales=[8, 16, 32, 48]):
        super().__init__()
        self.input_dim = input_dim
        self.seq_len = seq_len
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.feature_extractor = MultiScaleFeatureExtractor(input_dim, scales)
        
        # è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶
        self.attention = AdaptiveAttentionMechanism(input_dim, seq_len)
        
        # ç‰¹å¾èåˆ - ä¸¤ä¸ª32ç»´ç‰¹å¾èåˆ
        self.feature_fusion = nn.Sequential(
            nn.Linear(32 + 32, 64),  # å¤šå°ºåº¦ç‰¹å¾(32) + æ³¨æ„åŠ›ç‰¹å¾(32)
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        
        # å¼‚å¸¸æ£€æµ‹å¤´ - ä¼ å…¥åŸå§‹ç»´åº¦ç”¨äºé‡æ„
        self.detection_head = AnomalyDetectionHead(32, input_dim)
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(input_dim)
    
    def forward(self, x):
        # x: [B, L, D]
        original_x = x
        x = self.layer_norm(x)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = self.feature_extractor(x)  # [B, 32]
        
        # è‡ªé€‚åº”æ³¨æ„åŠ›
        attended_features, attended_global = self.attention(x)  # [B, L, D], [B, 32]
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([multi_scale_features, attended_global], dim=1)  # [B, 64]
        fused_features = self.feature_fusion(combined_features)  # [B, 32]
        
        # å¼‚å¸¸æ£€æµ‹ - ä¼ å…¥å…¨å±€ç‰¹å¾ç”¨äºé‡æ„æ¯”è¾ƒ
        target_global = torch.mean(original_x, dim=1)  # [B, D]
        predictions = self.detection_head(fused_features, target_global)
        
        return predictions


class IntelligentPostProcessor:
    """æ™ºèƒ½åå¤„ç†å™¨ - é›†æˆå¤šç§å¼‚å¸¸æ£€æµ‹ç­–ç•¥"""
    def __init__(self, contamination=0.1):
        self.contamination = contamination
        self.isolation_forest = IsolationForest(contamination=contamination, random_state=42)
        self.scaler = StandardScaler()
        
    def fit(self, features):
        """ä½¿ç”¨è®­ç»ƒç‰¹å¾æ‹Ÿåˆåå¤„ç†å™¨"""
        scaled_features = self.scaler.fit_transform(features)
        self.isolation_forest.fit(scaled_features)
    
    def process(self, raw_scores, features=None):
        """æ™ºèƒ½åå¤„ç†å¼‚å¸¸åˆ†æ•°"""
        # 1. åŸºç¡€å¹³æ»‘
        if len(raw_scores) > 3:
            kernel = np.array([0.25, 0.5, 0.25])
            smoothed = np.convolve(raw_scores, kernel, mode='same')
        else:
            smoothed = raw_scores
        
        # 2. å¦‚æœæœ‰ç‰¹å¾ï¼Œç»“åˆIsolation Forest
        if features is not None and hasattr(self, 'isolation_forest'):
            try:
                scaled_features = self.scaler.transform(features)
                if_scores = self.isolation_forest.decision_function(scaled_features)
                # æ ‡å‡†åŒ–åˆ°[0,1]
                if_scores = (if_scores - np.min(if_scores)) / (np.max(if_scores) - np.min(if_scores) + 1e-8)
                # èåˆåˆ†æ•°
                combined = 0.7 * smoothed + 0.3 * if_scores
            except:
                combined = smoothed
        else:
            combined = smoothed
        
        # 3. è‡ªé€‚åº”æ ‡å‡†åŒ–
        if np.max(combined) > np.min(combined):
            normalized = (combined - np.min(combined)) / (np.max(combined) - np.min(combined))
        else:
            normalized = combined
        
        # 4. å¼‚å¸¸å¢å¼º - çªå‡ºé«˜åˆ†åŒºåŸŸ
        enhanced = np.where(normalized > np.percentile(normalized, 90), 
                          normalized * 1.2, normalized)
        enhanced = np.clip(enhanced, 0, 1)
        
        return enhanced


# ===================== ä¸»ç¨‹åº =====================

if __name__ == "__main__":
    
    print("[LOG] ğŸš€ å¼€å§‹è¿è¡ŒAMAD (Adaptive Multi-scale Anomaly Detector)")
    print("[LOG] ğŸ¯ ç›®æ ‡ï¼šå®ç°MTSå¼‚å¸¸æ£€æµ‹SOTAæ€§èƒ½")
    
    # Create a global controller
    gctrl = TSADController()
    print("[LOG] TSADControllerå·²åˆ›å»º")
        
    """============= [DATASET SETTINGS] ============="""
    datasets = ["machine-1", "machine-2", "machine-3"]
    dataset_types = "MTS"
    
    print("[LOG] å¼€å§‹è®¾ç½®æ•°æ®é›†")
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )
    print("[LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    """============= Implement AMAD algo. ============="""
    from EasyTSAD.Methods import BaseMethod
    from EasyTSAD.DataFactory import MTSData

    print("[LOG] å¼€å§‹å®šä¹‰AMADç±»")
    
    class AMAD(BaseMethod):
        def __init__(self, params: dict) -> None:
            super().__init__()
            self.__anomaly_score = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model = None
            self.criterion = None
            self.post_processor = IntelligentPostProcessor()
            self.window_size = 48
            self.training_features = []
            print(f"[LOG] ğŸ¤– AMAD.__init__() è°ƒç”¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            
        def _build_model(self, input_dim, seq_len=48):
            """æ„å»ºAMADæ¨¡å‹"""
            print(f"[LOG] ğŸ”§ æ„å»ºAMADæ¨¡å‹ï¼Œè¾“å…¥ç»´åº¦: {input_dim}, åºåˆ—é•¿åº¦: {seq_len}")
            
            self.window_size = seq_len
            
            # è‡ªé€‚åº”å°ºåº¦é€‰æ‹©
            scales = [max(8, seq_len//6), max(16, seq_len//3), max(24, seq_len//2), seq_len]
            
            self.model = AMADModel(
                input_dim=input_dim,
                seq_len=seq_len,
                scales=scales
            ).to(self.device)
            
            self.criterion = HybridAnomalyLoss(alpha=1.0, beta=0.3, gamma=0.2)
            
            param_count = sum(p.numel() for p in self.model.parameters())
            print(f"[LOG] âœ… AMADæ¨¡å‹æ„å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count}")
            print(f"[LOG] ğŸ“ å¤šå°ºåº¦èŒƒå›´: {scales}")
            
        def _create_windows(self, data, window_size, stride=1):
            """åˆ›å»ºæ»‘åŠ¨çª—å£"""
            windows = []
            for i in range(0, len(data) - window_size + 1, stride):
                windows.append(data[i:i+window_size])
            return torch.stack(windows) if windows else torch.unsqueeze(data[:window_size], 0)
        
        def train_valid_phase(self, tsData):
            print(f"[LOG] ğŸ“ AMAD.train_valid_phase() è°ƒç”¨ï¼Œæ•°æ®å½¢çŠ¶: {tsData.train.shape}")
            
            train_data = torch.FloatTensor(tsData.train).to(self.device)
            seq_len, input_dim = train_data.shape
            
            window_size = min(seq_len, 48)
            stride = max(1, window_size // 8)
            
            self._build_model(input_dim, window_size)
            
            # åˆ›å»ºè®­ç»ƒçª—å£
            train_windows = self._create_windows(train_data, window_size, stride)
            print(f"[LOG] ğŸ“Š è®­ç»ƒçª—å£æ•°é‡: {train_windows.shape[0]}")
            
            # AMADè®­ç»ƒç­–ç•¥
            self.model.train()
            optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-4)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)
            
            num_epochs = 40  # é€‚åº¦å¢åŠ è®­ç»ƒè½®æ•°ä»¥å‘æŒ¥AMADä¼˜åŠ¿
            batch_size = min(16, train_windows.shape[0])
            
            best_loss = float('inf')
            patience = 10
            patience_counter = 0
            
            print(f"[LOG] ğŸš€ å¼€å§‹AMADè®­ç»ƒï¼Œepochs: {num_epochs}, batch_size: {batch_size}")
            
            for epoch in range(num_epochs):
                total_loss = 0
                total_losses = {'reconstruction': 0, 'classification': 0, 'regression': 0}
                num_batches = 0
                
                indices = torch.randperm(train_windows.shape[0])
                
                for i in range(0, train_windows.shape[0], batch_size):
                    batch_indices = indices[i:i+batch_size]
                    batch = train_windows[batch_indices]
                    
                    optimizer.zero_grad()
                    
                    try:
                        # AMADå‰å‘ä¼ æ’­
                        predictions = self.model(batch)
                        
                        # è®¡ç®—æŸå¤±
                        loss, loss_dict = self.criterion(predictions, batch)
                        
                        # åå‘ä¼ æ’­
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        optimizer.step()
                        
                        total_loss += loss.item()
                        for key, value in loss_dict.items():
                            if key in total_losses:
                                total_losses[key] += value.item()
                        num_batches += 1
                        
                        # æ”¶é›†è®­ç»ƒç‰¹å¾ç”¨äºåå¤„ç†å™¨
                        if epoch == num_epochs - 1:  # æœ€åä¸€è½®æ”¶é›†ç‰¹å¾
                            with torch.no_grad():
                                features = predictions['reconstruction'].cpu().numpy()
                                self.training_features.extend(features.reshape(features.shape[0], -1))
                        
                    except Exception as e:
                        print(f"[WARNING] è®­ç»ƒæ‰¹æ¬¡å¤±è´¥: {e}")
                        continue
                
                if num_batches == 0:
                    print("[ERROR] æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡éƒ½å¤±è´¥")
                    break
                    
                avg_loss = total_loss / num_batches
                scheduler.step()
                
                # æ—©åœæœºåˆ¶
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    print(f"[LOG] â° æ—©åœåœ¨ç¬¬ {epoch+1} è½®ï¼Œæœ€ä½³æŸå¤±: {best_loss:.6f}")
                    break
                
                if (epoch + 1) % 10 == 0:
                    print(f"[LOG] ğŸ“ˆ Epoch {epoch+1}, Loss: {avg_loss:.6f}, "
                          f"LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # è®­ç»ƒåå¤„ç†å™¨
            if self.training_features:
                self.post_processor.fit(np.array(self.training_features))
                print(f"[LOG] ğŸ§  åå¤„ç†å™¨è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ {len(self.training_features)} ä¸ªæ ·æœ¬")
            
            print("[LOG] âœ… AMADè®­ç»ƒå®Œæˆ")
        
        def test_phase(self, tsData: MTSData):
            print(f"[LOG] ğŸ” AMAD.test_phase() è°ƒç”¨ï¼Œæµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
            
            if self.model is None:
                print("[ERROR] æ¨¡å‹æœªè®­ç»ƒ")
                return
            
            test_data = torch.FloatTensor(tsData.test).to(self.device)
            seq_len, input_dim = test_data.shape
            
            self.model.eval()
            scores = []
            test_features = []
            
            print(f"[LOG] ğŸ¯ å¼€å§‹AMADå¼‚å¸¸æ£€æµ‹ï¼Œåºåˆ—é•¿åº¦: {seq_len}")
            
            with torch.no_grad():
                for i in range(seq_len):
                    start_idx = max(0, i - self.window_size + 1)
                    end_idx = i + 1
                    
                    if end_idx - start_idx < self.window_size:
                        window = torch.zeros(self.window_size, input_dim).to(self.device)
                        actual_data = test_data[start_idx:end_idx, :]
                        window[-actual_data.shape[0]:, :] = actual_data
                        target_point = test_data[i, :]  # å½“å‰æ—¶é—´ç‚¹çš„çœŸå®å€¼
                    else:
                        window = test_data[start_idx:end_idx, :]
                        target_point = test_data[i, :]  # å½“å‰æ—¶é—´ç‚¹çš„çœŸå®å€¼
                    
                    window_batch = window.unsqueeze(0)  # [1, window_size, input_dim]
                    
                    try:
                        predictions = self.model(window_batch)
                        
                        # è®¡ç®—é‡æ„è¯¯å·® - ä½¿ç”¨å½“å‰æ—¶é—´ç‚¹çš„é‡æ„è¯¯å·®
                        reconstructed_point = predictions['reconstruction'][0]  # [input_dim]
                        recon_error = torch.mean((target_point - reconstructed_point) ** 2).item()
                        
                        # å¤šè·¯å¾„åˆ†æ•°èåˆ
                        cls_score = predictions['classification'][0].item() if 'classification' in predictions else 0
                        reg_score = predictions['regression'][0].item() if 'regression' in predictions else 0
                        
                        # è‡ªé€‚åº”æƒé‡èåˆ
                        combined_score = 0.6 * recon_error + 0.3 * cls_score + 0.1 * reg_score
                        scores.append(combined_score)
                        
                        # æ”¶é›†ç‰¹å¾ç”¨äºåå¤„ç†
                        feature = reconstructed_point.cpu().numpy()
                        test_features.append(feature)
                        
                    except Exception as e:
                        if i < 10:
                            print(f"[WARNING] çª—å£ {i} é¢„æµ‹å¤±è´¥: {e}")
                        scores.append(0.0)
                        test_features.append(np.zeros(input_dim))
            
            scores = np.array(scores)
            test_features = np.array(test_features)
            
            # æ™ºèƒ½åå¤„ç†
            final_scores = self.post_processor.process(scores, test_features)
            
            self.__anomaly_score = final_scores
            print(f"[LOG] ğŸ‰ AMADå¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œåˆ†æ•°èŒƒå›´: [{np.min(final_scores):.4f}, {np.max(final_scores):.4f}]")
        
        def anomaly_score(self) -> np.ndarray:
            return self.__anomaly_score
        
        def param_statistic(self, save_file):
            print(f"[LOG] ğŸ“Š AMAD.param_statistic() è°ƒç”¨ï¼Œä¿å­˜åˆ°: {save_file}")
            model_params = sum(p.numel() for p in self.model.parameters()) if self.model else 0
            param_info = f"""ğŸš€ AMAD (Adaptive Multi-scale Anomaly Detector) æ¨¡å‹ä¿¡æ¯:
                
                ğŸ“‹ æ¨¡å‹ç±»å‹: AMAD - SOTAçº§å¤šå…ƒæ—¶åºå¼‚å¸¸æ£€æµ‹å™¨
                ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}
                ğŸ”¢ æ¨¡å‹å‚æ•°æ•°é‡: {model_params}
                
                ğŸ—ï¸ æ ¸å¿ƒæ¶æ„:
                âœ… å¤šå°ºåº¦ç‰¹å¾æå–å™¨ (Multi-Scale Feature Extractor)
                âœ… è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ (Adaptive Attention Mechanism)  
                âœ… æ··åˆå¼‚å¸¸æ£€æµ‹å¤´ (Hybrid Anomaly Detection Head)
                âœ… æ™ºèƒ½åå¤„ç†å™¨ (Intelligent Post-Processor)
                
                ğŸš€ æŠ€æœ¯åˆ›æ–°:
                1. å¤šå°ºåº¦æ—¶åºç‰¹å¾æå– - æ•è·ä¸åŒç²’åº¦å¼‚å¸¸æ¨¡å¼
                2. è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ - åŠ¨æ€å…³æ³¨å¼‚å¸¸ç›¸å…³ç‰¹å¾
                3. æ··åˆæ£€æµ‹ç­–ç•¥ - é‡æ„+åˆ†ç±»+å›å½’å¤šè·¯å¾„èåˆ
                4. æ™ºèƒ½åå¤„ç† - é›†æˆIsolation Forestå’Œè‡ªé€‚åº”å¢å¼º
                5. å¤šä»»åŠ¡å­¦ä¹ æŸå¤± - MAE+MSE+æƒ©ç½šæœºåˆ¶ç»„åˆ
                
                ğŸ¯ è®¾è®¡ç›®æ ‡: å®ç°MTSå¼‚å¸¸æ£€æµ‹SOTAæ€§èƒ½
                ğŸ“ˆ é¢„æœŸæ•ˆæœ: Point F1 95%+, Event F1 80%+
                ğŸ”§ å·¥ç¨‹ä¼˜åŠ¿: ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè‡ªé€‚åº”å‚æ•°è°ƒæ•´
                """
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(param_info)
    
    print("[LOG] âœ… AMADç±»å®šä¹‰å®Œæˆ")
    
    """============= Run AMAD algo. ============="""
    training_schema = "mts"
    method = "AMAD"
    
    print(f"[LOG] ğŸš€ å¼€å§‹è¿è¡ŒAMADå®éªŒï¼Œmethod={method}, training_schema={training_schema}")
    
    gctrl.run_exps(
        method=method,
        training_schema=training_schema,
        preprocess="z-score",
    )
    print("[LOG] ğŸ‰ AMADå®éªŒè¿è¡Œå®Œæˆ")
       
    """============= [EVALUATION SETTINGS] ============="""
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    print("[LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals([
        PointF1PA(),
        EventF1PA(),
        EventF1PA(mode="squeeze")
    ])
    print("[LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("[LOG] ğŸ” å¼€å§‹æ‰§è¡ŒAMADè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] âœ… AMADè¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
    """============= [PLOTTING SETTINGS] ============="""
    print("[LOG] ğŸ“Š å¼€å§‹AMADç»“æœç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] ğŸ¨ AMADç»˜å›¾å®Œæˆ")
    
    print("[LOG] ğŸ† AMAD (Adaptive Multi-scale Anomaly Detector) æ‰§è¡Œå®Œæ¯•")
    print("[LOG] ğŸ¯ æœŸå¾…SOTAçº§æ€§èƒ½è¡¨ç°ï¼") 