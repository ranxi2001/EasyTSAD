"""
OmniAnomalyç®—æ³•å®ç° - SMDæ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ (PyTorchå®ç°)
åŸºäºEasyTSADæ¡†æ¶

æ€§èƒ½ç›®æ ‡: åœ¨SMDæ•°æ®é›†ä¸Šè¾¾åˆ°95%+ F1åˆ†æ•°
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


class TimeSeriesDataset(Dataset):
    """æ—¶é—´åºåˆ—æ•°æ®é›†"""
    
    def __init__(self, data, window_size):
        self.data = torch.FloatTensor(data)
        self.window_size = window_size
        
    def __len__(self):
        return len(self.data) - self.window_size + 1
        
    def __getitem__(self, idx):
        window = self.data[idx:idx + self.window_size]
        return window


class Encoder(nn.Module):
    """ç¼–ç å™¨ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2):
        super().__init__()
        
        self.rnn = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.1
        )
        
        self.hidden_fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # å‡å€¼å’Œæ–¹å·®é¢„æµ‹å™¨
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_var = nn.Linear(hidden_dim, latent_dim)
        
    def forward(self, x):
        # x shape: [batch, seq_len, input_dim]
        output, _ = self.rnn(x)
        hidden = output[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        
        hidden = self.hidden_fc(hidden)
        
        # è®¡ç®—æ½œå˜é‡åˆ†å¸ƒå‚æ•°
        mu = self.fc_mu(hidden)
        log_var = self.fc_var(hidden)
        
        return mu, log_var


class Decoder(nn.Module):
    """è§£ç å™¨ç½‘ç»œ"""
    
    def __init__(self, latent_dim, hidden_dim, output_dim, n_layers=2):
        super().__init__()
        
        self.latent_fc = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.rnn = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.1
        )
        
        self.output_fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim * 2)  # è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        )
        
    def forward(self, z, seq_len):
        # z shape: [batch, latent_dim]
        hidden = self.latent_fc(z)
        
        # æ‰©å±•æˆåºåˆ—
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        output, _ = self.rnn(hidden)
        output = self.output_fc(output)
        
        # åˆ†ç¦»å‡å€¼å’Œæ–¹å·®
        mu, log_var = torch.chunk(output, 2, dim=-1)
        return mu, log_var


class OmniAnomalyModel(nn.Module):
    """OmniAnomalyæ¨¡å‹"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2):
        super().__init__()
        
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, n_layers)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, n_layers)
        
    def reparameterize(self, mu, log_var):
        """é‡å‚æ•°åŒ–é‡‡æ ·"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
        
    def forward(self, x):
        # ç¼–ç 
        mu, log_var = self.encoder(x)
        
        # é‡‡æ ·
        z = self.reparameterize(mu, log_var)
        
        # è§£ç 
        recon_mu, recon_log_var = self.decoder(z, x.size(1))
        
        return recon_mu, recon_log_var, mu, log_var


class OmniAnomaly(BaseMethod):
    """OmniAnomalyå¼‚å¸¸æ£€æµ‹æ–¹æ³• - SMDä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None  # åˆå§‹åŒ–modelå±æ€§ä¸ºNone
        
        if params is None:
            params = {}
            
        # SMDæ•°æ®é›†ä¼˜åŒ–é…ç½®
        self.config = {
            # æ¨¡å‹æ¶æ„é…ç½®
            'input_dim': params.get('input_dim', 38),  # SMDé»˜è®¤38ç»´
            'hidden_dim': params.get('hidden_dim', 500),
            'latent_dim': params.get('latent_dim', 8),
            'n_layers': params.get('n_layers', 2),
            'window_size': params.get('window_size', 100),
            
            # è®­ç»ƒå‚æ•°
            'batch_size': params.get('batch_size', 128),
            'epochs': params.get('epochs', 10),
            'learning_rate': params.get('lr', 1e-3),
            'beta': params.get('beta', 0.01),  # KLæŸå¤±æƒé‡
            
            # è¯„ä¼°å‚æ•°
            'n_samples': params.get('n_samples', 10),  # æµ‹è¯•æ—¶çš„é‡‡æ ·æ•°
        }
        
        print(f"[LOG] OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆ")
        print(f"[LOG] ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[LOG] é…ç½®: window={self.config['window_size']}, latent={self.config['latent_dim']}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = OmniAnomalyModel(
            input_dim=self.config['input_dim'],
            hidden_dim=self.config['hidden_dim'],
            latent_dim=self.config['latent_dim'],
            n_layers=self.config['n_layers']
        ).to(self.device)
        
    def train_valid_phase(self, tsTrain: MTSData):
        """ä¼˜åŒ–çš„è®­ç»ƒé˜¶æ®µ"""
        print(f"\n[LOG] ========== OmniAnomaly SMDä¼˜åŒ–è®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        self.config['input_dim'] = self.n_features
        
        print(f"[LOG] è®­ç»ƒæ•°æ®: {train_data.shape}")
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        dataset = TimeSeriesDataset(train_data, self.config['window_size'])
        train_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=0
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate']
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 5
        patience_counter = 0
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss = 0
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.config['epochs']}")
            for batch in pbar:
                batch = batch.to(self.device)
                
                # å‰å‘ä¼ æ’­
                recon_mu, recon_log_var, mu, log_var = self.model(batch)
                
                # ç®€åŒ–çš„é‡æ„æŸå¤±ï¼šä½¿ç”¨MSE
                recon_loss = F.mse_loss(recon_mu, batch, reduction='sum')
                
                # KLæ•£åº¦: KL(q(z|x)||p(z)) where p(z) = N(0,I)
                kl_loss = 0.5 * torch.sum(
                    mu.pow(2) + log_var.exp() - log_var - 1
                )
                
                # æ€»æŸå¤± = é‡æ„æŸå¤± + Î² * KLæ•£åº¦
                loss = recon_loss + self.config['beta'] * kl_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)
                optimizer.step()
                
                total_loss += loss.item()
                
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'recon': f'{recon_loss.item():.2f}',
                    'kl': f'{kl_loss.item():.4f}'
                })
            
            avg_loss = total_loss / len(train_loader)
            print(f"Epoch {epoch+1}: å¹³å‡æŸå¤± = {avg_loss:.4f}")
            
            # æ—©åœ
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"[LOG] æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_model_state'):
            self.model.load_state_dict(self.best_model_state)
            
        print(f"[LOG] ========== OmniAnomaly SMDä¼˜åŒ–è®­ç»ƒå®Œæˆ ==========\n")
        
    def test_phase(self, tsData: MTSData):
        """ä¼˜åŒ–çš„æµ‹è¯•é˜¶æ®µ"""
        print(f"\n[LOG] ========== OmniAnomaly SMDä¼˜åŒ–æµ‹è¯•å¼€å§‹ ==========")
        
        test_data = tsData.test
        print(f"[LOG] æµ‹è¯•æ•°æ®: {test_data.shape}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset = TimeSeriesDataset(test_data, self.config['window_size'])
        test_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=0
        )
        
        self.model.eval()
        scores = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="æµ‹è¯•ä¸­"):
                batch = batch.to(self.device)
                
                # å¤šæ¬¡é‡‡æ ·è®¡ç®—é‡æ„æ¦‚ç‡
                sample_scores = []
                for _ in range(self.config['n_samples']):
                    recon_mu, recon_log_var, _, _ = self.model(batch)
                    
                    # è®¡ç®—é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°
                    # è¯¯å·®è¶Šå¤§è¡¨ç¤ºè¶Šå¼‚å¸¸
                    recon_error = F.mse_loss(recon_mu, batch, reduction='none')
                    # å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªæ—¶é—´æ­¥å’Œç‰¹å¾æ±‚å¹³å‡ï¼Œç„¶åå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    sample_scores.append(recon_error.mean(dim=-1)[:, -1].cpu().numpy())
                
                # å–å¹³å‡
                batch_scores = np.mean(sample_scores, axis=0)
                scores.extend(batch_scores)
        
        # å¤„ç†åˆ†æ•°
        full_scores = self._process_scores(scores, len(test_data))

        self.__anomaly_score = full_scores
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡: å‡å€¼={np.mean(full_scores):.4f}, æ ‡å‡†å·®={np.std(full_scores):.4f}")
        print(f"[LOG] ========== OmniAnomaly SMDä¼˜åŒ–æµ‹è¯•å®Œæˆ ==========\n")
        
    def _process_scores(self, scores, data_length):
        """å¤„ç†å¼‚å¸¸åˆ†æ•°"""
        # å¡«å……å¼€å§‹çš„çª—å£
        full_scores = np.zeros(data_length)
        scores = np.array(scores)
        
        # ä½¿ç”¨æ»‘åŠ¨å¹³å‡å¡«å……å‰é¢çš„ç‚¹
        window_fill = min(self.config['window_size'] - 1, len(scores))
        if window_fill > 0:
            full_scores[:self.config['window_size']-1] = np.mean(scores[:window_fill])
        
        # å¡«å……å®é™…åˆ†æ•°
        full_scores[self.config['window_size']-1:self.config['window_size']-1+len(scores)] = scores
        
        # ç§»åŠ¨å¹³å‡å¹³æ»‘
        window = 5
        weights = np.ones(window) / window
        full_scores = np.convolve(full_scores, weights, mode='same')
        
        # æ ‡å‡†åŒ–åˆ°[0,1] (åˆ†æ•°è¶Šé«˜è¶Šå¼‚å¸¸)
        min_score = np.min(full_scores)
        max_score = np.max(full_scores)
        if max_score > min_score:
            full_scores = (full_scores - min_score) / (max_score - min_score)
        
        return full_scores
        
    def anomaly_score(self) -> np.ndarray:
        """è¿”å›å¼‚å¸¸åˆ†æ•°"""
        return self.__anomaly_score
        
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            param_info = f"""
                OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬å‚æ•°ç»Ÿè®¡:
                ==================================================
                è¾“å…¥ç»´åº¦: {self.config['input_dim']}
                éšè—ç»´åº¦: {self.config['hidden_dim']}
                æ½œåœ¨ç»´åº¦: {self.config['latent_dim']}
                RNNå±‚æ•°: {self.config['n_layers']}
                çª—å£å¤§å°: {self.config['window_size']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['learning_rate']}
                ==================================================
                SMDä¼˜åŒ–ç‰¹æ€§:
                âœ… PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
                âœ… GRUå¾ªç¯ç¥ç»ç½‘ç»œ
                âœ… å˜åˆ†è‡ªç¼–ç å™¨
                âœ… å¤šé‡é‡‡æ ·æ¨æ–­
                âœ… æ¢¯åº¦è£å‰ª
                âœ… æ—©åœæœºåˆ¶
                âœ… åˆ†æ•°åå¤„ç†ä¼˜åŒ–
                ==================================================
            """
        else:
            param_info = "OmniAnomalyæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬ (PyTorch) ==========")
    
    # Create a global controller
    gctrl = TSADController()
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )

    """============= è¿è¡Œä¼˜åŒ–çš„OmniAnomaly ============="""
    
    method = "OmniAnomaly"
    
    # SMDä¼˜åŒ–é…ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            'input_dim': 38,        # SMDé»˜è®¤38ç»´
            'hidden_dim': 500,
            'latent_dim': 8,
            'n_layers': 2,
            'window_size': 100,
            'batch_size': 128,
            'epochs': 10,
            'lr': 1e-3,
            'beta': 0.01,           # KLæŸå¤±æƒé‡
            'n_samples': 10         # æµ‹è¯•é‡‡æ ·æ•°
        },
        preprocess="z-score",
    )
       
    """============= è¯„ä¼°è®¾ç½® ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
        
    """============= ç»˜å›¾è®¾ç½® ============="""
    
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬æ‰§è¡Œå®Œæ¯• ==========") 