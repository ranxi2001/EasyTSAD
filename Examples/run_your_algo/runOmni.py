"""
OmniAnomalyç®—æ³•å®ç° - é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ (PyTorchå®ç°)
åŸºäºEasyTSADæ¡†æ¶

æ€§èƒ½ç›®æ ‡: åœ¨SMDæ•°æ®é›†ä¸Šè¾¾åˆ°95%+ F1åˆ†æ•°
ä¸»è¦ä¼˜åŒ–: ä¿®å¤æŸå¤±å‡½æ•°ã€æ”¹è¿›æ¨¡å‹æ¶æ„ã€ä¼˜åŒ–GPUåˆ©ç”¨ã€å¢å¼ºè®­ç»ƒç¨³å®šæ€§
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡å¹¶ä¼˜åŒ–GPUè®¾ç½®"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        # ä¼˜åŒ–CUDAè®¾ç½® for RTX 5080
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•è·å¾—æ›´å¥½æ€§èƒ½
        torch.cuda.set_per_process_memory_fraction(0.9)  # ä½¿ç”¨90%æ˜¾å­˜
        print(f"[GPU] ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"[GPU] æ˜¾å­˜å®¹é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return device
    else:
        return torch.device('cpu')


class TimeSeriesDataset(Dataset):
    """ä¼˜åŒ–çš„æ—¶é—´åºåˆ—æ•°æ®é›†"""
    
    def __init__(self, data, window_size):
        self.data = torch.FloatTensor(data)
        self.window_size = window_size
        
    def __len__(self):
        return len(self.data) - self.window_size + 1
        
    def __getitem__(self, idx):
        window = self.data[idx:idx + self.window_size]
        return window


class AttentionEncoder(nn.Module):
    """å¢å¼ºçš„æ³¨æ„åŠ›ç¼–ç å™¨"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2, n_heads=8):
        super().__init__()
        
        # è¾“å…¥å±‚å½’ä¸€åŒ–
        self.input_norm = nn.LayerNorm(input_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = nn.Parameter(torch.randn(1000, input_dim) * 0.1)
        
        # åŒå‘GRU
        self.rnn = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.2,
            bidirectional=True
        )
        
        # è‡ªæ³¨æ„åŠ›å±‚
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim * 2,  # åŒå‘RNN
            num_heads=n_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # ç‰¹å¾æå–ç½‘ç»œ
        self.feature_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # æ½œå˜é‡åˆ†å¸ƒé¢„æµ‹å™¨
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.GRU):
                for name, param in m.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥å½’ä¸€åŒ–å’Œä½ç½®ç¼–ç 
        x = self.input_norm(x)
        pos_enc = self.positional_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)
        x = x + pos_enc
        
        # åŒå‘RNN
        rnn_out, _ = self.rnn(x)
        
        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.attention(rnn_out, rnn_out, rnn_out)
        
        # æ®‹å·®è¿æ¥
        rnn_out = rnn_out + attn_out
        
        # å…¨å±€æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–
        max_pool = torch.max(rnn_out, dim=1)[0]
        avg_pool = torch.mean(rnn_out, dim=1)
        
        # ç‰¹å¾èåˆ
        features = max_pool + avg_pool
        features = self.feature_net(features)
        
        # æ½œå˜é‡åˆ†å¸ƒå‚æ•°
        mu = self.fc_mu(features)
        logvar = self.fc_logvar(features)
        
        return mu, logvar


class AttentionDecoder(nn.Module):
    """å¢å¼ºçš„æ³¨æ„åŠ›è§£ç å™¨"""
    
    def __init__(self, latent_dim, hidden_dim, output_dim, n_layers=2, n_heads=8):
        super().__init__()
        
        # æ½œå˜é‡æŠ•å½±
        self.latent_proj = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # åŒå‘GRU
        self.rnn = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim // 2,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.2,
            bidirectional=True
        )
        
        # è‡ªæ³¨æ„åŠ›å±‚
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=n_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # è¾“å‡ºæŠ•å½±ç½‘ç»œ
        self.output_net = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
    def forward(self, z, seq_len):
        batch_size = z.size(0)
        
        # æ½œå˜é‡æŠ•å½±
        hidden = self.latent_proj(z)
        
        # æ‰©å±•æˆåºåˆ—
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        # åŒå‘RNN
        rnn_out, _ = self.rnn(hidden)
        
        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.attention(rnn_out, rnn_out, rnn_out)
        
        # æ®‹å·®è¿æ¥
        output = rnn_out + attn_out
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_net(output)
        
        return output


class AdvancedOmniAnomalyModel(nn.Module):
    """é«˜çº§OmniAnomalyæ¨¡å‹"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2, n_heads=8):
        super().__init__()
        
        self.encoder = AttentionEncoder(input_dim, hidden_dim, latent_dim, n_layers, n_heads)
        self.decoder = AttentionDecoder(latent_dim, hidden_dim, input_dim, n_layers, n_heads)
        
        # æ¨¡å‹å‚æ•°
        self.latent_dim = latent_dim
        
    def reparameterize(self, mu, logvar):
        """æ”¹è¿›çš„é‡å‚æ•°åŒ–é‡‡æ ·"""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu  # æµ‹è¯•æ—¶ä½¿ç”¨å‡å€¼
        
    def forward(self, x):
        # ç¼–ç 
        mu, logvar = self.encoder(x)
        
        # é‡‡æ ·
        z = self.reparameterize(mu, logvar)
        
        # è§£ç 
        recon = self.decoder(z, x.size(1))
        
        return recon, mu, logvar


class OmniAnomaly(BaseMethod):
    """é«˜æ€§èƒ½OmniAnomalyå¼‚å¸¸æ£€æµ‹æ–¹æ³•"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None
        
        if params is None:
            params = {}
            
        # é«˜æ€§èƒ½é…ç½®for RTX 5080
        self.config = {
            # æ¨¡å‹æ¶æ„é…ç½® - å¢å¼ºç‰ˆ
            'input_dim': params.get('input_dim', 38),
            'hidden_dim': params.get('hidden_dim', 512),  # å¢å¤§éšè—å±‚
            'latent_dim': params.get('latent_dim', 16),   # å¢å¤§æ½œåœ¨ç»´åº¦
            'n_layers': params.get('n_layers', 3),        # å¢åŠ å±‚æ•°
            'n_heads': params.get('n_heads', 8),          # å¤šå¤´æ³¨æ„åŠ›
            'window_size': params.get('window_size', 100),
            
            # è®­ç»ƒå‚æ•° - GPUä¼˜åŒ–
            'batch_size': params.get('batch_size', 256),   # å¢å¤§æ‰¹é‡ä»¥å……åˆ†åˆ©ç”¨GPU
            'epochs': params.get('epochs', 50),            # å¢åŠ è®­ç»ƒè½®æ•°
            'learning_rate': params.get('lr', 1e-3),
            'beta': params.get('beta', 1.0),               # å¢å¼ºKLçº¦æŸ
            'beta_annealing': params.get('beta_annealing', True),
            
            # ä¼˜åŒ–å™¨é…ç½®
            'weight_decay': params.get('weight_decay', 1e-4),
            'warmup_epochs': params.get('warmup_epochs', 5),
            
            # è¯„ä¼°å‚æ•°
            'n_samples': params.get('n_samples', 50),      # å¢åŠ é‡‡æ ·æ•°æé«˜ç¨³å®šæ€§
            'score_window': params.get('score_window', 10), # åˆ†æ•°å¹³æ»‘çª—å£
        }
        
        print(f"[LOG] ğŸš€ é«˜æ€§èƒ½OmniAnomalyåˆå§‹åŒ–å®Œæˆ")
        print(f"[LOG] ğŸ“Š æ¨¡å‹é…ç½®: hidden={self.config['hidden_dim']}, latent={self.config['latent_dim']}, heads={self.config['n_heads']}")
        print(f"[LOG] ğŸ¯ GPUä¼˜åŒ–: batch={self.config['batch_size']}, epochs={self.config['epochs']}")
        
    def train_valid_phase(self, tsTrain: MTSData):
        """é«˜æ€§èƒ½è®­ç»ƒé˜¶æ®µ"""
        print(f"\n[LOG] ========== ğŸš€ é«˜æ€§èƒ½è®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        self.config['input_dim'] = self.n_features
        
        print(f"[LOG] ğŸ“Š è®­ç»ƒæ•°æ®: {train_data.shape}")
        
        # åˆ›å»ºé«˜æ€§èƒ½æ¨¡å‹
        self.model = AdvancedOmniAnomalyModel(
            input_dim=self.config['input_dim'],
            hidden_dim=self.config['hidden_dim'],
            latent_dim=self.config['latent_dim'],
            n_layers=self.config['n_layers'],
            n_heads=self.config['n_heads']
        ).to(self.device)
        
        # æ··åˆç²¾åº¦è®­ç»ƒfor RTX 5080
        scaler = torch.cuda.amp.GradScaler()
        
        # æ•°æ®åŠ è½½å™¨ - å¤šè¿›ç¨‹ä¼˜åŒ–
        dataset = TimeSeriesDataset(train_data, self.config['window_size'])
        train_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
            pin_memory=True,  # å›ºå®šå†…å­˜
            persistent_workers=True
        )
        
        # ä¼˜åŒ–å™¨ - AdamW with warmup
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=0.1, total_iters=self.config['warmup_epochs']
        )
        cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.config['epochs'] - self.config['warmup_epochs']
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss = 0
            total_recon_loss = 0
            total_kl_loss = 0
            
            # Betaé€€ç«ç­–ç•¥
            if self.config['beta_annealing']:
                beta = min(self.config['beta'], epoch / 10.0)
            else:
                beta = self.config['beta']
            
            pbar = tqdm(train_loader, desc=f"ğŸ”¥ Epoch {epoch+1}/{self.config['epochs']}")
            
            for batch in pbar:
                batch = batch.to(self.device, non_blocking=True)
                
                optimizer.zero_grad()
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast():
                    recon, mu, logvar = self.model(batch)
                    
                    # ä¿®å¤çš„æŸå¤±å‡½æ•° - ä½¿ç”¨meanè€Œä¸æ˜¯sum
                    recon_loss = F.mse_loss(recon, batch, reduction='mean')
                    
                    # KLæ•£åº¦ - æ­£ç¡®çš„è®¡ç®—æ–¹å¼
                    kl_loss = -0.5 * torch.mean(
                        1 + logvar - mu.pow(2) - logvar.exp()
                    )
                    
                    # ç¡®ä¿KLæŸå¤±ä¸ºæ­£
                    kl_loss = torch.clamp(kl_loss, min=0.0)
                    
                    # æ€»æŸå¤±
                    loss = recon_loss + beta * kl_loss
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                
                total_loss += loss.item()
                total_recon_loss += recon_loss.item()
                total_kl_loss += kl_loss.item()
                
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'recon': f'{recon_loss.item():.4f}',
                    'kl': f'{kl_loss.item():.4f}',
                    'beta': f'{beta:.3f}'
                })
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if epoch < self.config['warmup_epochs']:
                warmup_scheduler.step()
            else:
                cosine_scheduler.step()
            
            avg_loss = total_loss / len(train_loader)
            avg_recon = total_recon_loss / len(train_loader)
            avg_kl = total_kl_loss / len(train_loader)
            
            print(f"âœ… Epoch {epoch+1}: Loss={avg_loss:.4f}, Recon={avg_recon:.4f}, KL={avg_kl:.4f}, LR={optimizer.param_groups[0]['lr']:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"[LOG] â° æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_model_state'):
            self.model.load_state_dict(self.best_model_state)
            
        print(f"[LOG] ========== ğŸ‰ é«˜æ€§èƒ½è®­ç»ƒå®Œæˆ ==========\n")
        
    def test_phase(self, tsData: MTSData):
        """é«˜æ€§èƒ½æµ‹è¯•é˜¶æ®µ"""
        print(f"\n[LOG] ========== ğŸ” é«˜æ€§èƒ½æµ‹è¯•å¼€å§‹ ==========")
        
        test_data = tsData.test
        print(f"[LOG] ğŸ“Š æµ‹è¯•æ•°æ®: {test_data.shape}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        dataset = TimeSeriesDataset(test_data, self.config['window_size'])
        test_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.model.eval()
        all_scores = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="ğŸ¯ æµ‹è¯•ä¸­"):
                batch = batch.to(self.device, non_blocking=True)
                
                # å¤šæ¬¡é‡‡æ ·è·å¾—æ›´ç¨³å®šçš„åˆ†æ•°
                batch_scores = []
                for _ in range(self.config['n_samples']):
                    with torch.cuda.amp.autocast():
                        recon, mu, logvar = self.model(batch)
                        
                        # è®¡ç®—é‡æ„æ¦‚ç‡ - è´Ÿå¯¹æ•°ä¼¼ç„¶
                        recon_error = F.mse_loss(recon, batch, reduction='none')
                        # å¯¹ç‰¹å¾ç»´åº¦æ±‚å¹³å‡ï¼Œä¿ç•™æ—¶é—´ç»´åº¦
                        point_scores = recon_error.mean(dim=-1)
                        # å–çª—å£æœ€åä¸€ä¸ªç‚¹çš„åˆ†æ•°
                        scores = point_scores[:, -1].cpu().numpy()
                        batch_scores.append(scores)
                
                # å–å‡å€¼å’Œæ ‡å‡†å·®è€ƒè™‘ä¸ç¡®å®šæ€§
                mean_scores = np.mean(batch_scores, axis=0)
                std_scores = np.std(batch_scores, axis=0)
                # ç»“åˆå‡å€¼å’Œä¸ç¡®å®šæ€§
                final_scores = mean_scores + 0.5 * std_scores
                all_scores.extend(final_scores)
        
        # é«˜çº§åˆ†æ•°åå¤„ç†
        full_scores = self._advanced_score_processing(all_scores, len(test_data))
        
        self.__anomaly_score = full_scores
        print(f"[LOG] ğŸ“ˆ å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] ğŸ“Š å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡: å‡å€¼={np.mean(full_scores):.4f}, æ ‡å‡†å·®={np.std(full_scores):.4f}")
        print(f"[LOG] ========== âœ… é«˜æ€§èƒ½æµ‹è¯•å®Œæˆ ==========\n")
        
    def _advanced_score_processing(self, scores, data_length):
        """é«˜çº§åˆ†æ•°åå¤„ç†"""
        scores = np.array(scores)
        full_scores = np.zeros(data_length)
        
        # å¡«å……å‰é¢çš„çª—å£
        if len(scores) > 0:
            # ä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡å¡«å……
            alpha = 0.3
            fill_value = scores[0]
            for i in range(self.config['window_size'] - 1):
                full_scores[i] = fill_value
                
        # å¡«å……å®é™…åˆ†æ•°
        end_idx = min(self.config['window_size'] - 1 + len(scores), data_length)
        full_scores[self.config['window_size']-1:end_idx] = scores[:end_idx - self.config['window_size'] + 1]
        
        # å¤šé‡å¹³æ»‘å¤„ç†
        # 1. é«˜æ–¯å¹³æ»‘
        from scipy.ndimage import gaussian_filter1d
        full_scores = gaussian_filter1d(full_scores, sigma=2.0)
        
        # 2. ç§»åŠ¨å¹³å‡å¹³æ»‘
        window = self.config['score_window']
        weights = np.exp(np.linspace(-1, 0, window))  # æŒ‡æ•°æƒé‡
        weights /= weights.sum()
        
        # åº”ç”¨å·ç§¯å¹³æ»‘
        padded_scores = np.pad(full_scores, (window//2, window//2), mode='edge')
        smoothed = np.convolve(padded_scores, weights, mode='valid')
        full_scores = smoothed[:len(full_scores)]
        
        # 3. å¼‚å¸¸å€¼å¤„ç†
        q75, q25 = np.percentile(full_scores, [75, 25])
        iqr = q75 - q25
        upper_bound = q75 + 1.5 * iqr
        full_scores = np.clip(full_scores, None, upper_bound)
        
        # 4. è‡ªé€‚åº”å½’ä¸€åŒ–
        # ä½¿ç”¨Robust Scaler
        median = np.median(full_scores)
        mad = np.median(np.abs(full_scores - median))
        if mad > 0:
            full_scores = (full_scores - median) / (1.4826 * mad)  # 1.4826æ˜¯æ­£æ€åˆ†å¸ƒçš„ä¿®æ­£å› å­
            full_scores = np.clip(full_scores, -3, 3)  # é™åˆ¶åœ¨3å€MADå†…
        
        # 5. æ˜ å°„åˆ°[0,1]
        min_score = np.min(full_scores)
        max_score = np.max(full_scores)
        if max_score > min_score:
            full_scores = (full_scores - min_score) / (max_score - min_score)
        
        return full_scores
        
    def anomaly_score(self) -> np.ndarray:
        """è¿”å›å¼‚å¸¸åˆ†æ•°"""
        return self.__anomaly_score
        
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                ğŸš€ é«˜æ€§èƒ½OmniAnomalyå‚æ•°ç»Ÿè®¡:
                ==================================================
                ğŸ“Š æ¨¡å‹æ¶æ„:
                - è¾“å…¥ç»´åº¦: {self.config['input_dim']}
                - éšè—ç»´åº¦: {self.config['hidden_dim']}
                - æ½œåœ¨ç»´åº¦: {self.config['latent_dim']}
                - RNNå±‚æ•°: {self.config['n_layers']}
                - æ³¨æ„åŠ›å¤´æ•°: {self.config['n_heads']}
                - çª—å£å¤§å°: {self.config['window_size']}
                
                ğŸ¯ è®­ç»ƒé…ç½®:
                - æ‰¹é‡å¤§å°: {self.config['batch_size']}
                - è®­ç»ƒè½®æ•°: {self.config['epochs']}
                - å­¦ä¹ ç‡: {self.config['learning_rate']}
                - æƒé‡è¡°å‡: {self.config['weight_decay']}
                - Betaæƒé‡: {self.config['beta']}
                
                ğŸ’¾ æ¨¡å‹å‚æ•°:
                - æ€»å‚æ•°æ•°: {total_params:,}
                - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}
                
                ==================================================
                ğŸ”¥ é«˜æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§:
                âœ… RTX 5080 GPUåŠ é€Ÿ
                âœ… æ··åˆç²¾åº¦è®­ç»ƒ (FP16)
                âœ… å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
                âœ… åŒå‘GRU + æ®‹å·®è¿æ¥
                âœ… AdamWä¼˜åŒ–å™¨ + ä½™å¼¦é€€ç«
                âœ… å­¦ä¹ ç‡é¢„çƒ­ç­–ç•¥
                âœ… æ¢¯åº¦è£å‰ª + æ—©åœ
                âœ… å¤šé‡é‡‡æ ·æ¨æ–­
                âœ… é«˜çº§åˆ†æ•°åå¤„ç†
                âœ… é²æ£’å½’ä¸€åŒ–
                ==================================================
            """
        else:
            param_info = "OmniAnomalyæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== é«˜æ€§èƒ½OmniAnomaly (RTX 5080ä¼˜åŒ–ç‰ˆ) ==========")
    
    # Create a global controller
    gctrl = TSADController()
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )

    """============= è¿è¡Œé«˜æ€§èƒ½OmniAnomaly ============="""
    
    method = "OmniAnomaly"
    
    # RTX 5080ä¼˜åŒ–é…ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            'input_dim': 38,         # SMDæ•°æ®ç»´åº¦
            'hidden_dim': 512,       # å¢å¤§éšè—å±‚
            'latent_dim': 16,        # å¢å¤§æ½œåœ¨ç»´åº¦
            'n_layers': 3,           # å¢åŠ å±‚æ•°
            'n_heads': 8,            # å¤šå¤´æ³¨æ„åŠ›
            'window_size': 100,      # æ—¶é—´çª—å£
            'batch_size': 256,       # GPUä¼˜åŒ–æ‰¹é‡å¤§å°
            'epochs': 50,            # å……åˆ†è®­ç»ƒ
            'lr': 1e-3,              # å­¦ä¹ ç‡
            'beta': 1.0,             # KLæƒé‡
            'beta_annealing': True,  # Betaé€€ç«
            'weight_decay': 1e-4,    # æƒé‡è¡°å‡
            'warmup_epochs': 5,      # é¢„çƒ­è½®æ•°
            'n_samples': 50,         # é‡‡æ ·æ•°
            'score_window': 10       # åˆ†æ•°å¹³æ»‘çª—å£
        },
        preprocess="z-score",
    )
       
    """============= è¯„ä¼°è®¾ç½® ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
        
    """============= ç»˜å›¾è®¾ç½® ============="""
    
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== é«˜æ€§èƒ½OmniAnomalyæ‰§è¡Œå®Œæ¯• ==========")