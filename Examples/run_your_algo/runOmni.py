"""
OmniAnomalyç®—æ³•å®ç° - SMDæ•°æ®é›†ä¼˜åŒ–ç‰ˆæœ¬ (PyTorchå®ç°)
åŸºäºEasyTSADæ¡†æ¶

æ€§èƒ½ç›®æ ‡: åœ¨SMDæ•°æ®é›†ä¸Šè¾¾åˆ°95%+ F1åˆ†æ•°
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


def get_default_device():
    """é€‰æ‹©å¯ç”¨çš„è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


class TimeSeriesDataset(Dataset):
    """æ—¶é—´åºåˆ—æ•°æ®é›†"""
    
    def __init__(self, data, window_size):
        self.data = torch.FloatTensor(data)
        self.window_size = window_size
        
    def __len__(self):
        return len(self.data) - self.window_size + 1
        
    def __getitem__(self, idx):
        window = self.data[idx:idx + self.window_size]
        return window


class Encoder(nn.Module):
    """ç¼–ç å™¨ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2):
        super().__init__()
        
        self.rnn = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.1
        )
        
        self.hidden_fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # å‡å€¼å’Œæ–¹å·®é¢„æµ‹å™¨
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_var = nn.Linear(hidden_dim, latent_dim)
        
    def forward(self, x):
        # x shape: [batch, seq_len, input_dim]
        output, _ = self.rnn(x)
        hidden = output[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        
        hidden = self.hidden_fc(hidden)
        
        # è®¡ç®—æ½œå˜é‡åˆ†å¸ƒå‚æ•°
        mu = self.fc_mu(hidden)
        log_var = self.fc_var(hidden)
        
        return mu, log_var


class Decoder(nn.Module):
    """è§£ç å™¨ç½‘ç»œ"""
    
    def __init__(self, latent_dim, hidden_dim, output_dim, n_layers=2):
        super().__init__()
        
        self.latent_fc = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.rnn = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=0.1
        )
        
        self.output_fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim * 2)  # è¾“å‡ºå‡å€¼å’Œæ–¹å·®
        )
        
    def forward(self, z, seq_len):
        # z shape: [batch, latent_dim]
        hidden = self.latent_fc(z)
        
        # æ‰©å±•æˆåºåˆ—
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        output, _ = self.rnn(hidden)
        output = self.output_fc(output)
        
        # åˆ†ç¦»å‡å€¼å’Œæ–¹å·®
        mu, log_var = torch.chunk(output, 2, dim=-1)
        return mu, log_var


class OmniAnomalyModel(nn.Module):
    """OmniAnomalyæ¨¡å‹"""
    
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers=2):
        super().__init__()
        
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, n_layers)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, n_layers)
        
    def reparameterize(self, mu, log_var):
        """é‡å‚æ•°åŒ–é‡‡æ ·"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
        
    def forward(self, x):
        # ç¼–ç 
        mu, log_var = self.encoder(x)
        
        # é‡‡æ ·
        z = self.reparameterize(mu, log_var)
        
        # è§£ç 
        recon_mu, recon_log_var = self.decoder(z, x.size(1))
        
        return recon_mu, recon_log_var, mu, log_var


class OmniAnomaly(BaseMethod):
    """OmniAnomalyå¼‚å¸¸æ£€æµ‹æ–¹æ³• - SMDä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, params: dict = None) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.device = get_default_device()
        self.model = None  # åˆå§‹åŒ–modelå±æ€§ä¸ºNone
        
        if params is None:
            params = {}
            
        # SMDæ•°æ®é›†ä¼˜åŒ–é…ç½®
        self.config = {
            # æ¨¡å‹æ¶æ„é…ç½®
            'input_dim': params.get('input_dim', 38),  # SMDé»˜è®¤38ç»´
            'hidden_dim': params.get('hidden_dim', 500),
            'latent_dim': params.get('latent_dim', 8),
            'n_layers': params.get('n_layers', 2),
            'window_size': params.get('window_size', 100),
            
            # è®­ç»ƒå‚æ•°
            'batch_size': params.get('batch_size', 128),
            'epochs': params.get('epochs', 10),
            'learning_rate': params.get('lr', 1e-3),
            'beta': params.get('beta', 0.01),  # KLæŸå¤±æƒé‡
            
            # è¯„ä¼°å‚æ•°
            'n_samples': params.get('n_samples', 10),  # æµ‹è¯•æ—¶çš„é‡‡æ ·æ•°
        }
        
        print(f"[LOG] OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆ")
        print(f"[LOG] ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[LOG] é…ç½®: window={self.config['window_size']}, latent={self.config['latent_dim']}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = OmniAnomalyModel(
            input_dim=self.config['input_dim'],
            hidden_dim=self.config['hidden_dim'],
            latent_dim=self.config['latent_dim'],
            n_layers=self.config['n_layers']
        ).to(self.device)
        
    def train_valid_phase(self, tsTrain: MTSData):
        """ä¼˜åŒ–çš„è®­ç»ƒé˜¶æ®µ"""
        print(f"\n[LOG] ========== OmniAnomaly SMDä¼˜åŒ–è®­ç»ƒå¼€å§‹ ==========")
        
        train_data = tsTrain.train
        self.n_features = train_data.shape[1]
        self.config['input_dim'] = self.n_features
        
        print(f"[LOG] è®­ç»ƒæ•°æ®: {train_data.shape}")
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        dataset = TimeSeriesDataset(train_data, self.config['window_size'])
        train_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=0
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate']
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 5
        patience_counter = 0
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            total_loss = 0
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.config['epochs']}")
            for batch in pbar:
                batch = batch.to(self.device)
                
                # å‰å‘ä¼ æ’­
                recon_mu, recon_log_var, mu, log_var = self.model(batch)
                
                # ç®€åŒ–çš„é‡æ„æŸå¤±ï¼šä½¿ç”¨MSE
                recon_loss = F.mse_loss(recon_mu, batch, reduction='sum')
                
                # KLæ•£åº¦: KL(q(z|x)||p(z)) where p(z) = N(0,I)
                kl_loss = 0.5 * torch.sum(
                    mu.pow(2) + log_var.exp() - log_var - 1
                )
                
                # æ€»æŸå¤± = é‡æ„æŸå¤± + Î² * KLæ•£åº¦
                loss = recon_loss + self.config['beta'] * kl_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)
                optimizer.step()
                
                total_loss += loss.item()
                
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'recon': f'{recon_loss.item():.2f}',
                    'kl': f'{kl_loss.item():.4f}'
                })
            
            avg_loss = total_loss / len(train_loader)
            print(f"Epoch {epoch+1}: å¹³å‡æŸå¤± = {avg_loss:.4f}")
            
            # æ—©åœ
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"[LOG] æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_model_state'):
            self.model.load_state_dict(self.best_model_state)
            
        print(f"[LOG] ========== OmniAnomaly SMDä¼˜åŒ–è®­ç»ƒå®Œæˆ ==========\n")
        
    def test_phase(self, tsData: MTSData):
        """ä¼˜åŒ–çš„æµ‹è¯•é˜¶æ®µ"""
        print(f"\n[LOG] ========== OmniAnomaly SMDä¼˜åŒ–æµ‹è¯•å¼€å§‹ ==========")
        
        test_data = tsData.test
        print(f"[LOG] æµ‹è¯•æ•°æ®: {test_data.shape}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset = TimeSeriesDataset(test_data, self.config['window_size'])
        test_loader = DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=0
        )
        
        self.model.eval()
        scores = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="æµ‹è¯•ä¸­"):
                batch = batch.to(self.device)
                
                # å¤šæ¬¡é‡‡æ ·è®¡ç®—é‡æ„æ¦‚ç‡
                sample_scores = []
                for _ in range(self.config['n_samples']):
                    recon_mu, recon_log_var, _, _ = self.model(batch)
                    
                    # è®¡ç®—é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°
                    # è¯¯å·®è¶Šå¤§è¡¨ç¤ºè¶Šå¼‚å¸¸
                    recon_error = F.mse_loss(recon_mu, batch, reduction='none')
                    # å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªæ—¶é—´æ­¥å’Œç‰¹å¾æ±‚å¹³å‡ï¼Œç„¶åå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    sample_scores.append(recon_error.mean(dim=-1)[:, -1].cpu().numpy())
                
                # å–å¹³å‡
                batch_scores = np.mean(sample_scores, axis=0)
                scores.extend(batch_scores)
        
        # å¤„ç†åˆ†æ•°
        full_scores = self._process_scores(scores, len(test_data))

        self.__anomaly_score = full_scores
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{np.min(full_scores):.4f}, {np.max(full_scores):.4f}]")
        print(f"[LOG] å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡: å‡å€¼={np.mean(full_scores):.4f}, æ ‡å‡†å·®={np.std(full_scores):.4f}")
        print(f"[LOG] ========== OmniAnomaly SMDä¼˜åŒ–æµ‹è¯•å®Œæˆ ==========\n")
        
    def _process_scores(self, scores, data_length):
        """å¤„ç†å¼‚å¸¸åˆ†æ•°"""
        # å¡«å……å¼€å§‹çš„çª—å£
        full_scores = np.zeros(data_length)
        scores = np.array(scores)
        
        # ä½¿ç”¨æ»‘åŠ¨å¹³å‡å¡«å……å‰é¢çš„ç‚¹
        window_fill = min(self.config['window_size'] - 1, len(scores))
        if window_fill > 0:
            full_scores[:self.config['window_size']-1] = np.mean(scores[:window_fill])
        
        # å¡«å……å®é™…åˆ†æ•°
        full_scores[self.config['window_size']-1:self.config['window_size']-1+len(scores)] = scores
        
        # ç§»åŠ¨å¹³å‡å¹³æ»‘
        window = 5
        weights = np.ones(window) / window
        full_scores = np.convolve(full_scores, weights, mode='same')
        
        # æ ‡å‡†åŒ–åˆ°[0,1] (åˆ†æ•°è¶Šé«˜è¶Šå¼‚å¸¸)
        min_score = np.min(full_scores)
        max_score = np.max(full_scores)
        if max_score > min_score:
            full_scores = (full_scores - min_score) / (max_score - min_score)
        
        return full_scores
        
    def anomaly_score(self) -> np.ndarray:
        """è¿”å›å¼‚å¸¸åˆ†æ•°"""
        return self.__anomaly_score
        
    def param_statistic(self, save_file):
        """å‚æ•°ç»Ÿè®¡"""
        if self.model is not None:
            param_info = f"""
                OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬å‚æ•°ç»Ÿè®¡:
                ==================================================
                è¾“å…¥ç»´åº¦: {self.config['input_dim']}
                éšè—ç»´åº¦: {self.config['hidden_dim']}
                æ½œåœ¨ç»´åº¦: {self.config['latent_dim']}
                RNNå±‚æ•°: {self.config['n_layers']}
                çª—å£å¤§å°: {self.config['window_size']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['learning_rate']}
                ==================================================
                SMDä¼˜åŒ–ç‰¹æ€§:
                âœ… PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
                âœ… GRUå¾ªç¯ç¥ç»ç½‘ç»œ
                âœ… å˜åˆ†è‡ªç¼–ç å™¨
                âœ… å¤šé‡é‡‡æ ·æ¨æ–­
                âœ… æ¢¯åº¦è£å‰ª
                âœ… æ—©åœæœºåˆ¶
                âœ… åˆ†æ•°åå¤„ç†ä¼˜åŒ–
                ==================================================
            """
        else:
            param_info = "OmniAnomalyæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)


# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬ (PyTorch) ==========")
    
    # Create a global controller
    gctrl = TSADController()
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )

    """============= è¿è¡Œä¼˜åŒ–çš„OmniAnomaly ============="""
    
    method = "OmniAnomaly"
    
    # SMDä¼˜åŒ–é…ç½®
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            'input_dim': 38,        # SMDé»˜è®¤38ç»´
            'hidden_dim': 500,
            'latent_dim': 8,
            'n_layers': 2,
            'window_size': 100,
            'batch_size': 128,
            'epochs': 10,
            'lr': 1e-3,
            'beta': 0.01,           # KLæŸå¤±æƒé‡
            'n_samples': 10         # æµ‹è¯•é‡‡æ ·æ•°
        },
        preprocess="z-score",
    )
       
    """============= è¯„ä¼°è®¾ç½® ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    gctrl.set_evals([PointF1PA(), EventF1PA(), EventF1PA(mode="squeeze")])
    gctrl.do_evals(method=method, training_schema="mts")
        
    """============= ç»˜å›¾è®¾ç½® ============="""
    
    gctrl.plots(method=method, training_schema="mts")
    
    print("ğŸ‰ ========== OmniAnomaly SMDä¼˜åŒ–ç‰ˆæœ¬æ‰§è¡Œå®Œæ¯• ==========")

    # è¦æé«˜OmniAnomalyç®—æ³•çš„æ€§èƒ½ï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å…¥æ‰‹ï¼š
    #
    #
    # 1.
    # ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°
    # çª—å£å¤§å°ï¼ˆwindowï¼‰: è°ƒæ•´æ—¶é—´çª—å£å¤§å°ä»¥é€‚åº”æ•°æ®çš„æ—¶é—´ä¾èµ–æ€§ã€‚
    # æ½œåœ¨ç»´åº¦ï¼ˆlatentï¼‰: å¢åŠ æˆ–å‡å°‘æ½œåœ¨ç©ºé—´çš„ç»´åº¦ä»¥å¹³è¡¡æ¨¡å‹å¤æ‚åº¦å’Œæ€§èƒ½ã€‚
    # å­¦ä¹ ç‡: è°ƒæ•´ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ï¼Œå°è¯•æ›´å°çš„å­¦ä¹ ç‡ä»¥è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒã€‚
    # æ‰¹é‡å¤§å°ï¼ˆbatch
    # sizeï¼‰: å¢å¤§æ‰¹é‡å¤§å°ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œä½†éœ€æ³¨æ„æ˜¾å­˜é™åˆ¶ã€‚
    # 2.
    # æ”¹è¿›æ•°æ®é¢„å¤„ç†
    # å½’ä¸€åŒ–: ç¡®ä¿è¾“å…¥æ•°æ®ç»è¿‡æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–å¤„ç†ï¼Œé¿å…ç‰¹å¾å€¼èŒƒå›´è¿‡å¤§ã€‚
    # é™å™ª: å¯¹æ•°æ®è¿›è¡Œå¹³æ»‘æˆ–å»å™ªå¤„ç†ï¼Œå‡å°‘å™ªå£°å¯¹æ¨¡å‹çš„å¹²æ‰°ã€‚
    # ç‰¹å¾é€‰æ‹©: å»é™¤å†—ä½™æˆ–æ— å…³çš„ç‰¹å¾ï¼Œä¿ç•™å…³é”®ç‰¹å¾ã€‚
    # 3.
    # å¢å¼ºæ¨¡å‹ç»“æ„
    # æ”¹è¿›VAEç»“æ„: å°è¯•æ›´æ·±çš„ç½‘ç»œæˆ–æ›´å¤æ‚çš„ç¼–ç å™¨ / è§£ç å™¨ç»“æ„ã€‚
    # æ­£åˆ™åŒ–: æ·»åŠ L1 / L2æ­£åˆ™åŒ–æˆ–Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    # KLæ•£åº¦æƒé‡: è°ƒæ•´KLæ•£åº¦çš„æƒé‡ç³»æ•°ï¼Œå¹³è¡¡é‡æ„è¯¯å·®å’Œæ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–ã€‚
    # 4.
    # è®­ç»ƒæŠ€å·§
    # é¢„è®­ç»ƒ: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–æƒé‡ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚
    # å­¦ä¹ ç‡è°ƒåº¦: ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼ˆå¦‚ä½™å¼¦é€€ç«æˆ–ReduceLROnPlateauï¼‰ã€‚
    # æ¢¯åº¦è£å‰ª: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ·±å±‚ç½‘ç»œæ—¶ã€‚
    # 5.
    # ç¡¬ä»¶ä¼˜åŒ–
    # GPUåŠ é€Ÿ: ç¡®ä¿ä½¿ç”¨é«˜æ€§èƒ½GPUè¿›è¡Œè®­ç»ƒã€‚
    # æ··åˆç²¾åº¦è®­ç»ƒ: ä½¿ç”¨FP16æ··åˆç²¾åº¦è®­ç»ƒä»¥åŠ é€Ÿè®¡ç®—å¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚
    # 6.
    # æ•°æ®å¢å¼º
    # æ—¶é—´åºåˆ—å¢å¼º: é€šè¿‡æ»‘åŠ¨çª—å£ã€æ—¶é—´åˆ‡ç‰‡ç­‰æ–¹æ³•ç”Ÿæˆæ›´å¤šæ ·æœ¬ã€‚
    # æ•°æ®å¹³æ»‘: ä½¿ç”¨ç§»åŠ¨å¹³å‡æˆ–å…¶ä»–å¹³æ»‘æ–¹æ³•å¢å¼ºæ•°æ®è´¨é‡ã€‚
    # 7.
    # è¯„ä¼°ä¸è°ƒè¯•
    # äº¤å‰éªŒè¯: ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
    # å¼‚å¸¸åˆ†æ•°åˆ†æ: æ£€æŸ¥å¼‚å¸¸åˆ†æ•°çš„åˆ†å¸ƒï¼Œè°ƒæ•´é˜ˆå€¼ä»¥æé«˜æ£€æµ‹æ•ˆæœã€‚
    # é€šè¿‡ä»¥ä¸Šæ–¹æ³•ï¼Œå¯ä»¥é€æ­¥ä¼˜åŒ–OmniAnomalyç®—æ³•çš„æ€§èƒ½ã€‚å»ºè®®ä»è¶…å‚æ•°è°ƒæ•´å’Œæ•°æ®é¢„å¤„ç†å¼€å§‹ï¼Œé€æ­¥æµ‹è¯•æ¯é¡¹æ”¹è¿›çš„æ•ˆæœã€‚