import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from EasyTSAD.Controller import TSADController
from EasyTSAD.Methods import BaseMethod
from EasyTSAD.DataFactory import MTSData
from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")

# ============= æ•°æ®é›†ç±»å®šä¹‰ =============
class MTSDataset(torch.utils.data.Dataset):
    def __init__(self, tsData: MTSData, set_type: str, window: int, horize: int):
        assert set_type in ['train', 'test']
        self.set_type = set_type
        self.window = window
        self.horize = horize        
        
        if set_type == "train":
            rawdata = tsData.train
        elif set_type == "test":
            rawdata = tsData.test
        else:
            raise ValueError('Arg "set_type" in MTSDataset() must be one of "train", "test"')

        self.len, self.var_num = rawdata.shape
        self.sample_num = max(self.len - self.window - self.horize + 1, 0)
        self.samples, self.labels = self.__getsamples(rawdata)

    def __getsamples(self, data):
        X = torch.zeros((self.sample_num, self.window, self.var_num))
        Y = torch.zeros((self.sample_num, 1, self.var_num))

        for i in range(self.sample_num):
            start = i
            end = i + self.window
            X[i, :, :] = torch.from_numpy(data[start:end, :])
            Y[i, :, :] = torch.from_numpy(data[end+self.horize-1, :])

        return (X, Y)

    def __len__(self):
        return self.sample_num

    def __getitem__(self, idx):
        sample = [self.samples[idx, :, :], self.labels[idx, :, :]]
        return sample

# ============= å¢å¼ºä¸“å®¶ç½‘ç»œç±»å®šä¹‰ =============
class HyperExpert(nn.Module):
    def __init__(self, n_kernel, window, n_multiv, hidden_size, output_size, drop_out=0.2):
        super(HyperExpert, self).__init__()
        # ğŸš€ å¢å¼ºå·ç§¯å±‚ - ä½¿ç”¨æ›´å¤šå·ç§¯æ ¸
        self.conv1 = nn.Conv2d(1, n_kernel, (window, 1))
        self.conv2 = nn.Conv2d(n_kernel, n_kernel * 2, (1, 1))  # æ–°å¢ç¬¬äºŒå±‚å·ç§¯
        self.batch_norm1 = nn.BatchNorm2d(n_kernel)  # æ·»åŠ æ‰¹å½’ä¸€åŒ–
        self.batch_norm2 = nn.BatchNorm2d(n_kernel * 2)
        
        self.dropout = nn.Dropout(drop_out)
        
        # ğŸš€ æ›´æ·±çš„å…¨è¿æ¥ç½‘ç»œ
        conv_output_size = n_kernel * 2 * n_multiv
        self.fc1 = nn.Linear(conv_output_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # æ–°å¢ä¸­é—´å±‚
        self.fc3 = nn.Linear(hidden_size // 2, output_size)
        
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)  # ä½¿ç”¨LeakyReLU
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        x = x.unsqueeze(dim=1).contiguous()
        
        # ç¬¬ä¸€å±‚å·ç§¯
        x = self.conv1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚å·ç§¯
        x = self.conv2(x)
        x = self.batch_norm2(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        
        # å±•å¹³
        out = torch.flatten(x, start_dim=1).contiguous()
        
        # æ·±åº¦å…¨è¿æ¥ç½‘ç»œ
        out = self.fc1(out)
        out = self.relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        out = self.leaky_relu(out)
        out = self.dropout(out)
        
        out = self.fc3(out)
        
        return out

# ============= å¢å¼ºå¡”ç½‘ç»œç±»å®šä¹‰ =============    
class HyperTower(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=32, drop_out=0.1):
        super(HyperTower, self).__init__()
        # ğŸš€ æ›´æ·±çš„å¡”ç½‘ç»œ
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # æ–°å¢ä¸­é—´å±‚
        self.fc3 = nn.Linear(hidden_size // 2, output_size)
        
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)
        self.dropout = nn.Dropout(drop_out)
        self.layer_norm1 = nn.LayerNorm(hidden_size)  # ğŸ”§ ä¿®å¤: ä½¿ç”¨LayerNormæ›¿ä»£BatchNorm1d
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):  # ğŸ”§ ä¿®å¤: æ›´æ–°åˆå§‹åŒ–
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        out = self.fc1(x)
        out = self.layer_norm1(out)  # ğŸ”§ ä¿®å¤: ä½¿ç”¨LayerNorm
        out = self.relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        out = self.leaky_relu(out)
        out = self.dropout(out)
        
        out = self.fc3(out)
        
        return out

# ============= HMMoEè¶…å‚æ•°æ¨¡å‹ç±»å®šä¹‰ =============
class HyperMMoEModel(nn.Module):
    def __init__(self, n_kernel, window, n_multiv, hidden_size, output_size, n_expert=8, 
                 sg_ratio=0.7, exp_dropout=0.2, tow_dropout=0.1, towers_hidden=32):
        super(HyperMMoEModel, self).__init__()
        self.n_kernel = n_kernel
        self.window = window
        self.n_multiv = n_multiv
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_expert = n_expert
        self.sg_ratio = sg_ratio
        self.softmax = nn.Softmax(dim=1)

        # ğŸš€ æ›´å¤šçš„ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            HyperExpert(n_kernel, window, n_multiv, hidden_size, output_size, exp_dropout)
            for _ in range(n_expert)
        ])
        
        # ğŸš€ å¢å¼ºé—¨æ§ç½‘ç»œ - æ·»åŠ æ›´å¤šå‚æ•°
        self.w_gates = nn.ParameterList([
            nn.Parameter(torch.randn(window, n_expert), requires_grad=True)
            for _ in range(n_multiv)
        ])
        self.share_gate = nn.Parameter(torch.randn(window, n_expert), requires_grad=True)
        
        # æ–°å¢é¢å¤–çš„é—¨æ§æƒé‡
        self.expert_bias = nn.Parameter(torch.randn(n_expert), requires_grad=True)
        self.gate_temperature = nn.Parameter(torch.ones(1), requires_grad=True)  # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
        
        # ğŸš€ æ›´æ·±çš„å¡”ç½‘ç»œ
        self.towers = nn.ModuleList([
            HyperTower(output_size, 1, towers_hidden, tow_dropout)
            for _ in range(n_multiv)
        ])
        
        # å‚æ•°åˆå§‹åŒ–
        self._init_parameters()
        
    def _init_parameters(self):
        for gate in self.w_gates:
            nn.init.xavier_normal_(gate)
        nn.init.xavier_normal_(self.share_gate)
        nn.init.constant_(self.expert_bias, 0)
        nn.init.constant_(self.gate_temperature, 1.0)

    def forward(self, x):
        # ä¸“å®¶ç½‘ç»œè¾“å‡º
        experts_out = [e(x) for e in self.experts]
        experts_out_tensor = torch.stack(experts_out)
        
        # ğŸš€ å¢å¼ºé—¨æ§ç½‘ç»œè¾“å‡º - æ·»åŠ æ¸©åº¦ç¼©æ”¾å’Œåç½®
        gates_out = []
        for i in range(self.n_multiv):
            gate_weight = (x[:,:,i] @ self.w_gates[i]) * (1 - self.sg_ratio) + \
                         (x[:,:,i] @ self.share_gate) * self.sg_ratio
            
            # æ·»åŠ ä¸“å®¶åç½®å’Œæ¸©åº¦ç¼©æ”¾
            gate_weight = (gate_weight + self.expert_bias) / self.gate_temperature
            gates_out.append(self.softmax(gate_weight))
        
        # é—¨æ§åŠ æƒä¸“å®¶è¾“å‡º
        tower_input = [
            g.t().unsqueeze(2).expand(-1, -1, self.output_size) * experts_out_tensor
            for g in gates_out
        ]
        tower_input = [torch.sum(ti, dim=0) for ti in tower_input]
        
        # å¡”ç½‘ç»œè¾“å‡º
        tower_output = [
            t(ti)
            for t, ti in zip(self.towers, tower_input)
        ]
        tower_output = torch.stack(tower_output, dim=0).permute(1,2,0)
        
        return tower_output

# ============= HMMoEå¼‚å¸¸æ£€æµ‹æ–¹æ³•ç±» =============
class HMMoE(BaseMethod):
    def __init__(self, params: dict) -> None:
        super().__init__()
        self.__anomaly_score = None
        self.model = None  # åˆå§‹åŒ–modelå±æ€§
        
        # ğŸš€ HMMoEè¶…å‚æ•°é…ç½® - å¤§å¹…å¢åŠ æ¨¡å‹å®¹é‡
        self.config = {
            'seed': 2023,
            'n_multiv': 38,          # æ ¹æ®æ•°æ®è°ƒæ•´
            'horize': 1,
            'window': 20,            # ğŸš€ å¢å¤§çª—å£å¤§å° 16â†’20
            'batch_size': 32,        # ğŸš€ é€‚å½“å‡å°æ‰¹é‡å¤§å°ä»¥å®¹çº³æ›´å¤§æ¨¡å‹
            'epochs': 12,            # ğŸš€ å¢åŠ è®­ç»ƒè½®æ•° 5â†’12

            'num_experts': 8,        # ğŸš€ å¤§å¹…å¢åŠ ä¸“å®¶æ•°é‡ 4â†’8
            'n_kernel': 16,          # ğŸš€ å¤§å¹…å¢åŠ å·ç§¯æ ¸æ•°é‡ 8â†’16
            'experts_out': 128,      # ğŸš€ å¢åŠ ä¸“å®¶è¾“å‡ºç»´åº¦ 64â†’128
            'experts_hidden': 256,   # ğŸš€ å¤§å¹…å¢åŠ ä¸“å®¶éšè—å±‚ 128â†’256
            'towers_hidden': 32,     # ğŸš€ å¢åŠ å¡”ç½‘ç»œéšè—å±‚ 16â†’32
            'criterion': 'l2',       # æŸå¤±å‡½æ•°
            'exp_dropout': 0.25,     # ğŸš€ é€‚å½“å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            'tow_dropout': 0.15,     # ğŸš€ é€‚å½“å¢åŠ dropout
            'sg_ratio': 0.8,         # ğŸš€ è°ƒæ•´å…±äº«é—¨æ§æ¯”ä¾‹
            'lr': 0.0005,            # ğŸš€ é™ä½å­¦ä¹ ç‡ 0.001â†’0.0005
            'weight_decay': 1e-4     # ğŸš€ æ·»åŠ æƒé‡è¡°å‡
        }
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(self.config['seed'])
        np.random.seed(self.config['seed'])
        
        print(f"ğŸš€ [LOG] HMMoEè¶…å‚æ•°ç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ’ª [LOG] è¶…å¤§é…ç½®: {self.config['num_experts']}ä¸“å®¶, {self.config['n_kernel']}å·ç§¯æ ¸, {self.config['epochs']}è½®è®­ç»ƒ")
        print(f"ğŸ¯ [LOG] è®¾è®¡ç†å¿µ: å¤§æ¨¡å‹å¤§æ•°æ®ï¼Œè¿½æ±‚æè‡´æ€§èƒ½")

    def train_valid_phase(self, tsData: MTSData):
        print(f"\nğŸš€ [LOG] ========== HMMoEè¶…å‚æ•°è®­ç»ƒé˜¶æ®µå¼€å§‹ ==========")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ [LOG] ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä½¿ç”¨configä¸­çš„å‚æ•°
        window_size = self.config['window']
        batch_size = self.config['batch_size']
        epochs = self.config['epochs']
        lr = self.config['lr']
        
        # ä»MTSDatasetè·å–æ•°æ®
        train_dataset = MTSDataset(tsData=tsData, set_type='train', window=window_size, horize=self.config['horize'])
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        
        # åŠ¨æ€è·å–æ•°æ®ç»´åº¦å¹¶æ›´æ–°config
        n_multiv = tsData.train.shape[1]
        self.config['n_multiv'] = n_multiv
        print(f"ğŸ“Š [LOG] æ•°æ®ç»´åº¦: {n_multiv}, è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        
        # ğŸš€ ä½¿ç”¨configä¸­çš„å‚æ•°åˆ›å»ºè¶…å¤§æ¨¡å‹
        self.model = HyperMMoEModel(
            n_kernel=self.config['n_kernel'],
            window=window_size,
            n_multiv=n_multiv,
            hidden_size=self.config['experts_hidden'],
            output_size=self.config['experts_out'],
            n_expert=self.config['num_experts'],
            sg_ratio=self.config['sg_ratio'],
            exp_dropout=self.config['exp_dropout'],
            tow_dropout=self.config['tow_dropout'],
            towers_hidden=self.config['towers_hidden']
        ).to(device)
        
        # ğŸš€ ä¼˜åŒ–å™¨é…ç½® - æ·»åŠ æƒé‡è¡°å‡
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=self.config['weight_decay'])
        
        # ğŸš€ å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)
        
        criterion = nn.MSELoss()
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ’ª [LOG] æ¨¡å‹å‚æ•°é‡: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
        self.model.train()
        print(f"ğŸš€ [LOG] å¼€å§‹è¶…å‚æ•°è®­ç»ƒï¼Œå…± {epochs} ä¸ªepoch")
        print(f"âš™ï¸ [LOG] ä½¿ç”¨å‚æ•°: window={window_size}, batch_size={batch_size}, lr={lr}")
        print(f"ğŸ—ï¸ [LOG] æ¨¡å‹é…ç½®: experts={self.config['num_experts']}, kernel={self.config['n_kernel']}, hidden={self.config['experts_hidden']}")
        
        # æ·»åŠ å¤–å±‚è¿›åº¦æ¡æ˜¾ç¤ºæ•´ä½“è®­ç»ƒè¿›åº¦
        epoch_bar = tqdm(range(epochs), desc="ğŸš€ HMMoEè¶…å‚æ•°è®­ç»ƒ", ncols=100)
        
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in epoch_bar:
            total_loss = 0
            batch_count = 0
            
            # å†…å±‚è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰epochçš„batchè¿›åº¦
            batch_bar = tqdm(train_loader, desc=f"ğŸ“Š Epoch {epoch+1}/{epochs}", leave=False, ncols=80)
            
            for batch_idx, (data, target) in enumerate(batch_bar):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                
                # ğŸš€ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
                
                # æ›´æ–°batchè¿›åº¦æ¡æ˜¾ç¤ºå½“å‰loss
                batch_bar.set_postfix({'loss': f'{loss.item():.6f}'})
            
            avg_loss = total_loss / batch_count if batch_count > 0 else 0
            
            # ğŸš€ å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_loss)
            
            # æ›´æ–°epochè¿›åº¦æ¡æ˜¾ç¤ºå¹³å‡loss
            epoch_bar.set_postfix({'avg_loss': f'{avg_loss:.6f}', 'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'})
            print(f"âœ… Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # ğŸš€ æ—©åœæœºåˆ¶
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
        epoch_bar.close()
        print(f"ğŸ‰ [LOG] ========== HMMoEè¶…å‚æ•°è®­ç»ƒé˜¶æ®µå®Œæˆ ==========")
        print(f"ğŸ“ˆ [LOG] æœ€ä½³æŸå¤±: {best_loss:.6f}")

    def test_phase(self, tsData: MTSData):
        print(f"\nğŸ” [LOG] ========== HMMoEè¶…å‚æ•°æµ‹è¯•é˜¶æ®µå¼€å§‹ ==========")
        print(f"ğŸ“Š [LOG] æµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä½¿ç”¨configä¸­çš„å‚æ•°
        window_size = self.config['window']
        batch_size = self.config['batch_size']
        
        # ä½¿ç”¨MTSDatasetå¤„ç†æµ‹è¯•æ•°æ®
        test_dataset = MTSDataset(tsData=tsData, set_type='test', window=window_size, horize=self.config['horize'])
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        
        self.model.eval()
        anomaly_scores = []
        
        print(f"ğŸ” [LOG] å¼€å§‹è¶…å‚æ•°æµ‹è¯•ï¼Œå…± {len(test_loader)} ä¸ªbatch")
        
        with torch.no_grad():
            for data, target in tqdm(test_loader, desc="ğŸ” HMMoEè¶…å‚æ•°æµ‹è¯•", ncols=80):
                data, target = data.to(device), target.to(device)
                
                output = self.model(data)
                mse = torch.mean((output - target) ** 2, dim=(1, 2))
                anomaly_scores.extend(mse.cpu().numpy())
        
        # è°ƒæ•´å¼‚å¸¸åˆ†æ•°é•¿åº¦ä»¥åŒ¹é…åŸå§‹æµ‹è¯•æ•°æ®
        full_scores = np.zeros(len(tsData.test))
        
        # å‰window_sizeä¸ªç‚¹ä½¿ç”¨0åˆ†æ•°
        full_scores[:window_size] = 0
        
        # ä»ç¬¬window_sizeä¸ªç‚¹å¼€å§‹ä½¿ç”¨å®é™…è®¡ç®—çš„åˆ†æ•°
        for i, score in enumerate(anomaly_scores):
            if i + window_size < len(full_scores):
                full_scores[i + window_size] = score
        
        self.__anomaly_score = full_scores
        print(f"ğŸ‰ [LOG] ========== HMMoEè¶…å‚æ•°æµ‹è¯•é˜¶æ®µå®Œæˆ ==========\n")

    def anomaly_score(self) -> np.ndarray:
        return self.__anomaly_score

    def param_statistic(self, save_file):
        if self.model is not None:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            param_info = f"""
                HMMoEè¶…å‚æ•°å¤šä¸“å®¶æ··åˆæ¨¡å‹å‚æ•°ç»Ÿè®¡:
                ==================================================
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                ä¸“å®¶æ•°é‡: {self.config['num_experts']}
                å·ç§¯æ ¸æ•°é‡: {self.config['n_kernel']}
                ä¸“å®¶éšè—å±‚: {self.config['experts_hidden']}
                ä¸“å®¶è¾“å‡ºç»´åº¦: {self.config['experts_out']}
                å¡”ç½‘ç»œéšè—å±‚: {self.config['towers_hidden']}
                çª—å£å¤§å°: {self.config['window']}
                æ‰¹é‡å¤§å°: {self.config['batch_size']}
                è®­ç»ƒè½®æ•°: {self.config['epochs']}
                å­¦ä¹ ç‡: {self.config['lr']}
                æƒé‡è¡°å‡: {self.config['weight_decay']}

                ğŸš€ è¶…å‚æ•°å¢å¼ºç‰¹æ€§:
                - åŒå±‚å·ç§¯ + æ‰¹å½’ä¸€åŒ–
                - 8ä¸ªä¸“å®¶ç½‘ç»œ (vs 4ä¸ª)
                - æ›´æ·±çš„å…¨è¿æ¥å±‚
                - å¯å­¦ä¹ é—¨æ§æ¸©åº¦
                - æ¢¯åº¦è£å‰ª
                - å­¦ä¹ ç‡è°ƒåº¦
                - æ—©åœæœºåˆ¶

                ==================================================
                è®¾è®¡ç†å¿µ: å¤§æ¨¡å‹å¤§æ•°æ®ï¼Œæè‡´æ€§èƒ½è¿½æ±‚
            """
        else:
            param_info = "HMMoEæ¨¡å‹å°šæœªåˆå§‹åŒ–"
            
        with open(save_file, 'w', encoding='utf-8') as f:
            f.write(param_info)

# ============= ä¸»ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    print("ğŸš€ ========== HMMoEè¶…å‚æ•°å¤šä¸“å®¶æ··åˆå¼‚å¸¸æ£€æµ‹ ==========")
    print("ğŸ’ª [LOG] ç¨‹åºå¼€å§‹æ‰§è¡Œ")
    
    # Create a global controller
    gctrl = TSADController()
    print("ğŸ”§ [LOG] TSADControllerå·²åˆ›å»º")
    
    # Set dataset
    datasets = ["machine-1", "machine-2", "machine-3"]
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )
    print("ğŸ“Š [LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    print("ğŸ—ï¸ [LOG] HMMoEç±»å®šä¹‰å®Œæˆ")

    """============= Run HMMoE algo. ============="""
    
    # some settings of this anomaly detection method
    method = "HMMoE"  # string of your algo class

    print(f"ğŸš€ [LOG] å¼€å§‹è¿è¡Œè¶…å‚æ•°å®éªŒï¼Œmethod={method}")
    # run models
    gctrl.run_exps(
        method=method,
        training_schema="mts",
        hparams={
            "window": 20,        # ğŸš€ å¢å¤§çª—å£
            "batch_size": 32,    # ğŸš€ é€‚ä¸­æ‰¹é‡å¤§å°
            "epochs": 12,        # ğŸš€ å¢åŠ è®­ç»ƒè½®æ•°
            "lr": 0.0005,        # ğŸš€ é™ä½å­¦ä¹ ç‡
        },
        # use which method to preprocess original data. 
        preprocess="z-score",
    )
    print("ğŸ‰ [LOG] è¶…å‚æ•°å®éªŒè¿è¡Œå®Œæˆ")
       
        
    """============= [EVALUATION SETTINGS] ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    # Specifying evaluation protocols
    print("ğŸ“Š [LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals(
        [
            PointF1PA(),
            EventF1PA(),
            EventF1PA(mode="squeeze")
        ]
    )
    print("âœ… [LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("ğŸ” [LOG] å¼€å§‹æ‰§è¡Œè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema="mts"
    )
    print("ğŸ‰ [LOG] è¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
        
    """============= [PLOTTING SETTINGS] ============="""
    
    # plot anomaly scores for each curve
    print("ğŸ“ˆ [LOG] å¼€å§‹ç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema="mts"
    )
    print("ğŸ¨ [LOG] ç»˜å›¾å®Œæˆ")
    
    print("ğŸš€ ========== HMMoEè¶…å‚æ•°ç‰ˆæ‰§è¡Œå®Œæ¯• ==========")