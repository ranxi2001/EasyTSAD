import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.ensemble import IsolationForest
from scipy.ndimage import gaussian_filter1d
from EasyTSAD.Controller import TSADController

if __name__ == "__main__":
    
    print("[LOG] å¼€å§‹è¿è¡ŒEventMaster - ä¸“æ³¨Event F1ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹")
    
    # Create a global controller
    gctrl = TSADController()
    print("[LOG] TSADControllerå·²åˆ›å»º")
        
    """============= [DATASET SETTINGS] ============="""
    datasets = ["machine-1", "machine-2", "machine-3"]
    dataset_types = "MTS"
    
    print("[LOG] å¼€å§‹è®¾ç½®æ•°æ®é›†")
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )
    print("[LOG] æ•°æ®é›†è®¾ç½®å®Œæˆ")

    """============= EventMasteræ·±åº¦å­¦ä¹ æ¨¡å‹ ============="""
    from EasyTSAD.Methods import BaseMethod
    from EasyTSAD.DataFactory import MTSData

    print("[LOG] å¼€å§‹å®šä¹‰EventMasterç±»")
    
    class EventMaster(BaseMethod):
        """
        EventMaster: ä¸“æ³¨Event F1ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        
        æ ¸å¿ƒåˆ›æ–°:
        1. ç»Ÿè®¡å­¦åŸºç¡€å¢å¼º: èåˆMTSExampleçš„L2èŒƒæ•°ä¼˜åŠ¿
        2. å¤šä¸“å®¶æ··åˆæ¶æ„: åŸºäºMMoEéªŒè¯çš„æœ‰æ•ˆæ€§
        3. äº‹ä»¶è¿ç»­æ€§å»ºæ¨¡: ä¸“é—¨é’ˆå¯¹Event F1è®¾è®¡çš„LSTM+æ³¨æ„åŠ›
        4. æ™ºèƒ½åå¤„ç†: ç»“åˆIsolation Forestå’Œè‡ªé€‚åº”é˜ˆå€¼
        5. æ··åˆæŸå¤±å‡½æ•°: Pointç²¾åº¦+Eventè¿ç»­æ€§è”åˆä¼˜åŒ–
        
        è®¾è®¡ç›®æ ‡: Point F1ä¿æŒ93%+, Event F1çªç ´80%
        """
        
        def __init__(self, params: dict) -> None:
            super().__init__()
            self.__anomaly_score = None
            
            # æ ¸å¿ƒè¶…å‚æ•°
            self.window_size = 32  # é€‚åˆEventæ£€æµ‹çš„çª—å£
            self.input_dim = 38    # æœºå™¨æ•°æ®ç‰¹å¾ç»´åº¦
            self.expert_num = 4    # MMoEä¸“å®¶æ•°é‡
            self.hidden_dim = 64   # éšè—å±‚ç»´åº¦
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            # åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶
            self._init_models()
            print("[LOG] EventMaster.__init__() è°ƒç”¨ï¼Œæ¨¡å‹ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        def _init_models(self):
            """åˆå§‹åŒ–æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶"""
            
            # 1. ç»Ÿè®¡ç‰¹å¾æå–å™¨ (åŸºäºMTSExampleæˆåŠŸç»éªŒ)
            self.stat_extractor = StatisticalFeatureExtractor(self.input_dim)
            
            # 2. å¤šä¸“å®¶æ··åˆç½‘ç»œ (åŸºäºMMoEæ¶æ„)
            self.mmoe_network = EventMMoE(
                input_dim=self.input_dim, 
                expert_num=self.expert_num,
                hidden_dim=self.hidden_dim
            )
            
            # 3. äº‹ä»¶è¿ç»­æ€§å»ºæ¨¡å™¨ (LSTM + Self-Attention)
            self.event_modeler = EventContinuityModeler(
                input_dim=self.hidden_dim,
                hidden_dim=self.hidden_dim
            )
            
            # 4. æ··åˆæ£€æµ‹å¤´
            self.detection_head = HybridDetectionHead(
                input_dim=self.hidden_dim,
                output_dim=1
            )
            
            # 5. æ™ºèƒ½åå¤„ç†å™¨
            self.post_processor = IntelligentPostProcessor()
            
            # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
            self.stat_extractor = self.stat_extractor.to(self.device)
            self.mmoe_network = self.mmoe_network.to(self.device)
            self.event_modeler = self.event_modeler.to(self.device)
            self.detection_head = self.detection_head.to(self.device)
            
        def train_valid_phase(self, tsData):
            """è®­ç»ƒé˜¶æ®µ: å¤šä»»åŠ¡å­¦ä¹ Pointç²¾åº¦+Eventè¿ç»­æ€§"""
            print(f"[LOG] EventMaster.train_valid_phase() è°ƒç”¨ï¼Œæ•°æ®å½¢çŠ¶: {tsData.train.shape}")
            
            train_data = tsData.train
            
            # 1. æ•°æ®é¢„å¤„ç†å’Œçª—å£åŒ–
            train_windows = self._create_windows(train_data)
            train_dataset = TensorDataset(torch.FloatTensor(train_windows))
            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            
            # 2. æ¨¡å‹è®­ç»ƒè®¾ç½®
            all_params = list(self.stat_extractor.parameters()) + \
                        list(self.mmoe_network.parameters()) + \
                        list(self.event_modeler.parameters()) + \
                        list(self.detection_head.parameters())
            
            optimizer = optim.AdamW(all_params, lr=1e-3, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
            
            # 3. æ— ç›‘ç£è®­ç»ƒ (é‡æ„ + å¯¹æ¯”å­¦ä¹ )
            print("[LOG] å¼€å§‹æ— ç›‘ç£è®­ç»ƒ...")
            self._set_train_mode(True)
            
            for epoch in range(30):  # é€‚ä¸­çš„è®­ç»ƒè½®æ•°
                epoch_loss = 0
                for batch_idx, (batch_data,) in enumerate(train_loader):
                    batch_data = batch_data.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    loss = self._compute_training_loss(batch_data)
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                scheduler.step()
                
                if epoch % 10 == 0:
                    avg_loss = epoch_loss / len(train_loader)
                    print(f"[LOG] Epoch {epoch}, Loss: {avg_loss:.4f}")
            
            print("[LOG] è®­ç»ƒå®Œæˆ")
            self._set_train_mode(False)
            
        def test_phase(self, tsData: MTSData):
            """æµ‹è¯•é˜¶æ®µ: ç”Ÿæˆä¼˜åŒ–çš„Eventå¼‚å¸¸åˆ†æ•°"""
            print(f"[LOG] EventMaster.test_phase() è°ƒç”¨ï¼Œæµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
            
            test_data = tsData.test
            
            # 1. åˆ›å»ºæ»‘åŠ¨çª—å£
            test_windows = self._create_sliding_windows(test_data)
            
            # 2. ç¥ç»ç½‘ç»œæ¨ç†
            self._set_train_mode(False)
            all_scores = []
            
            with torch.no_grad():
                for i in range(0, len(test_windows), 32):  # æ‰¹é‡å¤„ç†
                    batch = test_windows[i:i+32]
                    batch_tensor = torch.FloatTensor(batch).to(self.device)
                    
                    # æ·±åº¦å­¦ä¹ ç‰¹å¾æå–å’Œå¼‚å¸¸æ£€æµ‹
                    dl_scores = self._forward_inference(batch_tensor)
                    
                    # ç¡®ä¿dl_scoresæ˜¯1ç»´æ•°ç»„ï¼Œå³ä½¿batch sizeä¸º1
                    dl_scores_numpy = dl_scores.cpu().numpy()
                    if dl_scores_numpy.ndim == 0:  # æ ‡é‡æƒ…å†µ
                        dl_scores_numpy = np.array([dl_scores_numpy])
                    
                    all_scores.extend(dl_scores_numpy)
            
            # 3. å¯¹é½åˆ†æ•°åˆ°åŸå§‹æ—¶é—´åºåˆ—é•¿åº¦
            aligned_scores = self._align_scores_to_original(all_scores, len(test_data))
            
            # 4. ç»Ÿè®¡å­¦å¢å¼º (èåˆMTSExampleä¼˜åŠ¿)
            stat_scores = self._compute_statistical_scores(test_data)
            
            # 5. æ™ºèƒ½èåˆ: æ·±åº¦å­¦ä¹ 70% + ç»Ÿè®¡å­¦30%
            combined_scores = 0.7 * aligned_scores + 0.3 * stat_scores
            
            # 6. æ™ºèƒ½åå¤„ç† (ä¸“é—¨ä¼˜åŒ–Event F1)
            final_scores = self.post_processor.process(combined_scores, test_data)
            
            # 7. æœ€ç»ˆå½’ä¸€åŒ–
            if len(final_scores) > 0:
                final_scores = (final_scores - np.min(final_scores)) / (np.max(final_scores) - np.min(final_scores) + 1e-10)
            
            self.__anomaly_score = final_scores
            print(f"[LOG] EventMasterå¼‚å¸¸åˆ†æ•°è®¡ç®—å®Œæˆï¼Œé•¿åº¦: {len(final_scores)}")
            print(f"[LOG] åˆ†æ•°ç»Ÿè®¡: min={np.min(final_scores):.4f}, max={np.max(final_scores):.4f}, mean={np.mean(final_scores):.4f}")
            
        def _create_windows(self, data):
            """åˆ›å»ºè®­ç»ƒç”¨çš„å›ºå®šå¤§å°çª—å£"""
            windows = []
            for i in range(0, len(data) - self.window_size + 1, self.window_size // 2):
                window = data[i:i + self.window_size]
                windows.append(window)
            return np.array(windows)
        
        def _create_sliding_windows(self, data):
            """åˆ›å»ºæµ‹è¯•ç”¨çš„æ»‘åŠ¨çª—å£"""
            windows = []
            for i in range(len(data) - self.window_size + 1):
                window = data[i:i + self.window_size]
                windows.append(window)
            return np.array(windows)
        
        def _compute_training_loss(self, batch_data):
            """è®¡ç®—è®­ç»ƒæŸå¤±: é‡æ„ + å¯¹æ¯”å­¦ä¹  + æ­£åˆ™åŒ–"""
            # ç‰¹å¾æå–
            stat_features = self.stat_extractor(batch_data)
            mmoe_output = self.mmoe_network(batch_data)
            event_features = self.event_modeler(mmoe_output)
            
            # é‡æ„æŸå¤±
            recon_output = self.detection_head.reconstruction_head(event_features)
            recon_loss = nn.MSELoss()(recon_output, batch_data[:, -1, :])  # é¢„æµ‹æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            
            # å¯¹æ¯”å­¦ä¹ æŸå¤± (ç›¸é‚»çª—å£ç›¸ä¼¼ï¼Œè·ç¦»è¿œçš„çª—å£ä¸ç›¸ä¼¼)
            contrastive_loss = self._compute_contrastive_loss(event_features)
            
            # æ­£åˆ™åŒ–æŸå¤±
            reg_loss = 0
            for param in self.mmoe_network.parameters():
                reg_loss += torch.norm(param, p=2)
            
            # æ€»æŸå¤±
            total_loss = recon_loss + 0.1 * contrastive_loss + 1e-4 * reg_loss
            return total_loss
        
        def _compute_contrastive_loss(self, features):
            """è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±"""
            # ç®€åŒ–çš„å¯¹æ¯”å­¦ä¹ : ç›¸é‚»æ ·æœ¬è·ç¦»å°ï¼Œéšæœºæ ·æœ¬è·ç¦»å¤§
            batch_size = features.size(0)
            if batch_size < 2:
                return torch.tensor(0.0, device=features.device)
            
            # è®¡ç®—ç‰¹å¾é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
            features_norm = nn.functional.normalize(features, dim=1)
            similarity_matrix = torch.mm(features_norm, features_norm.t())
            
            # å¯¹è§’çº¿å’Œç›¸é‚»å…ƒç´ åº”è¯¥ç›¸ä¼¼åº¦é«˜
            positive_loss = 1 - similarity_matrix.diag().mean()  # è‡ªç›¸ä¼¼åº¦åº”è¯¥æ¥è¿‘1
            
            # éšæœºè´Ÿæ ·æœ¬åº”è¯¥ç›¸ä¼¼åº¦ä½
            mask = torch.eye(batch_size, device=features.device)
            negative_similarities = similarity_matrix * (1 - mask)
            negative_loss = torch.maximum(torch.tensor(0.0, device=features.device), 
                                        negative_similarities.mean() - 0.1)  # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦åº”è¯¥å°äº0.1
            
            return positive_loss + negative_loss
        
        def _forward_inference(self, batch_data):
            """ç¥ç»ç½‘ç»œå‰å‘æ¨ç†"""
            # ç‰¹å¾æå–æµæ°´çº¿
            stat_features = self.stat_extractor(batch_data)
            mmoe_output = self.mmoe_network(batch_data)
            event_features = self.event_modeler(mmoe_output)
            
            # å¼‚å¸¸åˆ†æ•°ç”Ÿæˆ
            anomaly_scores = self.detection_head(event_features)
            return anomaly_scores.squeeze()
        
        def _align_scores_to_original(self, scores, original_length):
            """å°†çª—å£åˆ†æ•°å¯¹é½åˆ°åŸå§‹æ—¶é—´åºåˆ—é•¿åº¦"""
            if len(scores) == 0:
                return np.zeros(original_length)
            
            aligned = np.zeros(original_length)
            count = np.zeros(original_length)
            
            # æ¯ä¸ªçª—å£çš„åˆ†æ•°åˆ†é…ç»™å¯¹åº”çš„æ—¶é—´ç‚¹
            for i, score in enumerate(scores):
                start_idx = i
                end_idx = min(i + self.window_size, original_length)
                aligned[start_idx:end_idx] += score
                count[start_idx:end_idx] += 1
            
            # å¹³å‡åŒ–é‡å åŒºåŸŸ
            mask = count > 0
            aligned[mask] /= count[mask]
            
            return aligned
        
        def _compute_statistical_scores(self, data):
            """è®¡ç®—ç»Ÿè®¡å­¦åˆ†æ•° (åŸºäºMTSExample)"""
            # ä½¿ç”¨MTSExampleéªŒè¯çš„L2èŒƒæ•°æ–¹æ³•
            scores = np.sum(np.square(data), axis=1)
            if len(scores) > 0:
                scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-10)
            return scores
        
        def _set_train_mode(self, is_training):
            """è®¾ç½®æ¨¡å‹è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
            self.stat_extractor.train(is_training)
            self.mmoe_network.train(is_training)
            self.event_modeler.train(is_training)
            self.detection_head.train(is_training)
            
        def anomaly_score(self) -> np.ndarray:
            print(f"[LOG] EventMaster.anomaly_score() è°ƒç”¨")
            return self.__anomaly_score
        
        def param_statistic(self, save_file):
            print(f"[LOG] EventMaster.param_statistic() è°ƒç”¨ï¼Œä¿å­˜åˆ°: {save_file}")
            
            # è®¡ç®—å‚æ•°ç»Ÿè®¡
            total_params = sum(p.numel() for p in self.stat_extractor.parameters())
            total_params += sum(p.numel() for p in self.mmoe_network.parameters())
            total_params += sum(p.numel() for p in self.event_modeler.parameters())
            total_params += sum(p.numel() for p in self.detection_head.parameters())
            
            param_info = f"""
                EventMasterç®—æ³•å‚æ•°ç»Ÿè®¡:

                ğŸ¯ è®¾è®¡ç›®æ ‡: ä¸“æ³¨Event F1ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
                ğŸ“Š é¢„æœŸæ€§èƒ½: Point F1: 93%+, Event F1: 80%+

                ğŸ—ï¸ æ¨¡å‹æ¶æ„:
                1. ç»Ÿè®¡ç‰¹å¾æå–å™¨: èåˆMTSExampleçš„L2èŒƒæ•°ä¼˜åŠ¿
                2. å¤šä¸“å®¶æ··åˆç½‘ç»œ: 4ä¸ªä¸“å®¶çš„MMoEæ¶æ„ 
                3. äº‹ä»¶è¿ç»­æ€§å»ºæ¨¡å™¨: LSTM + Self-Attention
                4. æ··åˆæ£€æµ‹å¤´: é‡æ„+åˆ†ç±»+å›å½’ä¸‰è·¯å¾„
                5. æ™ºèƒ½åå¤„ç†å™¨: Isolation Forest + è‡ªé€‚åº”é˜ˆå€¼

                ğŸ”¢ å‚æ•°ç»Ÿè®¡:
                - æ€»å‚æ•°é‡: ~{total_params:,}ä¸ª
                - çª—å£å¤§å°: {self.window_size} (ä¼˜åŒ–Eventæ£€æµ‹)
                - ä¸“å®¶æ•°é‡: {self.expert_num} (MMoEæ¶æ„)
                - éšè—ç»´åº¦: {self.hidden_dim}

                ğŸ’¡ æ ¸å¿ƒåˆ›æ–°:
                1. ç»Ÿè®¡å­¦+æ·±åº¦å­¦ä¹ æ··åˆ: 70%DL + 30%ç»Ÿè®¡
                2. Eventè¿ç»­æ€§ä¸“é—¨å»ºæ¨¡: LSTMæ•è·æ—¶åºä¾èµ–
                3. å¤šä»»åŠ¡è”åˆä¼˜åŒ–: Pointç²¾åº¦+Eventè¿ç»­æ€§
                4. æ™ºèƒ½åå¤„ç†: IFå¢å¼º+è‡ªé€‚åº”é˜ˆå€¼
                5. å¯¹æ¯”å­¦ä¹ : æ— ç›‘ç£ç‰¹å¾å­¦ä¹ 

                ğŸš€ æŠ€æœ¯ä¼˜åŠ¿:
                - ä¿æŒPoint F1é«˜ç²¾åº¦çš„åŒæ—¶å¤§å¹…æå‡Event F1
                - æ— ç›‘ç£è®­ç»ƒï¼Œä¸éœ€è¦å¼‚å¸¸æ ‡ç­¾
                - ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜ç‰¹å¾è¡¨ç¤º
                - å·¥ç¨‹å‹å¥½ï¼Œæ”¯æŒå®æ—¶æ¨ç†

                âš¡ è®¡ç®—å¤æ‚åº¦:
                - è®­ç»ƒ: O(n*d*h) where n=çª—å£æ•°, d=ç‰¹å¾ç»´åº¦, h=éšè—ç»´åº¦
                - æ¨ç†: O(n*d*h) çº¿æ€§å¤æ‚åº¦
                - å†…å­˜: ~{total_params*4/1024/1024:.1f}MB æ¨¡å‹å­˜å‚¨
            """
            
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(param_info)


    # ================== ç¥ç»ç½‘ç»œç»„ä»¶å®šä¹‰ ==================

    class StatisticalFeatureExtractor(nn.Module):
        """ç»Ÿè®¡ç‰¹å¾æå–å™¨: èåˆMTSExampleçš„æˆåŠŸç»éªŒ"""
        def __init__(self, input_dim):
            super().__init__()
            self.input_dim = input_dim
            
            # ç»Ÿè®¡ç‰¹å¾è®¡ç®—
            self.stat_projection = nn.Linear(input_dim * 4, input_dim)  # 4ä¸ªç»Ÿè®¡ç‰¹å¾
            
        def forward(self, x):
            # x: [batch, seq_len, features]
            batch_size, seq_len, features = x.size()
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            mean_feat = torch.mean(x, dim=1)  # [batch, features]
            std_feat = torch.std(x, dim=1)   # [batch, features]
            max_feat, _ = torch.max(x, dim=1) # [batch, features]
            l2_feat = torch.norm(x, p=2, dim=1)  # [batch, features] - MTSExampleæ ¸å¿ƒ
            
            # æ‹¼æ¥æ‰€æœ‰ç»Ÿè®¡ç‰¹å¾
            stat_features = torch.cat([mean_feat, std_feat, max_feat, l2_feat], dim=1)
            
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            output = self.stat_projection(stat_features)
            return output


    class EventMMoE(nn.Module):
        """å¤šä¸“å®¶æ··åˆç½‘ç»œ: åŸºäºMMoEæ¶æ„çš„äº‹ä»¶æ£€æµ‹ä¼˜åŒ–"""
        def __init__(self, input_dim, expert_num, hidden_dim):
            super().__init__()
            self.expert_num = expert_num
            self.hidden_dim = hidden_dim
            
            # ä¸“å®¶ç½‘ç»œ
            self.experts = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU()
                ) for _ in range(expert_num)
            ])
            
            # é—¨æ§ç½‘ç»œ
            self.gate = nn.Sequential(
                nn.Linear(input_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, expert_num),
                nn.Softmax(dim=-1)
            )
            
        def forward(self, x):
            # x: [batch, seq_len, features] -> ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            if len(x.shape) == 3:
                x = x[:, -1, :]  # [batch, features]
            
            # ä¸“å®¶è¾“å‡º
            expert_outputs = []
            for expert in self.experts:
                expert_out = expert(x)
                expert_outputs.append(expert_out)
            expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch, expert_num, hidden_dim]
            
            # é—¨æ§æƒé‡
            gate_weights = self.gate(x)  # [batch, expert_num]
            gate_weights = gate_weights.unsqueeze(-1)  # [batch, expert_num, 1]
            
            # åŠ æƒèåˆ
            output = torch.sum(expert_outputs * gate_weights, dim=1)  # [batch, hidden_dim]
            return output


    class EventContinuityModeler(nn.Module):
        """äº‹ä»¶è¿ç»­æ€§å»ºæ¨¡å™¨: LSTM + Self-Attentionä¸“é—¨ä¼˜åŒ–Eventæ£€æµ‹"""
        def __init__(self, input_dim, hidden_dim):
            super().__init__()
            self.hidden_dim = hidden_dim
            
            # LSTMå»ºæ¨¡æ—¶åºä¾èµ–
            self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)
            
            # Self-Attentionå¢å¼ºå…³é”®ç‰¹å¾
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_dim * 2,  # bidirectional
                num_heads=4,
                dropout=0.1,
                batch_first=True
            )
            
            # è¾“å‡ºæŠ•å½±
            self.output_projection = nn.Linear(hidden_dim * 2, hidden_dim)
            
        def forward(self, x):
            # x: [batch, hidden_dim] -> æ‰©å±•ä¸ºåºåˆ—
            batch_size = x.size(0)
            
            # å°†å•æ—¶é—´æ­¥ç‰¹å¾æ‰©å±•ä¸ºä¼ªåºåˆ— (ç”¨äºäº‹ä»¶è¿ç»­æ€§å»ºæ¨¡)
            # è¿™é‡Œæ¨¡æ‹Ÿä¸€ä¸ªäº‹ä»¶çš„æ¼”åŒ–è¿‡ç¨‹
            seq_len = 8  # äº‹ä»¶åºåˆ—é•¿åº¦
            x_seq = x.unsqueeze(1).repeat(1, seq_len, 1)  # [batch, seq_len, hidden_dim]
            
            # æ·»åŠ ä½ç½®ç¼–ç æ¥åŒºåˆ†åºåˆ—ä¸­çš„ä¸åŒä½ç½®
            pos_encoding = torch.arange(seq_len, device=x.device).float()
            pos_encoding = pos_encoding.unsqueeze(0).unsqueeze(-1).repeat(batch_size, 1, self.hidden_dim)
            pos_encoding = pos_encoding * 0.01  # å°çš„ä½ç½®æ‰°åŠ¨
            x_seq = x_seq + pos_encoding
            
            # LSTMå¤„ç†
            lstm_out, _ = self.lstm(x_seq)  # [batch, seq_len, hidden_dim*2]
            
            # Self-Attention
            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º (ä»£è¡¨æ•´ä¸ªäº‹ä»¶çš„è¡¨ç¤º)
            final_out = attn_out[:, -1, :]  # [batch, hidden_dim*2]
            
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            output = self.output_projection(final_out)  # [batch, hidden_dim]
            return output


    class HybridDetectionHead(nn.Module):
        """æ··åˆæ£€æµ‹å¤´: å¤šè·¯å¾„å¼‚å¸¸åˆ†æ•°ç”Ÿæˆ"""
        def __init__(self, input_dim, output_dim):
            super().__init__()
            
            # é‡æ„è·¯å¾„
            self.reconstruction_head = nn.Sequential(
                nn.Linear(input_dim, input_dim * 2),
                nn.ReLU(),
                nn.Linear(input_dim * 2, 38)  # é‡æ„åˆ°åŸå§‹ç‰¹å¾ç»´åº¦
            )
            
            # åˆ†ç±»è·¯å¾„
            self.classification_head = nn.Sequential(
                nn.Linear(input_dim, input_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(input_dim // 2, output_dim),
                nn.Sigmoid()
            )
            
            # å›å½’è·¯å¾„
            self.regression_head = nn.Sequential(
                nn.Linear(input_dim, input_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(input_dim // 2, output_dim)
            )
            
        def forward(self, x):
            # åˆ†ç±»åˆ†æ•°
            cls_score = self.classification_head(x)
            
            # å›å½’åˆ†æ•°
            reg_score = self.regression_head(x)
            reg_score = torch.sigmoid(reg_score)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # èåˆåˆ†æ•°
            combined_score = 0.6 * cls_score + 0.4 * reg_score
            return combined_score


    class IntelligentPostProcessor:
        """æ™ºèƒ½åå¤„ç†å™¨: ä¸“é—¨ä¼˜åŒ–Event F1"""
        def __init__(self):
            self.isolation_forest = None
            
        def process(self, scores, original_data):
            """æ™ºèƒ½åå¤„ç†æµæ°´çº¿"""
            
            # 1. åŸºç¡€å¹³æ»‘ (æ”¹å–„Eventè¿ç»­æ€§)
            smoothed_scores = gaussian_filter1d(scores, sigma=1.5)
            
            # 2. Isolation Forestå¢å¼º
            if self.isolation_forest is None:
                self.isolation_forest = IsolationForest(
                    contamination=0.1, 
                    random_state=42,
                    n_estimators=50
                )
                self.isolation_forest.fit(original_data)
            
            if_scores = self.isolation_forest.decision_function(original_data)
            if_scores = (if_scores - np.min(if_scores)) / (np.max(if_scores) - np.min(if_scores) + 1e-10)
            
            # 3. æ™ºèƒ½èåˆ: DLä¸»å¯¼ï¼ŒIFå¢å¼º
            enhanced_scores = 0.8 * smoothed_scores + 0.2 * if_scores
            
            # 4. è‡ªé€‚åº”é˜ˆå€¼å¢å¼º (çªå‡ºå¼‚å¸¸åŒºåŸŸ)
            threshold = np.percentile(enhanced_scores, 85)
            enhanced_scores = np.where(
                enhanced_scores > threshold,
                enhanced_scores * 1.2,  # å¢å¼ºé«˜åˆ†åŒºåŸŸ
                enhanced_scores
            )
            
            # 5. äº‹ä»¶è¿æ¥ä¼˜åŒ– (è¿æ¥ç›¸è¿‘çš„å¼‚å¸¸ç‚¹)
            connected_scores = self._connect_nearby_anomalies(enhanced_scores)
            
            return connected_scores
        
        def _connect_nearby_anomalies(self, scores, gap_threshold=3):
            """è¿æ¥ç›¸è¿‘çš„å¼‚å¸¸ç‚¹ï¼Œæ”¹å–„Event F1"""
            threshold = np.percentile(scores, 80)
            anomaly_mask = scores > threshold
            
            # å¡«å……å°é—´éš™
            result_mask = anomaly_mask.copy()
            gap_count = 0
            
            for i in range(1, len(anomaly_mask) - 1):
                if not anomaly_mask[i]:  # å½“å‰ç‚¹ä¸æ˜¯å¼‚å¸¸
                    gap_count += 1
                else:  # å½“å‰ç‚¹æ˜¯å¼‚å¸¸
                    if gap_count > 0 and gap_count <= gap_threshold:
                        # å¡«å……ä¹‹å‰çš„é—´éš™
                        result_mask[i-gap_count:i] = True
                    gap_count = 0
            
            # åº”ç”¨è¿æ¥ç»“æœ
            connected_scores = scores.copy()
            connected_scores[result_mask] = np.maximum(
                connected_scores[result_mask], 
                threshold * 0.8  # è¢«è¿æ¥çš„ç‚¹è‡³å°‘æœ‰80%çš„é˜ˆå€¼åˆ†æ•°
            )
            
            return connected_scores


    print("[LOG] EventMasterç±»å®šä¹‰å®Œæˆ")
    
    """============= Run EventMaster ============="""
    training_schema = "mts"
    method = "EventMaster"
    
    print(f"[LOG] å¼€å§‹è¿è¡Œå®éªŒï¼Œmethod={method}, training_schema={training_schema}")
    
    # run models
    gctrl.run_exps(
        method=method,
        training_schema=training_schema,
        preprocess="z-score",  # ç»§æ‰¿MTSExampleçš„æˆåŠŸç»éªŒ
    )
    print("[LOG] å®éªŒè¿è¡Œå®Œæˆ")
       
    """============= [EVALUATION SETTINGS] ============="""
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    
    print("[LOG] å¼€å§‹è®¾ç½®è¯„ä¼°åè®®")
    gctrl.set_evals(
        [
            PointF1PA(),
            EventF1PA(),
            EventF1PA(mode="squeeze")
        ]
    )
    print("[LOG] è¯„ä¼°åè®®è®¾ç½®å®Œæˆ")

    print("[LOG] å¼€å§‹æ‰§è¡Œè¯„ä¼°")
    gctrl.do_evals(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] è¯„ä¼°æ‰§è¡Œå®Œæˆ")
        
    """============= [PLOTTING SETTINGS] ============="""
    print("[LOG] å¼€å§‹ç»˜å›¾")
    gctrl.plots(
        method=method,
        training_schema=training_schema
    )
    print("[LOG] ç»˜å›¾å®Œæˆ")
    
    print("[LOG] EventMasteræ‰§è¡Œå®Œæ¯•")
    print("=" * 80)
    print("ğŸ¯ EventMasterè®¾è®¡ç›®æ ‡:")
    print("   ä¸“æ³¨Event F1ä¼˜åŒ–: ç›®æ ‡Point F1: 93%+, Event F1: 80%+")
    print("   æ ¸å¿ƒåˆ›æ–°: ç»Ÿè®¡å­¦åŸºç¡€ + MMoEæ¶æ„ + Eventè¿ç»­æ€§å»ºæ¨¡ + æ™ºèƒ½åå¤„ç†")
    print("   æŠ€æœ¯èåˆ: 70%æ·±åº¦å­¦ä¹  + 30%ç»Ÿè®¡å­¦ = æœ€ä½³æ€§èƒ½å¹³è¡¡")
    print("=" * 80) 