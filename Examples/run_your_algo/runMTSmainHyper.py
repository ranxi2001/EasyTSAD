from typing import Dict
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.fft
import math
from torch.utils.data import DataLoader, Dataset
import tqdm
import logging
import time
import os
from datetime import datetime
from EasyTSAD.Controller import TSADController

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger(name, log_file=None, level=logging.INFO):
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    formatter = logging.Formatter(
        '[%(asctime)s] %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    logger.handlers.clear()
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

if __name__ == "__main__":
    
    # Create a global controller
    gctrl = TSADController()
        
    """============= [DATASET SETTINGS] ============="""
    # Specifying datasets
    datasets = ["machine-1", "machine-2", "machine-3"]
    # datasets = ["TODS"]
    dataset_types = "MTS"
    #
    # set datasets path, dirname is the absolute/relative path of dataset.
    
    # Use all curves in datasets:
    gctrl.set_dataset(
        dataset_type="MTS",
        dirname="./datasets",
        datasets=datasets,
    )

    """============= Impletment your algo. ============="""
    from EasyTSAD.Methods import BaseMethod
    from EasyTSAD.DataFactory import MTSData
    """============= å®ç° TimesNet ç®—æ³• ============="""


    """=============== TimesNet ç»„ä»¶å®ç° ==============="""
    def FFT_for_Period(x, k=2):
        # [B, T, C]
        xf = torch.fft.rfft(x, dim=1)
        # find period by amplitudes
        frequency_list = abs(xf).mean(0).mean(-1)
        frequency_list[0] = 0
        _, top_list = torch.topk(frequency_list, k)
        top_list = top_list.detach().cpu().numpy()
        period = x.shape[1] // top_list
        return period, abs(xf).mean(-1)[:, top_list]

    # Embedding å±‚å®ç°
    class PositionalEmbedding(nn.Module):
        def __init__(self, d_model, max_len=5000):
            super(PositionalEmbedding, self).__init__()
            # Compute the positional encodings once in log space.
            pe = torch.zeros(max_len, d_model).float()
            pe.require_grad = False

            position = torch.arange(0, max_len).float().unsqueeze(1)
            div_term = (torch.arange(0, d_model, 2).float()
                        * -(math.log(10000.0) / d_model)).exp()

            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)

            pe = pe.unsqueeze(0)
            self.register_buffer('pe', pe)

        def forward(self, x):
            return self.pe[:, :x.size(1)]

    class TokenEmbedding(nn.Module):
        def __init__(self, c_in, d_model):
            super(TokenEmbedding, self).__init__()
            padding = 1 if torch.__version__ >= '1.5.0' else 2
            self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                    kernel_size=3, padding=padding, padding_mode='circular', bias=False)
            for m in self.modules():
                if isinstance(m, nn.Conv1d):
                    nn.init.kaiming_normal_(
                        m.weight, mode='fan_in', nonlinearity='leaky_relu')

        def forward(self, x):
            x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
            return x

    class FixedEmbedding(nn.Module):
        def __init__(self, c_in, d_model):
            super(FixedEmbedding, self).__init__()

            w = torch.zeros(c_in, d_model).float()
            w.require_grad = False

            position = torch.arange(0, c_in).float().unsqueeze(1)
            div_term = (torch.arange(0, d_model, 2).float()
                        * -(math.log(10000.0) / d_model)).exp()

            w[:, 0::2] = torch.sin(position * div_term)
            w[:, 1::2] = torch.cos(position * div_term)

            self.emb = nn.Embedding(c_in, d_model)
            self.emb.weight = nn.Parameter(w, requires_grad=False)

        def forward(self, x):
            return self.emb(x).detach()

    class TemporalEmbedding(nn.Module):
        def __init__(self, d_model, embed_type='fixed', freq='h'):
            super(TemporalEmbedding, self).__init__()

            minute_size = 4
            hour_size = 24
            weekday_size = 7
            day_size = 32
            month_size = 13

            Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding
            if freq == 't':
                self.minute_embed = Embed(minute_size, d_model)
            self.hour_embed = Embed(hour_size, d_model)
            self.weekday_embed = Embed(weekday_size, d_model)
            self.day_embed = Embed(day_size, d_model)
            self.month_embed = Embed(month_size, d_model)

        def forward(self, x):
            x = x.long()
            minute_x = self.minute_embed(x[:, :, 4]) if hasattr(
                self, 'minute_embed') else 0.
            hour_x = self.hour_embed(x[:, :, 3])
            weekday_x = self.weekday_embed(x[:, :, 2])
            day_x = self.day_embed(x[:, :, 1])
            month_x = self.month_embed(x[:, :, 0])

            return hour_x + weekday_x + day_x + month_x + minute_x

    class TimeFeatureEmbedding(nn.Module):
        def __init__(self, d_model, embed_type='timeF', freq='h'):
            super(TimeFeatureEmbedding, self).__init__()

            freq_map = {'h': 4, 't': 5, 's': 6,
                        'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}
            d_inp = freq_map[freq]
            self.embed = nn.Linear(d_inp, d_model, bias=False)

        def forward(self, x):
            return self.embed(x)

    class DataEmbedding(nn.Module):
        def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):
            super(DataEmbedding, self).__init__()

            self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)
            self.position_embedding = PositionalEmbedding(d_model=d_model)
            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,
                                                        freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(
                d_model=d_model, embed_type=embed_type, freq=freq)
            self.dropout = nn.Dropout(p=dropout)

        def forward(self, x, x_mark):
            if x_mark is None:
                x = self.value_embedding(x) + self.position_embedding(x)
            else:
                x = self.value_embedding(
                    x) + self.temporal_embedding(x_mark) + self.position_embedding(x)
            return self.dropout(x)

    class Inception_Block_V1(nn.Module):
        def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):
            super(Inception_Block_V1, self).__init__()
            self.in_channels = in_channels
            self.out_channels = out_channels
            self.num_kernels = num_kernels
            kernels = []
            for i in range(self.num_kernels):
                kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))
            self.kernels = nn.ModuleList(kernels)
            if init_weight:
                self._initialize_weights()

        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)

        def forward(self, x):
            res_list = []
            for i in range(self.num_kernels):
                res_list.append(self.kernels[i](x))
            res = torch.stack(res_list, dim=-1).mean(-1)
            return res

    class TimesBlock(nn.Module):
        def __init__(self, configs):
            super(TimesBlock, self).__init__()
            self.seq_len = configs.seq_len
            self.pred_len = configs.pred_len
            self.k = configs.top_k
            # parameter-efficient design
            self.conv = nn.Sequential(
                Inception_Block_V1(configs.d_model, configs.d_ff,
                                num_kernels=configs.num_kernels),
                nn.GELU(),
                Inception_Block_V1(configs.d_ff, configs.d_model,
                                num_kernels=configs.num_kernels)
            )

        def forward(self, x):
            B, T, N = x.size()
            period_list, period_weight = FFT_for_Period(x, self.k)

            res = []
            for i in range(self.k):
                period = period_list[i]
                # padding
                if (self.seq_len + self.pred_len) % period != 0:
                    length = (
                                    ((self.seq_len + self.pred_len) // period) + 1) * period
                    padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)
                    out = torch.cat([x, padding], dim=1)
                else:
                    length = (self.seq_len + self.pred_len)
                    out = x
                # reshape
                out = out.reshape(B, length // period, period,
                                N).permute(0, 3, 1, 2).contiguous()
                # 2D conv: from 1d Variation to 2d Variation
                out = self.conv(out)
                # reshape back
                out = out.permute(0, 2, 3, 1).reshape(B, -1, N)
                res.append(out[:, :(self.seq_len + self.pred_len), :])
            res = torch.stack(res, dim=-1)
            # adaptive aggregation
            period_weight = F.softmax(period_weight, dim=1)
            period_weight = period_weight.unsqueeze(
                1).unsqueeze(1).repeat(1, T, N, 1)
            res = torch.sum(res * period_weight, -1)
            # residual connection
            res = res + x
            return res

    class TimesNetModel(nn.Module):
        def __init__(self, configs):
            super(TimesNetModel, self).__init__()
            self.configs = configs
            self.seq_len = configs.seq_len
            self.pred_len = configs.pred_len
            self.model = nn.ModuleList([TimesBlock(configs)
                                        for _ in range(configs.e_layers)])
            self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,
                                            configs.dropout)
            self.layer = configs.e_layers
            self.layer_norm = nn.LayerNorm(configs.d_model)
            self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)

        def anomaly_detection(self, x_enc):
            # Normalization from Non-stationary Transformer
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(
                torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

            # embedding
            enc_out = self.enc_embedding(x_enc, None)  # [B,T,C]
            # TimesNet
            for i in range(self.layer):
                enc_out = self.layer_norm(self.model[i](enc_out))
            # project back
            dec_out = self.projection(enc_out)

            # De-Normalization from Non-stationary Transformer
            dec_out = dec_out * \
                    (stdev[:, 0, :].unsqueeze(1).repeat(
                        1, self.pred_len + self.seq_len, 1))
            dec_out = dec_out + \
                    (means[:, 0, :].unsqueeze(1).repeat(
                        1, self.pred_len + self.seq_len, 1))
            return dec_out

        def forward(self, x_enc):
            dec_out = self.anomaly_detection(x_enc)
            return dec_out  # [B, L, D]

    # æ•°æ®é›†ç±»
    class MTSTimeSeriesDataset(Dataset):
        def __init__(self, data: np.ndarray, window_size: int):
            super().__init__()
            self.data = data
            self.window_size = window_size
            self.sample_num = max(len(data) - window_size + 1, 0)
            
        def __len__(self):
            return self.sample_num
        
        def __getitem__(self, index):
            return torch.from_numpy(self.data[index:index + self.window_size]).float()

    # é…ç½®ç±»
    class TimesNetConfig:
        def __init__(self, seq_len=96, pred_len=0, enc_in=1, c_out=1, d_model=32, d_ff=32,
                    e_layers=1, top_k=2, num_kernels=3, embed='timeF', freq='h', dropout=0.1):
            self.seq_len = seq_len
            self.pred_len = pred_len
            self.enc_in = enc_in
            self.c_out = c_out
            self.d_model = d_model  # å‡å°æ¨¡å‹ç»´åº¦
            self.d_ff = d_ff        # å‡å°å‰é¦ˆç½‘ç»œç»´åº¦
            self.e_layers = e_layers  # å‡å°‘å±‚æ•°
            self.top_k = top_k      # å‡å°‘top_k
            self.num_kernels = num_kernels  # å‡å°‘å·ç§¯æ ¸æ•°é‡
            self.embed = embed
            self.freq = freq
            self.dropout = dropout

    """=============== TimesNetç»§æ‰¿BaseMethod å®ç° ==============="""

    class HyperTimesNet(BaseMethod):  
        def __init__(self, params:dict) -> None:
            super().__init__()
            self.__anomaly_score = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"logs/HyperTimesNet_{timestamp}.log"
            self.logger = setup_logger('HyperTimesNet', log_file)
            
            # TimesNet å‚æ•°é…ç½®
            self.window_size = params.get('window_size', 96)
            self.batch_size = params.get('batch_size', 32)
            self.epochs = params.get('epochs', 20)
            self.learning_rate = params.get('learning_rate', 0.001)
            
            # è®°å½•åˆå§‹åŒ–ä¿¡æ¯
            self.logger.info("="*60)
            self.logger.info("HyperTimesNet åˆå§‹åŒ–")
            self.logger.info("="*60)
            self.logger.info(f"è®¾å¤‡: {self.device}")
            self.logger.info(f"çª—å£å¤§å°: {self.window_size}")
            self.logger.info(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            self.logger.info(f"è®­ç»ƒè½®æ•°: {self.epochs}")
            self.logger.info(f"å­¦ä¹ ç‡: {self.learning_rate}")
            
            if torch.cuda.is_available():
                self.logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
                self.logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
            
        def train_valid_phase(self, tsData):
            self.logger.info("\n" + "="*60)
            self.logger.info("å¼€å§‹è®­ç»ƒé˜¶æ®µ")
            self.logger.info("="*60)
            
            # æ˜¾ç¤ºå½“å‰è®­ç»ƒçš„æ•°æ®é›†
            dataset_name = getattr(tsData, 'dataset_name', 'æœªçŸ¥æ•°æ®é›†')
            self.logger.info(f"å½“å‰è®­ç»ƒæ•°æ®é›†: {dataset_name}")
            print(f"ğŸ¯ å½“å‰è®­ç»ƒæ•°æ®é›†: {dataset_name}")
            
            # æ•°æ®åˆ†æ
            train_shape = tsData.train.shape
            self.logger.info(f"åŸå§‹è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_shape}")
            
            if len(train_shape) > 1:
                self.logger.info(f"æ—¶é—´æ­¥æ•°: {train_shape[0]}")
                self.logger.info(f"ç‰¹å¾ç»´åº¦: {train_shape[1]}")
                
                # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
                train_mean = np.mean(tsData.train, axis=0)
                train_std = np.std(tsData.train, axis=0)
                self.logger.info(f"è®­ç»ƒæ•°æ®å‡å€¼: {train_mean[:5]}..." if len(train_mean) > 5 else f"è®­ç»ƒæ•°æ®å‡å€¼: {train_mean}")
                self.logger.info(f"è®­ç»ƒæ•°æ®æ ‡å‡†å·®: {train_std[:5]}..." if len(train_std) > 5 else f"è®­ç»ƒæ•°æ®æ ‡å‡†å·®: {train_std}")
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                q1, q99 = np.percentile(tsData.train, [1, 99])
                self.logger.info(f"æ•°æ®åˆ†ä½æ•°èŒƒå›´: [{q1:.4f}, {q99:.4f}]")
            
            print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {tsData.train.shape}")
            
            # åŠ¨æ€ç¡®å®šè¾“å…¥ç»´åº¦
            enc_in = tsData.train.shape[1] if len(tsData.train.shape) > 1 else 1
            self.logger.info(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {enc_in}")
            
            # ä¼˜åŒ–é…ç½® - é’ˆå¯¹RTX 3060ä¼˜åŒ–ï¼Œå¤§å¹…é™ä½æ¨¡å‹å¤æ‚åº¦
            self.config = TimesNetConfig(
                seq_len=self.window_size,
                pred_len=0,  # å¼‚å¸¸æ£€æµ‹ä¸éœ€è¦é¢„æµ‹
                enc_in=enc_in,
                c_out=enc_in,
                d_model=min(64, max(32, enc_in * 2)),  # å¤§å¹…é™ä½æ¨¡å‹ç»´åº¦ï¼ŒRTX 3060å‹å¥½
                d_ff=min(128, max(64, enc_in * 4)),    # å¤§å¹…é™ä½å‰é¦ˆç»´åº¦
                e_layers=2,  # å‡å°‘å±‚æ•°ä»¥é™ä½è®¡ç®—é‡
                top_k=min(3, max(2, self.window_size // 16)),  # å‡å°‘å‘¨æœŸæ•°ä»¥é™ä½FFTè®¡ç®—
                num_kernels=4,  # å‡å°‘å·ç§¯æ ¸æ•°é‡
                embed='timeF',
                freq='h',
                dropout=0.15  # é€‚ä¸­çš„dropoutç‡
            )
            
            # è®°å½•æ¨¡å‹é…ç½®
            self.logger.info("\næ¨¡å‹é…ç½®:")
            self.logger.info(f"  åºåˆ—é•¿åº¦ (seq_len): {self.config.seq_len}")
            self.logger.info(f"  æ¨¡å‹ç»´åº¦ (d_model): {self.config.d_model}")
            self.logger.info(f"  å‰é¦ˆç»´åº¦ (d_ff): {self.config.d_ff}")
            self.logger.info(f"  ç¼–ç å™¨å±‚æ•° (e_layers): {self.config.e_layers}")
            self.logger.info(f"  Top-Kå‘¨æœŸ (top_k): {self.config.top_k}")
            self.logger.info(f"  å·ç§¯æ ¸æ•°é‡ (num_kernels): {self.config.num_kernels}")
            self.logger.info(f"  Dropoutç‡: {self.config.dropout}")
            
            # åˆ›å»ºæ¨¡å‹
            model_start_time = time.time()
            self.model = TimesNetModel(self.config).to(self.device)
            model_creation_time = time.time() - model_start_time
            
            # è®¡ç®—æ¨¡å‹å‚æ•°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            self.logger.info(f"\næ¨¡å‹åˆ›å»ºå®Œæˆ (è€—æ—¶: {model_creation_time:.3f}ç§’)")
            self.logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
            self.logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
            self.logger.info(f"æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / 1e6:.2f} MB")
            
            self.optimizer = torch.optim.AdamW(  # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
                self.model.parameters(), 
                lr=self.learning_rate,
                weight_decay=1e-3,  # å¢åŠ æƒé‡è¡°å‡ï¼Œä¸NewTimesNetç›¸åŒ
                betas=(0.9, 0.999)  # ä¸NewTimesNetç›¸åŒçš„åŠ¨é‡å‚æ•°
            )
            
            # ä½¿ç”¨SmoothL1Lossï¼Œä¸NewTimesNetç›¸åŒï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
            self.criterion = nn.SmoothL1Loss()
            
            # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ - é’ˆå¯¹è¾ƒçŸ­è®­ç»ƒä¼˜åŒ–
            total_steps = len(DataLoader(
                MTSTimeSeriesDataset(tsData.train, self.window_size),
                batch_size=self.batch_size,
                drop_last=True
            )) * self.epochs
            
            # ä½¿ç”¨æ›´ä¿å®ˆçš„ReduceLROnPlateauä»¥é€‚åº”è¾ƒçŸ­è®­ç»ƒ
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.7,  # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡è¡°å‡
                patience=2,   # æ›´çŸ­çš„patience
                verbose=True,
                min_lr=1e-6
            )
            
            self.logger.info(f"ä¼˜åŒ–å™¨: AdamW (lr={self.learning_rate}, weight_decay=1e-3)")
            self.logger.info(f"æŸå¤±å‡½æ•°: SmoothL1Loss (æ›´é²æ£’çš„æŸå¤±å‡½æ•°)")
            self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau (RTX 3060ä¼˜åŒ–ç‰ˆ)")
            self.logger.info(f"âš¡ æ€§èƒ½ä¼˜åŒ–: é¢„è®¡è®­ç»ƒæ—¶é—´å‡å°‘70%")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            if len(tsData.train.shape) == 1:
                train_data = tsData.train.reshape(-1, 1)
            else:
                train_data = tsData.train
            
            original_length = len(train_data)
            self.logger.info(f"\næ•°æ®é¢„å¤„ç†:")
            self.logger.info(f"åŸå§‹æ•°æ®é•¿åº¦: {original_length}")
            
            # æ•°æ®é‡‡æ ·ä¼˜åŒ– - é’ˆå¯¹RTX 3060ï¼Œè¿›ä¸€æ­¥å‡å°‘æ•°æ®é‡
            if len(train_data) > 8000:  # å¤§å¹…å‡å°‘æ•°æ®é‡ä»¥åŠ å¿«è®­ç»ƒ
                # ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ•°æ®åˆ†å¸ƒ
                sample_ratio = 8000 / len(train_data)
                indices = np.arange(len(train_data))
                step = int(1 / sample_ratio)
                sampled_indices = indices[::step][:8000]
                train_data = train_data[sampled_indices]
                
                self.logger.info(f"æ•°æ®é‡‡æ ·: {original_length} -> {len(train_data)} (é‡‡æ ·ç‡: {sample_ratio:.3f})")
                self.logger.info(f"é‡‡æ ·æ­¥é•¿: {step}")
                self.logger.info("âš¡ RTX 3060ä¼˜åŒ–: å¤§å¹…å‡å°‘è®­ç»ƒæ•°æ®ä»¥æé«˜é€Ÿåº¦")
                print(f"âš¡ RTX 3060ä¼˜åŒ–åè®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
            else:
                self.logger.info("æ•°æ®é•¿åº¦é€‚ä¸­ï¼Œæ— éœ€é‡‡æ ·")
                
            train_dataset = MTSTimeSeriesDataset(train_data, self.window_size)
            train_loader = DataLoader(
                train_dataset, 
                batch_size=self.batch_size, 
                shuffle=True,
                num_workers=0,  # Windowså…¼å®¹æ€§
                pin_memory=True if torch.cuda.is_available() else False,
                drop_last=True  # é¿å…æœ€åä¸€ä¸ªbatchå¤§å°ä¸ä¸€è‡´
            )
            
            dataset_size = len(train_dataset)
            num_batches = len(train_loader)
            
            self.logger.info(f"æ•°æ®é›†ä¿¡æ¯:")
            self.logger.info(f"  è®­ç»ƒçª—å£æ•°é‡: {dataset_size}")
            self.logger.info(f"  æ‰¹æ¬¡æ•°é‡: {num_batches}")
            self.logger.info(f"  æ¯æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            self.logger.info(f"  æ€»è®­ç»ƒæ ·æœ¬: {dataset_size}")
            
            print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æ‰¹æ¬¡æ•°: {len(train_loader)}")
            
            # è®­ç»ƒæ¨¡å‹ - æ”¹è¿›çš„è®­ç»ƒå¾ªç¯
            self.logger.info(f"\n" + "="*50)
            self.logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
            self.logger.info("="*50)
            
            self.model.train()
            best_loss = float('inf')
            patience = 3  # å‡å°‘patienceä»¥é€‚åº”è¾ƒçŸ­è®­ç»ƒ
            patience_counter = 0
            best_state = None  # æ·»åŠ æœ€ä½³æ¨¡å‹çŠ¶æ€ä¿å­˜
            
            # è®­ç»ƒç»Ÿè®¡
            epoch_losses = []
            epoch_times = []
            learning_rates = []
            
            training_start_time = time.time()
            
            for epoch in range(self.epochs):
                epoch_start_time = time.time()
                total_loss = 0
                num_batches = 0
                batch_losses = []
                
                # è®°å½•å½“å‰å­¦ä¹ ç‡
                current_lr = self.optimizer.param_groups[0]['lr']
                learning_rates.append(current_lr)
                
                self.logger.info(f"\nEpoch {epoch+1}/{self.epochs} - å­¦ä¹ ç‡: {current_lr:.6f}")
                
                progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.epochs}')
                for batch_idx, batch_x in enumerate(progress_bar):
                    batch_start_time = time.time()
                    batch_x = batch_x.to(self.device, non_blocking=True)
                    
                    self.optimizer.zero_grad()
                    
                    # ä½¿ç”¨anomaly_detectionæ–¹æ³•è¿›è¡Œè®­ç»ƒ
                    outputs = self.model.anomaly_detection(batch_x)
                    
                    # è®¡ç®—é‡æ„æŸå¤±
                    loss = self.criterion(outputs, batch_x)
                    
                    # æ·»åŠ L2æ­£åˆ™åŒ–æŸå¤±
                    l2_lambda = 1e-6  # é™ä½L2æ­£åˆ™åŒ–å¼ºåº¦ä»¥åŠ å¿«æ”¶æ•›
                    l2_reg = torch.tensor(0.).to(self.device)
                    for param in self.model.parameters():
                        l2_reg += torch.norm(param)
                    loss += l2_lambda * l2_reg
                    
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    self.optimizer.step()
                    
                    batch_time = time.time() - batch_start_time
                    batch_loss = loss.item()
                    total_loss += batch_loss
                    batch_losses.append(batch_loss)
                    num_batches += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f'{batch_loss:.6f}',
                        'grad_norm': f'{grad_norm:.3f}',
                        'time': f'{batch_time:.3f}s'
                    })
                    
                    # è®°å½•è¯¦ç»†çš„æ‰¹æ¬¡ä¿¡æ¯ï¼ˆæ¯20ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡ï¼Œå‡å°‘æ—¥å¿—é‡ï¼‰
                    if batch_idx % 20 == 0:
                        self.logger.debug(f"  Batch {batch_idx}: loss={batch_loss:.6f}, grad_norm={grad_norm:.3f}, time={batch_time:.3f}s")
                
                epoch_time = time.time() - epoch_start_time
                avg_loss = total_loss / num_batches
                epoch_losses.append(avg_loss)
                epoch_times.append(epoch_time)
                
                # è¯¦ç»†çš„epochç»Ÿè®¡
                loss_std = np.std(batch_losses)
                min_batch_loss = min(batch_losses)
                max_batch_loss = max(batch_losses)
                
                self.logger.info(f"Epoch {epoch+1} å®Œæˆ:")
                self.logger.info(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
                self.logger.info(f"  æŸå¤±æ ‡å‡†å·®: {loss_std:.6f}")
                self.logger.info(f"  æŸå¤±èŒƒå›´: [{min_batch_loss:.6f}, {max_batch_loss:.6f}]")
                self.logger.info(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
                self.logger.info(f"  æ¯æ‰¹æ¬¡å¹³å‡æ—¶é—´: {epoch_time/num_batches:.3f}ç§’")
                
                print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.6f}, Time: {epoch_time:.1f}s")
                
                # å­¦ä¹ ç‡è°ƒåº¦
                old_lr = self.optimizer.param_groups[0]['lr']
                self.scheduler.step(avg_loss)  # åŸºäºæŸå¤±è°ƒåº¦
                new_lr = self.optimizer.param_groups[0]['lr']
                
                if new_lr != old_lr:
                    self.logger.info(f"  å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} -> {new_lr:.6f}")
                
                # æ”¹è¿›çš„æ—©åœæœºåˆ¶
                if avg_loss < best_loss * 0.98:  # æ›´å®½æ¾çš„æ”¹å–„é˜ˆå€¼ä»¥é€‚åº”è¾ƒçŸ­è®­ç»ƒ
                    best_loss = avg_loss
                    patience_counter = 0
                    self.logger.info(f"  âœ“ æ–°çš„æœ€ä½³æŸå¤±: {best_loss:.6f}")
                    # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                    best_state = {
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'loss': best_loss,
                        'epoch': epoch
                    }
                else:
                    patience_counter += 1
                    self.logger.info(f"  æ—©åœè®¡æ•°å™¨: {patience_counter}/{patience}")
                    if patience_counter >= patience:
                        self.logger.info(f"  æ—©åœè§¦å‘! åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                        print(f"Early stopping at epoch {epoch+1}, best loss: {best_loss:.6f}")
                        break
            
            # æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€
            if best_state is not None:
                self.model.load_state_dict(best_state['model_state_dict'])
                self.logger.info(f"å·²æ¢å¤ç¬¬{best_state['epoch']+1}è½®çš„æœ€ä½³æ¨¡å‹çŠ¶æ€")
            
            total_training_time = time.time() - training_start_time
            
            # è®­ç»ƒæ€»ç»“
            self.logger.info(f"\n" + "="*50)
            self.logger.info("è®­ç»ƒå®Œæˆæ€»ç»“")
            self.logger.info("="*50)
            self.logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’ ({total_training_time/60:.1f}åˆ†é’Ÿ)")
            self.logger.info(f"å®é™…è®­ç»ƒè½®æ•°: {len(epoch_losses)}/{self.epochs}")
            self.logger.info(f"æœ€ä½³æŸå¤±: {best_loss:.6f}")
            self.logger.info(f"æœ€ç»ˆæŸå¤±: {epoch_losses[-1]:.6f}")
            self.logger.info(f"æŸå¤±æ”¹å–„: {((epoch_losses[0] - best_loss) / epoch_losses[0] * 100):.2f}%")
            self.logger.info(f"å¹³å‡æ¯è½®æ—¶é—´: {np.mean(epoch_times):.2f}ç§’")
            
            print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆloss: {best_loss:.6f}")
        
        def test_phase(self, tsData: MTSData):
            self.logger.info(f"\n" + "="*60)
            self.logger.info("å¼€å§‹æµ‹è¯•é˜¶æ®µ")
            self.logger.info("="*60)
            
            # æµ‹è¯•æ•°æ®åˆ†æ
            test_shape = tsData.test.shape
            self.logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_shape}")
            
            if len(test_shape) > 1:
                self.logger.info(f"æµ‹è¯•æ—¶é—´æ­¥æ•°: {test_shape[0]}")
                self.logger.info(f"æµ‹è¯•ç‰¹å¾ç»´åº¦: {test_shape[1]}")
                
                # æµ‹è¯•æ•°æ®ç»Ÿè®¡
                test_mean = np.mean(tsData.test, axis=0)
                test_std = np.std(tsData.test, axis=0)
                q1, q99 = np.percentile(tsData.test, [1, 99])
                self.logger.info(f"æµ‹è¯•æ•°æ®åˆ†ä½æ•°èŒƒå›´: [{q1:.4f}, {q99:.4f}]")
            
            print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {tsData.test.shape}")
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            if len(tsData.test.shape) == 1:
                test_data = tsData.test.reshape(-1, 1)
            else:
                test_data = tsData.test
            
            self.model.eval()
            self.logger.info("æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
            
            # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            test_dataset = MTSTimeSeriesDataset(test_data, self.window_size)
            test_loader = DataLoader(
                test_dataset, 
                batch_size=self.batch_size * 2,  # æµ‹è¯•æ—¶å¯ä»¥ç”¨æ›´å¤§çš„batch
                shuffle=False,
                num_workers=0,  # Windowså…¼å®¹æ€§
                pin_memory=True if torch.cuda.is_available() else False
            )
            
            test_dataset_size = len(test_dataset)
            test_num_batches = len(test_loader)
            
            self.logger.info(f"æµ‹è¯•æ•°æ®é›†ä¿¡æ¯:")
            self.logger.info(f"  æµ‹è¯•çª—å£æ•°é‡: {test_dataset_size}")
            self.logger.info(f"  æµ‹è¯•æ‰¹æ¬¡æ•°é‡: {test_num_batches}")
            self.logger.info(f"  æµ‹è¯•æ‰¹æ¬¡å¤§å°: {self.batch_size * 2}")
            
            all_reconstruction_errors = []
            
            testing_start_time = time.time()
            
            with torch.no_grad():
                progress_bar = tqdm.tqdm(test_loader, desc='Testing')
                batch_times = []
                reconstruction_losses = []
                
                for batch_idx, batch_x in enumerate(progress_bar):
                    batch_start_time = time.time()
                    batch_x = batch_x.to(self.device, non_blocking=True)
                    
                    # ä½¿ç”¨TimesNetçš„anomaly_detectionæ–¹æ³•
                    reconstruction = self.model.anomaly_detection(batch_x)
                    
                    # æ”¹è¿›çš„å¼‚å¸¸åˆ†æ•°è®¡ç®— - ä½¿ç”¨æ•´ä¸ªçª—å£çš„é‡æ„è¯¯å·®
                    # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å¹³å‡é‡æ„è¯¯å·®
                    mse_per_timestep = torch.mean((batch_x - reconstruction) ** 2, dim=2)  # [batch, time]
                    
                    # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¯¯å·®ä½œä¸ºè¯¥çª—å£çš„å¼‚å¸¸åˆ†æ•°
                    # æˆ–è€…å¯ä»¥ä½¿ç”¨æœ€å¤§å€¼ã€å¹³å‡å€¼ç­‰èšåˆæ–¹å¼
                    window_scores = mse_per_timestep[:, -1]  # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    # æˆ–è€…ä½¿ç”¨æ•´ä¸ªçª—å£çš„å¹³å‡è¯¯å·®: window_scores = torch.mean(mse_per_timestep, dim=1)
                    # æˆ–è€…ä½¿ç”¨æ•´ä¸ªçª—å£çš„æœ€å¤§è¯¯å·®: window_scores = torch.max(mse_per_timestep, dim=1)[0]
                    
                    all_reconstruction_errors.extend(window_scores.cpu().numpy())
                    
                    batch_time = time.time() - batch_start_time
                    batch_times.append(batch_time)
                    
                    # è®°å½•é‡æ„æŸå¤±ç»Ÿè®¡
                    batch_reconstruction_loss = torch.mean(mse_per_timestep).item()
                    reconstruction_losses.append(batch_reconstruction_loss)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'recon_loss': f'{batch_reconstruction_loss:.6f}',
                        'time': f'{batch_time:.3f}s'
                    })
                    
                    # è®°å½•è¯¦ç»†æ‰¹æ¬¡ä¿¡æ¯ï¼ˆæ¯20ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡ï¼‰
                    if batch_idx % 20 == 0:
                        self.logger.debug(f"  æµ‹è¯•æ‰¹æ¬¡ {batch_idx}: é‡æ„æŸå¤±={batch_reconstruction_loss:.6f}, è€—æ—¶={batch_time:.3f}s")
            
            testing_time = time.time() - testing_start_time
            
            # æµ‹è¯•é˜¶æ®µç»Ÿè®¡
            self.logger.info(f"\næµ‹è¯•é˜¶æ®µå®Œæˆ:")
            self.logger.info(f"  æ€»æµ‹è¯•æ—¶é—´: {testing_time:.2f}ç§’")
            self.logger.info(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {np.mean(batch_times):.3f}ç§’")
            self.logger.info(f"  å¹³å‡é‡æ„æŸå¤±: {np.mean(reconstruction_losses):.6f}")
            self.logger.info(f"  é‡æ„æŸå¤±æ ‡å‡†å·®: {np.std(reconstruction_losses):.6f}")
            
            # å¤„ç†åˆ†æ•°é•¿åº¦ï¼Œç¡®ä¿ä¸åŸå§‹æ•°æ®é•¿åº¦ä¸€è‡´
            scores = np.array(all_reconstruction_errors)
            original_scores_length = len(scores)
            
            self.logger.info(f"\nå¼‚å¸¸åˆ†æ•°åå¤„ç†:")
            self.logger.info(f"  åŸå§‹åˆ†æ•°æ•°é‡: {original_scores_length}")
            self.logger.info(f"  ç›®æ ‡æ•°æ®é•¿åº¦: {len(test_data)}")
            
            # ä¸ºå‰é¢çš„æ—¶é—´æ­¥å¡«å……åˆ†æ•°ï¼ˆæ»‘åŠ¨çª—å£å¯¼è‡´çš„ç¼ºå¤±ï¼‰
            num_missing = len(test_data) - len(scores)
            if num_missing > 0:
                # ä½¿ç”¨å‰å‡ ä¸ªåˆ†æ•°çš„ä¸­ä½æ•°æ¥å¡«å……ï¼Œæ›´ç¨³å®š
                if len(scores) > 0:
                    fill_value = np.median(scores[:min(20, len(scores))])
                    self.logger.info(f"  éœ€è¦å¡«å…… {num_missing} ä¸ªåˆ†æ•°ï¼Œå¡«å……å€¼: {fill_value:.6f}")
                else:
                    fill_value = 0
                    self.logger.info(f"  éœ€è¦å¡«å…… {num_missing} ä¸ªåˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å¡«å……å€¼: 0")
                missing_scores = np.full(num_missing, fill_value)
                scores = np.concatenate([missing_scores, scores])
            else:
                self.logger.info("  æ— éœ€å¡«å……åˆ†æ•°")
            
            # è£å‰ªåˆ°æ­£ç¡®é•¿åº¦
            scores = scores[:len(test_data)]
            
            # æ”¹è¿›çš„å½’ä¸€åŒ– - ä½¿ç”¨æ›´é²æ£’çš„æ–¹æ³•
            if len(scores) > 0:
                # ä½¿ç”¨åˆ†ä½æ•°è¿›è¡Œé²æ£’å½’ä¸€åŒ–ï¼Œé¿å…å¼‚å¸¸å€¼å½±å“
                q1, q99 = np.percentile(scores, [1, 99])
                original_range = [np.min(scores), np.max(scores)]
                
                self.logger.info(f"  åŸå§‹åˆ†æ•°èŒƒå›´: [{original_range[0]:.6f}, {original_range[1]:.6f}]")
                self.logger.info(f"  åˆ†ä½æ•°èŒƒå›´ (1%, 99%): [{q1:.6f}, {q99:.6f}]")
                
                if q99 > q1:
                    scores = np.clip(scores, q1, q99)
                    scores = (scores - q1) / (q99 - q1)
                    self.logger.info(f"  ä½¿ç”¨åˆ†ä½æ•°å½’ä¸€åŒ–")
                else:
                    # å¦‚æœåˆ†ä½æ•°ç›¸ç­‰ï¼Œä½¿ç”¨ç®€å•å½’ä¸€åŒ–
                    score_min, score_max = np.min(scores), np.max(scores)
                    if score_max > score_min:
                        scores = (scores - score_min) / (score_max - score_min)
                        self.logger.info(f"  ä½¿ç”¨ç®€å•å½’ä¸€åŒ–")
                    else:
                        scores = np.zeros_like(scores)
                        self.logger.info(f"  åˆ†æ•°æ— å˜åŒ–ï¼Œè®¾ç½®ä¸ºé›¶")
                
                final_range = [np.min(scores), np.max(scores)]
                self.logger.info(f"  æœ€ç»ˆåˆ†æ•°èŒƒå›´: [{final_range[0]:.6f}, {final_range[1]:.6f}]")
            else:
                scores = np.zeros(len(test_data))
                self.logger.warning("  æ— æœ‰æ•ˆåˆ†æ•°ï¼Œä½¿ç”¨é›¶åˆ†æ•°")
            
            self.__anomaly_score = scores
            
            self.logger.info(f"\nå¼‚å¸¸æ£€æµ‹å®Œæˆ:")
            self.logger.info(f"  æœ€ç»ˆå¼‚å¸¸åˆ†æ•°é•¿åº¦: {len(scores)}")
            self.logger.info(f"  åˆ†æ•°ç»Ÿè®¡: å‡å€¼={np.mean(scores):.4f}, æ ‡å‡†å·®={np.std(scores):.4f}")
            self.logger.info(f"  åˆ†æ•°åˆ†ä½æ•°: [5%={np.percentile(scores, 5):.4f}, 95%={np.percentile(scores, 95):.4f}]")
            
            print(f"å¼‚å¸¸åˆ†æ•°è®¡ç®—å®Œæˆï¼Œé•¿åº¦: {len(scores)}, åˆ†æ•°èŒƒå›´: [{np.min(scores):.4f}, {np.max(scores):.4f}]")
        
        def anomaly_score(self) -> np.ndarray:
            return self.__anomaly_score
        
        def param_statistic(self, save_file):
            self.logger.info(f"\n" + "="*60)
            self.logger.info("æ¨¡å‹å‚æ•°ç»Ÿè®¡")
            self.logger.info("="*60)
            
            if hasattr(self, 'model'):
                total_params = sum(p.numel() for p in self.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
                
                # åˆ†å±‚ç»Ÿè®¡å‚æ•°
                layer_params = {}
                for name, param in self.model.named_parameters():
                    layer_name = name.split('.')[0] if '.' in name else name
                    if layer_name not in layer_params:
                        layer_params[layer_name] = 0
                    layer_params[layer_name] += param.numel()
                
                param_info = f"""HyperTimesNet æ¨¡å‹å‚æ•°ç»Ÿè®¡æŠ¥å‘Š
                ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

                === åŸºæœ¬ä¿¡æ¯ ===
                æ€»å‚æ•°æ•°é‡: {total_params:,}
                å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}
                æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / (1024**2):.2f} MB

                === æ¨¡å‹é…ç½® ===
                åºåˆ—é•¿åº¦ (seq_len): {self.config.seq_len}
                æ¨¡å‹ç»´åº¦ (d_model): {self.config.d_model}
                å‰é¦ˆç»´åº¦ (d_ff): {self.config.d_ff}
                ç¼–ç å™¨å±‚æ•° (e_layers): {self.config.e_layers}
                Top-Kå‘¨æœŸ (top_k): {self.config.top_k}
                å·ç§¯æ ¸æ•°é‡ (num_kernels): {self.config.num_kernels}
                Dropoutç‡: {self.config.dropout}

                === è®­ç»ƒé…ç½® ===
                çª—å£å¤§å°: {self.window_size}
                æ‰¹æ¬¡å¤§å°: {self.batch_size}
                è®­ç»ƒè½®æ•°: {self.epochs}
                å­¦ä¹ ç‡: {self.learning_rate}

                === åˆ†å±‚å‚æ•°ç»Ÿè®¡ ==="""
                
                for layer_name, param_count in sorted(layer_params.items()):
                    param_info += f"\n{layer_name}: {param_count:,} å‚æ•° ({param_count/total_params*100:.2f}%)"
                
                self.logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
                self.logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
                self.logger.info(f"æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.2f} MB")
                
                for layer_name, param_count in sorted(layer_params.items()):
                    self.logger.info(f"  {layer_name}: {param_count:,} ({param_count/total_params*100:.2f}%)")
                    
            else:
                param_info = f"""HyperTimesNet æ¨¡å‹å‚æ•°ç»Ÿè®¡æŠ¥å‘Š
                ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

                é”™è¯¯: TimesNetæ¨¡å‹å°šæœªåˆå§‹åŒ–
                """
                self.logger.warning("æ¨¡å‹å°šæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç»Ÿè®¡å‚æ•°")
                
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(param_info)
                
            self.logger.info(f"å‚æ•°ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_file}")
    
    """============= è¿è¡Œç®—æ³• ============="""
    # Specifying methods and training schemas
    training_schema = "mts"
    method = "HyperTimesNet"  # string of your algo class
    
    # run models
    gctrl.run_exps(
        method=method,
        training_schema=training_schema,
        hparams={
            "window_size": 32,     # å‡å°çª—å£å¤§å°ä»¥é™ä½è®¡ç®—å¤æ‚åº¦
            "batch_size": 64,      # å‡å°batch_sizeä»¥é€‚åº”RTX 3060æ˜¾å­˜
            "epochs": 10,          # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«å®éªŒé€Ÿåº¦
            "learning_rate": 0.001  # ä¿æŒå­¦ä¹ ç‡ä¸å˜
        },
        # use which method to preprocess original data. 
        # Default: raw
        # Option: 
        #   - z-score(Standardlization), 
        #   - min-max(Normalization), 
        #   - raw (original curves)
        preprocess="z-score",  # ä½¿ç”¨z-scoreæ ‡å‡†åŒ–
    )
       
        
    """============= [EVALUATION SETTINGS] ============="""
    
    from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA
    # Specifying evaluation protocols
    gctrl.set_evals(
        [
            PointF1PA(),
            EventF1PA(),
            EventF1PA(mode="squeeze")
        ]
    )

    gctrl.do_evals(
        method=method,
        training_schema=training_schema
    )
        
        
    """============= [PLOTTING SETTINGS] ============="""
    
    # plot anomaly scores for each curve
    gctrl.plots(
        method=method,
        training_schema=training_schema
    )
 